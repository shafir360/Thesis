{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55078,"status":"ok","timestamp":1659728944838,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"klpNNdQNRAzv","outputId":"e445116f-4320-4ecb-8959-cb1dd0cbd661"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34138,"status":"ok","timestamp":1659728978967,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"iX00mr6WfSHG","outputId":"2b7870b7-aa08-4cfb-e9f2-74451cd6c5d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.0 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 63.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 9.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.8.1)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 12.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 55.1 MB/s \n","\u001b[?25hRequirement already satisfied: dill\u003c0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Collecting fsspec[http]\u003e=2021.11.1\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 54.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 45.2 MB/s \n","\u001b[?25hCollecting responses\u003c0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyarrow\u003e=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.1.0-\u003edatasets) (3.7.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.1.0-\u003edatasets) (6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.1.0-\u003edatasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003edatasets) (3.0.9)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (1.24.3)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (6.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (0.13.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.3.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (22.1.0)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (2.1.0)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.2.0)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (4.0.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003edatasets) (1.8.1)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003edatasets) (3.8.1)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2022.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas-\u003edatasets) (1.15.0)\n","Installing collected packages: urllib3, fsspec, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 fsspec-2022.7.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 12.6 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting GPUtil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=e51da25197095d3dd95da6feb257a70c8b6d3c5f6b61b0dc53bd266b75ec356a\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built GPUtil\n","Installing collected packages: GPUtil\n","Successfully installed GPUtil-1.4.0\n"]}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install GPUtil\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","#!pip install wandb\n","import json\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","import tensorflow as tf\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3571,"status":"ok","timestamp":1659728982526,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"NoSFcYvATZJ_"},"outputs":[],"source":["\n","def ConvertLabel2ModelLabel(label):\n","  if label == 'Positive':\n","    o = 2\n","  elif label == 'Neutral':\n","    o = 1\n","  elif label == 'Negative':\n","    o = 0\n","  elif label == 'spam':\n","    o = 1\n","  else: \n","    print(\" error at ConvertLabel2ModelLabel label is \" , label)\n","    o = np.nan\n","  return o\n","\n","import torch\n","from GPUtil import showUtilization as gpu_usage\n","from numba import cuda\n","\n","def free_gpu_cache():\n","    print(\"Initial GPU Usage\")\n","    gpu_usage()                             \n","\n","    torch.cuda.empty_cache()\n","\n","    cuda.select_device(0)\n","    cuda.close()\n","    cuda.select_device(0)\n","\n","    print(\"GPU Usage after emptying the cache\")\n","    gpu_usage()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2304,"status":"ok","timestamp":1659728984813,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"0Yk0sHM_ff68","outputId":"55faa164-1342-4a5c-d5f3-6730cb79ca47"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-899103b6-621c-4921-b36e-d685db18537a\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e339\u003c/td\u003e\n","      \u003ctd\u003eКриптоВести BTC 🤘  Blockasset Taps Well-Known ...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e747\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e301\u003c/td\u003e\n","      \u003ctd\u003e__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1743\u003c/td\u003e\n","      \u003ctd\u003e$btc btc health check! No custom death cross; ...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e1108\u003c/td\u003e\n","      \u003ctd\u003e44 🏦FTX | BTC PERP  🦋 If you have trouble imag...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2785\u003c/th\u003e\n","      \u003ctd\u003e683\u003c/td\u003e\n","      \u003ctd\u003eThis will be very useful in the future because...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2786\u003c/th\u003e\n","      \u003ctd\u003e1451\u003c/td\u003e\n","      \u003ctd\u003eAnd without Bitcoin Maxies hyping Bitcoin , th...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2787\u003c/th\u003e\n","      \u003ctd\u003e1106\u003c/td\u003e\n","      \u003ctd\u003e11 IST   88.724  37996.605 ₿  3371120.946  253...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2788\u003c/th\u003e\n","      \u003ctd\u003e1787\u003c/td\u003e\n","      \u003ctd\u003eBitcoin is Liberation Network.\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2789\u003c/th\u003e\n","      \u003ctd\u003e344\u003c/td\u003e\n","      \u003ctd\u003ebitcoin is the best MONEY in the history of ma...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e2790 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-899103b6-621c-4921-b36e-d685db18537a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-899103b6-621c-4921-b36e-d685db18537a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-899103b6-621c-4921-b36e-d685db18537a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text     label  \\\n","0            339  КриптоВести BTC 🤘  Blockasset Taps Well-Known ...  Positive   \n","1            747  Full thanks to  as they have analysed this Bit...   Neutral   \n","2            301  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...   Neutral   \n","3           1743  $btc btc health check! No custom death cross; ...   Neutral   \n","4           1108  44 🏦FTX | BTC PERP  🦋 If you have trouble imag...   Neutral   \n","...          ...                                                ...       ...   \n","2785         683  This will be very useful in the future because...  Positive   \n","2786        1451  And without Bitcoin Maxies hyping Bitcoin , th...  Positive   \n","2787        1106  11 IST   88.724  37996.605 ₿  3371120.946  253...  Positive   \n","2788        1787                     Bitcoin is Liberation Network.  Positive   \n","2789         344  bitcoin is the best MONEY in the history of ma...   Neutral   \n","\n","             source quality  \n","0      new_turk_low     low  \n","1     new_turk_high    high  \n","2       self_manual    high  \n","3      new_turk_low     low  \n","4      new_turk_low     low  \n","...             ...     ...  \n","2785   new_turk_low     low  \n","2786   new_turk_low     low  \n","2787   new_turk_low     low  \n","2788   new_turk_low     low  \n","2789   new_turk_low     low  \n","\n","[2790 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-de35d9f2-b08a-40cc-9633-f48a8d61cc75\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e71\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e266\u003c/td\u003e\n","      \u003ctd\u003eTyping mistake for Satoshi not bitcoin\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e410\u003c/td\u003e\n","      \u003ctd\u003eSaitama SaitamaWolfPack SaitamaInu SaitaMask $...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e333\u003c/td\u003e\n","      \u003ctd\u003e100%, the nuclear industry has no idea what is...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e401\u003c/td\u003e\n","      \u003ctd\u003eairdrop Airdrops Crypto BSC DeFi Polygon Bitco...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1190\u003c/th\u003e\n","      \u003ctd\u003e171\u003c/td\u003e\n","      \u003ctd\u003eThis is a very good project so don't miss to j...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1191\u003c/th\u003e\n","      \u003ctd\u003e319\u003c/td\u003e\n","      \u003ctd\u003eWhoever didn’t panic sell we made it 🤝🎉 BTC\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1192\u003c/th\u003e\n","      \u003ctd\u003e896\u003c/td\u003e\n","      \u003ctd\u003eTo know more about IPOs and Unlisted shares, c...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1193\u003c/th\u003e\n","      \u003ctd\u003e425\u003c/td\u003e\n","      \u003ctd\u003eSoon to the moon 🌜   Unitycol $Unity Unityprot...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1194\u003c/th\u003e\n","      \u003ctd\u003e153\u003c/td\u003e\n","      \u003ctd\u003eBitcoin Price     $48665.02     $37554.85     ...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e1195 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de35d9f2-b08a-40cc-9633-f48a8d61cc75')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-de35d9f2-b08a-40cc-9633-f48a8d61cc75 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-de35d9f2-b08a-40cc-9633-f48a8d61cc75');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text     label  \\\n","0             71  Full thanks to  as they have analysed this Bit...  Positive   \n","1            266             Typing mistake for Satoshi not bitcoin   Neutral   \n","2            410  Saitama SaitamaWolfPack SaitamaInu SaitaMask $...   Neutral   \n","3            333  100%, the nuclear industry has no idea what is...   Neutral   \n","4            401  airdrop Airdrops Crypto BSC DeFi Polygon Bitco...   Neutral   \n","...          ...                                                ...       ...   \n","1190         171  This is a very good project so don't miss to j...  Positive   \n","1191         319        Whoever didn’t panic sell we made it 🤝🎉 BTC  Positive   \n","1192         896  To know more about IPOs and Unlisted shares, c...   Neutral   \n","1193         425  Soon to the moon 🌜   Unitycol $Unity Unityprot...  Positive   \n","1194         153  Bitcoin Price     $48665.02     $37554.85     ...   Neutral   \n","\n","                source quality  \n","0     incomplete_valid    high  \n","1     incomplete_valid    high  \n","2     incomplete_valid    high  \n","3        new_turk_high    high  \n","4          self_manual    high  \n","...                ...     ...  \n","1190     new_turk_high    high  \n","1191     new_turk_high    high  \n","1192     new_turk_high    high  \n","1193  incomplete_valid    high  \n","1194     new_turk_high    high  \n","\n","[1195 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-15f19765-c62c-4092-9a08-fbce96661b0c\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e747\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e301\u003c/td\u003e\n","      \u003ctd\u003e__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003e758\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e15\u003c/th\u003e\n","      \u003ctd\u003e285\u003c/td\u003e\n","      \u003ctd\u003eBTC DOMINANCE  BULLISH PENNANT READY FOR BREAKOUT\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003emanual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e25\u003c/th\u003e\n","      \u003ctd\u003e530\u003c/td\u003e\n","      \u003ctd\u003eMining Farm Turning Waste Coal Into Bitcoin Ra...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2762\u003c/th\u003e\n","      \u003ctd\u003e11\u003c/td\u003e\n","      \u003ctd\u003eBitcoin interoperability platform Interlay rai...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2772\u003c/th\u003e\n","      \u003ctd\u003e426\u003c/td\u003e\n","      \u003ctd\u003e🇧🇷 BRL - R$ 0.00238059 🇺🇸 USD - $ 0.00042746 🇪...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2775\u003c/th\u003e\n","      \u003ctd\u003e652\u003c/td\u003e\n","      \u003ctd\u003eCurrent Bitcoin Price is $40714 BTC Crypto\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2780\u003c/th\u003e\n","      \u003ctd\u003e140\u003c/td\u003e\n","      \u003ctd\u003ebtc Crypto  cryptocurrency   Target 1 close.  ...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2783\u003c/th\u003e\n","      \u003ctd\u003e330\u003c/td\u003e\n","      \u003ctd\u003eCBNEWS WEEKLY CRYPTO WRAPUP!!!  ZILLIQA AIR DR...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e539 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15f19765-c62c-4092-9a08-fbce96661b0c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-15f19765-c62c-4092-9a08-fbce96661b0c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-15f19765-c62c-4092-9a08-fbce96661b0c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text     label  \\\n","1            747  Full thanks to  as they have analysed this Bit...   Neutral   \n","2            301  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...   Neutral   \n","6            758  Full thanks to  as they have analysed this Bit...   Neutral   \n","15           285  BTC DOMINANCE  BULLISH PENNANT READY FOR BREAKOUT  Positive   \n","25           530  Mining Farm Turning Waste Coal Into Bitcoin Ra...  Positive   \n","...          ...                                                ...       ...   \n","2762          11  Bitcoin interoperability platform Interlay rai...  Positive   \n","2772         426  🇧🇷 BRL - R$ 0.00238059 🇺🇸 USD - $ 0.00042746 🇪...   Neutral   \n","2775         652         Current Bitcoin Price is $40714 BTC Crypto   Neutral   \n","2780         140  btc Crypto  cryptocurrency   Target 1 close.  ...  Positive   \n","2783         330  CBNEWS WEEKLY CRYPTO WRAPUP!!!  ZILLIQA AIR DR...   Neutral   \n","\n","             source quality  \n","1     new_turk_high    high  \n","2       self_manual    high  \n","6     new_turk_high    high  \n","15           manual    high  \n","25    new_turk_high    high  \n","...             ...     ...  \n","2762  new_turk_high    high  \n","2772    self_manual    high  \n","2775  new_turk_high    high  \n","2780  new_turk_high    high  \n","2783    self_manual    high  \n","\n","[539 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-7dec03c1-3cb9-43a5-97dd-5f738310b138\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e339\u003c/td\u003e\n","      \u003ctd\u003eКриптоВести BTC 🤘  Blockasset Taps Well-Known ...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1743\u003c/td\u003e\n","      \u003ctd\u003e$btc btc health check! No custom death cross; ...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e1108\u003c/td\u003e\n","      \u003ctd\u003e44 🏦FTX | BTC PERP  🦋 If you have trouble imag...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003e103\u003c/td\u003e\n","      \u003ctd\u003eBitcoin maxis are born in bear markets\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003e1768\u003c/td\u003e\n","      \u003ctd\u003eBTC has risen by 9.19% in the last week. The p...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2785\u003c/th\u003e\n","      \u003ctd\u003e683\u003c/td\u003e\n","      \u003ctd\u003eThis will be very useful in the future because...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2786\u003c/th\u003e\n","      \u003ctd\u003e1451\u003c/td\u003e\n","      \u003ctd\u003eAnd without Bitcoin Maxies hyping Bitcoin , th...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2787\u003c/th\u003e\n","      \u003ctd\u003e1106\u003c/td\u003e\n","      \u003ctd\u003e11 IST   88.724  37996.605 ₿  3371120.946  253...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2788\u003c/th\u003e\n","      \u003ctd\u003e1787\u003c/td\u003e\n","      \u003ctd\u003eBitcoin is Liberation Network.\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2789\u003c/th\u003e\n","      \u003ctd\u003e344\u003c/td\u003e\n","      \u003ctd\u003ebitcoin is the best MONEY in the history of ma...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e2251 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dec03c1-3cb9-43a5-97dd-5f738310b138')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-7dec03c1-3cb9-43a5-97dd-5f738310b138 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7dec03c1-3cb9-43a5-97dd-5f738310b138');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text     label  \\\n","0            339  КриптоВести BTC 🤘  Blockasset Taps Well-Known ...  Positive   \n","3           1743  $btc btc health check! No custom death cross; ...   Neutral   \n","4           1108  44 🏦FTX | BTC PERP  🦋 If you have trouble imag...   Neutral   \n","5            103             Bitcoin maxis are born in bear markets  Positive   \n","7           1768  BTC has risen by 9.19% in the last week. The p...   Neutral   \n","...          ...                                                ...       ...   \n","2785         683  This will be very useful in the future because...  Positive   \n","2786        1451  And without Bitcoin Maxies hyping Bitcoin , th...  Positive   \n","2787        1106  11 IST   88.724  37996.605 ₿  3371120.946  253...  Positive   \n","2788        1787                     Bitcoin is Liberation Network.  Positive   \n","2789         344  bitcoin is the best MONEY in the history of ma...   Neutral   \n","\n","            source quality  \n","0     new_turk_low     low  \n","3     new_turk_low     low  \n","4     new_turk_low     low  \n","5     new_turk_low     low  \n","7     new_turk_low     low  \n","...            ...     ...  \n","2785  new_turk_low     low  \n","2786  new_turk_low     low  \n","2787  new_turk_low     low  \n","2788  new_turk_low     low  \n","2789  new_turk_low     low  \n","\n","[2251 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["train_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_3.0:4.0:4.0/train_dataset.csv'\n","valid_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_3.0:4.0:4.0/valid_dataset.csv'\n","#model_name = \"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\"\n","#out_dir = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/tomas23_3:4:4/with_high'\n","#out_dir2 = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/tomas23_3:4:4/with_high_then_low'\n","\n","import shutil\n","\n","\n","\n","\n","\n","name_of_text_column = \"text\"\n","hyperparameter_tuning = False\n","reduce_ = False\n","num_hyper_trails = 100\n","epoch = 60\n","\n","df_train = pd.read_csv(train_path)\n","\n","df_train_high = df_train[df_train['quality']==\"high\"]\n","df_train_low = df_train[df_train['quality']==\"low\"]\n","\n","df_valid = pd.read_csv(valid_path)\n","\n","display(df_train)\n","display(df_valid)\n","display(df_train_high)\n","display(df_train_low)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1659728984816,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"GoIvErk1T_LX","outputId":"a3167e79-acd3-443d-9018-2c6f0c9a8ec2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"]},{"data":{"text/html":["\n","  \u003cdiv id=\"df-d1aa4c09-365b-4a2b-8fa4-0d86762cb0db\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e339\u003c/td\u003e\n","      \u003ctd\u003eКриптоВести BTC 🤘  Blockasset Taps Well-Known ...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e747\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e301\u003c/td\u003e\n","      \u003ctd\u003e__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1743\u003c/td\u003e\n","      \u003ctd\u003e$btc btc health check! No custom death cross; ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e1108\u003c/td\u003e\n","      \u003ctd\u003e44 🏦FTX | BTC PERP  🦋 If you have trouble imag...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2785\u003c/th\u003e\n","      \u003ctd\u003e683\u003c/td\u003e\n","      \u003ctd\u003eThis will be very useful in the future because...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2786\u003c/th\u003e\n","      \u003ctd\u003e1451\u003c/td\u003e\n","      \u003ctd\u003eAnd without Bitcoin Maxies hyping Bitcoin , th...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2787\u003c/th\u003e\n","      \u003ctd\u003e1106\u003c/td\u003e\n","      \u003ctd\u003e11 IST   88.724  37996.605 ₿  3371120.946  253...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2788\u003c/th\u003e\n","      \u003ctd\u003e1787\u003c/td\u003e\n","      \u003ctd\u003eBitcoin is Liberation Network.\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2789\u003c/th\u003e\n","      \u003ctd\u003e344\u003c/td\u003e\n","      \u003ctd\u003ebitcoin is the best MONEY in the history of ma...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e2790 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1aa4c09-365b-4a2b-8fa4-0d86762cb0db')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-d1aa4c09-365b-4a2b-8fa4-0d86762cb0db button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d1aa4c09-365b-4a2b-8fa4-0d86762cb0db');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text  label  \\\n","0            339  КриптоВести BTC 🤘  Blockasset Taps Well-Known ...      2   \n","1            747  Full thanks to  as they have analysed this Bit...      1   \n","2            301  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...      1   \n","3           1743  $btc btc health check! No custom death cross; ...      1   \n","4           1108  44 🏦FTX | BTC PERP  🦋 If you have trouble imag...      1   \n","...          ...                                                ...    ...   \n","2785         683  This will be very useful in the future because...      2   \n","2786        1451  And without Bitcoin Maxies hyping Bitcoin , th...      2   \n","2787        1106  11 IST   88.724  37996.605 ₿  3371120.946  253...      2   \n","2788        1787                     Bitcoin is Liberation Network.      2   \n","2789         344  bitcoin is the best MONEY in the history of ma...      1   \n","\n","             source quality  \n","0      new_turk_low     low  \n","1     new_turk_high    high  \n","2       self_manual    high  \n","3      new_turk_low     low  \n","4      new_turk_low     low  \n","...             ...     ...  \n","2785   new_turk_low     low  \n","2786   new_turk_low     low  \n","2787   new_turk_low     low  \n","2788   new_turk_low     low  \n","2789   new_turk_low     low  \n","\n","[2790 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-0b5dc3be-e3ec-41e1-96b4-4123edc6abda\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e71\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e266\u003c/td\u003e\n","      \u003ctd\u003eTyping mistake for Satoshi not bitcoin\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e410\u003c/td\u003e\n","      \u003ctd\u003eSaitama SaitamaWolfPack SaitamaInu SaitaMask $...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e333\u003c/td\u003e\n","      \u003ctd\u003e100%, the nuclear industry has no idea what is...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e401\u003c/td\u003e\n","      \u003ctd\u003eairdrop Airdrops Crypto BSC DeFi Polygon Bitco...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1190\u003c/th\u003e\n","      \u003ctd\u003e171\u003c/td\u003e\n","      \u003ctd\u003eThis is a very good project so don't miss to j...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1191\u003c/th\u003e\n","      \u003ctd\u003e319\u003c/td\u003e\n","      \u003ctd\u003eWhoever didn’t panic sell we made it 🤝🎉 BTC\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1192\u003c/th\u003e\n","      \u003ctd\u003e896\u003c/td\u003e\n","      \u003ctd\u003eTo know more about IPOs and Unlisted shares, c...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1193\u003c/th\u003e\n","      \u003ctd\u003e425\u003c/td\u003e\n","      \u003ctd\u003eSoon to the moon 🌜   Unitycol $Unity Unityprot...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003eincomplete_valid\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1194\u003c/th\u003e\n","      \u003ctd\u003e153\u003c/td\u003e\n","      \u003ctd\u003eBitcoin Price     $48665.02     $37554.85     ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e1195 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b5dc3be-e3ec-41e1-96b4-4123edc6abda')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-0b5dc3be-e3ec-41e1-96b4-4123edc6abda button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0b5dc3be-e3ec-41e1-96b4-4123edc6abda');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text  label  \\\n","0             71  Full thanks to  as they have analysed this Bit...      2   \n","1            266             Typing mistake for Satoshi not bitcoin      1   \n","2            410  Saitama SaitamaWolfPack SaitamaInu SaitaMask $...      1   \n","3            333  100%, the nuclear industry has no idea what is...      1   \n","4            401  airdrop Airdrops Crypto BSC DeFi Polygon Bitco...      1   \n","...          ...                                                ...    ...   \n","1190         171  This is a very good project so don't miss to j...      2   \n","1191         319        Whoever didn’t panic sell we made it 🤝🎉 BTC      2   \n","1192         896  To know more about IPOs and Unlisted shares, c...      1   \n","1193         425  Soon to the moon 🌜   Unitycol $Unity Unityprot...      2   \n","1194         153  Bitcoin Price     $48665.02     $37554.85     ...      1   \n","\n","                source quality  \n","0     incomplete_valid    high  \n","1     incomplete_valid    high  \n","2     incomplete_valid    high  \n","3        new_turk_high    high  \n","4          self_manual    high  \n","...                ...     ...  \n","1190     new_turk_high    high  \n","1191     new_turk_high    high  \n","1192     new_turk_high    high  \n","1193  incomplete_valid    high  \n","1194     new_turk_high    high  \n","\n","[1195 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-c25e09d8-1149-47a1-9c04-842f77872daf\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e747\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e301\u003c/td\u003e\n","      \u003ctd\u003e__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003e758\u003c/td\u003e\n","      \u003ctd\u003eFull thanks to  as they have analysed this Bit...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e15\u003c/th\u003e\n","      \u003ctd\u003e285\u003c/td\u003e\n","      \u003ctd\u003eBTC DOMINANCE  BULLISH PENNANT READY FOR BREAKOUT\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003emanual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e25\u003c/th\u003e\n","      \u003ctd\u003e530\u003c/td\u003e\n","      \u003ctd\u003eMining Farm Turning Waste Coal Into Bitcoin Ra...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2762\u003c/th\u003e\n","      \u003ctd\u003e11\u003c/td\u003e\n","      \u003ctd\u003eBitcoin interoperability platform Interlay rai...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2772\u003c/th\u003e\n","      \u003ctd\u003e426\u003c/td\u003e\n","      \u003ctd\u003e🇧🇷 BRL - R$ 0.00238059 🇺🇸 USD - $ 0.00042746 🇪...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2775\u003c/th\u003e\n","      \u003ctd\u003e652\u003c/td\u003e\n","      \u003ctd\u003eCurrent Bitcoin Price is $40714 BTC Crypto\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2780\u003c/th\u003e\n","      \u003ctd\u003e140\u003c/td\u003e\n","      \u003ctd\u003ebtc Crypto  cryptocurrency   Target 1 close.  ...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_high\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2783\u003c/th\u003e\n","      \u003ctd\u003e330\u003c/td\u003e\n","      \u003ctd\u003eCBNEWS WEEKLY CRYPTO WRAPUP!!!  ZILLIQA AIR DR...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003eself_manual\u003c/td\u003e\n","      \u003ctd\u003ehigh\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e539 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c25e09d8-1149-47a1-9c04-842f77872daf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-c25e09d8-1149-47a1-9c04-842f77872daf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c25e09d8-1149-47a1-9c04-842f77872daf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text  label  \\\n","1            747  Full thanks to  as they have analysed this Bit...      1   \n","2            301  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...      1   \n","6            758  Full thanks to  as they have analysed this Bit...      1   \n","15           285  BTC DOMINANCE  BULLISH PENNANT READY FOR BREAKOUT      2   \n","25           530  Mining Farm Turning Waste Coal Into Bitcoin Ra...      2   \n","...          ...                                                ...    ...   \n","2762          11  Bitcoin interoperability platform Interlay rai...      2   \n","2772         426  🇧🇷 BRL - R$ 0.00238059 🇺🇸 USD - $ 0.00042746 🇪...      1   \n","2775         652         Current Bitcoin Price is $40714 BTC Crypto      1   \n","2780         140  btc Crypto  cryptocurrency   Target 1 close.  ...      2   \n","2783         330  CBNEWS WEEKLY CRYPTO WRAPUP!!!  ZILLIQA AIR DR...      1   \n","\n","             source quality  \n","1     new_turk_high    high  \n","2       self_manual    high  \n","6     new_turk_high    high  \n","15           manual    high  \n","25    new_turk_high    high  \n","...             ...     ...  \n","2762  new_turk_high    high  \n","2772    self_manual    high  \n","2775  new_turk_high    high  \n","2780  new_turk_high    high  \n","2783    self_manual    high  \n","\n","[539 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-22c4f5ce-311f-4bd5-ac7f-6c3c60f31fc0\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003esource\u003c/th\u003e\n","      \u003cth\u003equality\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e339\u003c/td\u003e\n","      \u003ctd\u003eКриптоВести BTC 🤘  Blockasset Taps Well-Known ...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e1743\u003c/td\u003e\n","      \u003ctd\u003e$btc btc health check! No custom death cross; ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e1108\u003c/td\u003e\n","      \u003ctd\u003e44 🏦FTX | BTC PERP  🦋 If you have trouble imag...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003e103\u003c/td\u003e\n","      \u003ctd\u003eBitcoin maxis are born in bear markets\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003e1768\u003c/td\u003e\n","      \u003ctd\u003eBTC has risen by 9.19% in the last week. The p...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2785\u003c/th\u003e\n","      \u003ctd\u003e683\u003c/td\u003e\n","      \u003ctd\u003eThis will be very useful in the future because...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2786\u003c/th\u003e\n","      \u003ctd\u003e1451\u003c/td\u003e\n","      \u003ctd\u003eAnd without Bitcoin Maxies hyping Bitcoin , th...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2787\u003c/th\u003e\n","      \u003ctd\u003e1106\u003c/td\u003e\n","      \u003ctd\u003e11 IST   88.724  37996.605 ₿  3371120.946  253...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2788\u003c/th\u003e\n","      \u003ctd\u003e1787\u003c/td\u003e\n","      \u003ctd\u003eBitcoin is Liberation Network.\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2789\u003c/th\u003e\n","      \u003ctd\u003e344\u003c/td\u003e\n","      \u003ctd\u003ebitcoin is the best MONEY in the history of ma...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003enew_turk_low\u003c/td\u003e\n","      \u003ctd\u003elow\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e2251 rows × 5 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22c4f5ce-311f-4bd5-ac7f-6c3c60f31fc0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-22c4f5ce-311f-4bd5-ac7f-6c3c60f31fc0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-22c4f5ce-311f-4bd5-ac7f-6c3c60f31fc0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["      Unnamed: 0                                               text  label  \\\n","0            339  КриптоВести BTC 🤘  Blockasset Taps Well-Known ...      2   \n","3           1743  $btc btc health check! No custom death cross; ...      1   \n","4           1108  44 🏦FTX | BTC PERP  🦋 If you have trouble imag...      1   \n","5            103             Bitcoin maxis are born in bear markets      2   \n","7           1768  BTC has risen by 9.19% in the last week. The p...      1   \n","...          ...                                                ...    ...   \n","2785         683  This will be very useful in the future because...      2   \n","2786        1451  And without Bitcoin Maxies hyping Bitcoin , th...      2   \n","2787        1106  11 IST   88.724  37996.605 ₿  3371120.946  253...      2   \n","2788        1787                     Bitcoin is Liberation Network.      2   \n","2789         344  bitcoin is the best MONEY in the history of ma...      1   \n","\n","            source quality  \n","0     new_turk_low     low  \n","3     new_turk_low     low  \n","4     new_turk_low     low  \n","5     new_turk_low     low  \n","7     new_turk_low     low  \n","...            ...     ...  \n","2785  new_turk_low     low  \n","2786  new_turk_low     low  \n","2787  new_turk_low     low  \n","2788  new_turk_low     low  \n","2789  new_turk_low     low  \n","\n","[2251 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df_train['label'] = df_train['label'].apply(ConvertLabel2ModelLabel)\n","df_valid['label'] = df_valid['label'].apply(ConvertLabel2ModelLabel)\n","\n","df_train_high['label'] = df_train_high['label'].apply(ConvertLabel2ModelLabel)\n","df_train_low['label'] = df_train_low['label'].apply(ConvertLabel2ModelLabel)\n","\n","display(df_train)\n","display(df_valid)\n","display(df_train_high)\n","display(df_train_low)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1659728985297,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"SG6S7inA_RKv"},"outputs":[],"source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","#learning_rate = 5e-05\n","#batch_size = 16\n","#eval_batch_size = 4\n","seed = 40\n","#optimizer = Adam \n","#with betas=(0.9,0.999) and epsilon=1e-08\n","adam_beta1 = 0.9\n","adam_beta2 =0.999\n","lr_scheduler_type = \"linear\"\n","num_epochs = 15\n","#args = TrainingArguments(\"test_trainer\",report_to=\"wandb\" ,logging_strategy = \"epoch\",evaluation_strategy=\"epoch\",learning_rate = learning_rate,num_train_epochs = num_epochs,lr_scheduler_type =lr_scheduler_type, adam_beta1 = adam_beta1,adam_beta2 =adam_beta2  )\n","\n","\n","def run_trainer(model_name,out_dir,epoch,batch_size,df_train,df_valid,eval_bool):\n","  df_train = df_train[['text','label']].set_index('text')\n","  df_valid = df_valid[['text','label']].set_index('text')\n","  #display(df_train)\n","  #display(df_valid)\n","\n","  dataset = ds.dataset(pa.Table.from_pandas(df_valid).to_batches())\n","  ### convert to Huggingface dataset\n","  validation_dataset_torch = Dataset(pa.Table.from_pandas(df_valid))\n","\n","  dataset = ds.dataset(pa.Table.from_pandas(df_train).to_batches())\n","  ### convert to Huggingface dataset\n","  training_dataset_torch = Dataset(pa.Table.from_pandas(df_train))\n","\n","  #print(training_dataset_torch)\n","  #print(validation_dataset_torch)\n","\n","  from transformers import AutoTokenizer\n","\n","  tokenizer = AutoTokenizer.from_pretrained(model_name,model_max_length=512)\n","\n","\n","\n","  def tokenize_function(data):\n","      return tokenizer(data['text'], padding=\"max_length\", truncation=True,)\n","\n","\n","  train_dataset = training_dataset_torch.map(tokenize_function, batched=True)\n","  eval_dataset = validation_dataset_torch.map(tokenize_function, batched=True)\n","  print(train_dataset)\n","  print(eval_dataset)\n","\n","  import numpy as np\n","  from datasets import load_metric\n","\n","  metric = load_metric(\"matthews_correlation\")\n","\n","  def compute_metrics(eval_pred):\n","      logits, labels = eval_pred\n","      predictions = np.argmax(logits, axis=-1)\n","      return metric.compute(predictions=predictions, references=labels)\n","\n","\n","  from transformers import AutoModelForSequenceClassification\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","  #model.to(device)\n","  args = TrainingArguments(\n","      #'/content/drive/MyDrive/fyp/fyp2/model/model2-supervised/' + f\"{model_name}-finetuned-\",\n","      #'/content/' + f\"{model_name}-finetuned-\",\n","      out_dir,\n","      #report_to=\"wandb\",\n","      overwrite_output_dir = True,\n","      logging_dir = out_dir,\n","      evaluation_strategy = \"epoch\",\n","      logging_strategy = \"epoch\",\n","      #save_strategy = \"NO\",\n","      learning_rate=2.6510704963161386e-06,\n","      per_device_train_batch_size=batch_size,\n","      per_device_eval_batch_size=batch_size,\n","      num_train_epochs=epoch,\n","      weight_decay=0.01,\n","      metric_for_best_model='matthews_correlation',\n","      save_total_limit = 2,\n","      save_strategy = \"epoch\",\n","      load_best_model_at_end = True,\n","      seed = 18\n","      #push_to_hub=True,\n","  )\n","\n","\n","\n","  trainer = Trainer(\n","      model,\n","      args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics\n","  )\n","\n","  #trainer = Trainer(\n","    # model=model,\n","      #args=args,\n","    # train_dataset=train_dataset,\n","    #  eval_dataset=eval_dataset,\n","  #   compute_metrics=compute_metrics,\n","  #)\n","\n","  \n","  if eval_bool:\n","    eval = trainer.evaluate()\n","    #print(\"bob\")\n","    #print(eval)\n","    \n","    # training\n","  else:\n","    train_result = trainer.train() \n","    logs = trainer.state.log_history\n","    # compute train results\n","    metrics = train_result.metrics\n","    max_train_samples = len(train_dataset)\n","    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","    # save train results\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","\n","    # compute evaluation results\n","    metrics = trainer.evaluate()\n","    max_val_samples = len(eval_dataset)\n","    metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n","\n","    # save evaluation results\n","    trainer.log_metrics(\"eval\", metrics)\n","    trainer.save_metrics(\"eval\", metrics)\n","    import pickle\n","    save_logs_dir = out_dir\n","    name_logs = \"logs\"\n","    \n","    my_file = save_logs_dir + \"/\" + name_logs + \".p\"\n","    print(\"saving to \",my_file )\n","\n","    pickle.dump( logs, open( my_file, \"wb\" )    ) \n","\n","    ld = pickle.load( open( my_file, \"rb\" ) )\n","\n","\n","    print(logs)\n","    print(logs[2])\n","    print(type(logs))\n","\n","    print(ld)\n","    print(type(ld))\n","    print(logs == ld)\n","    eval = trainer.evaluate()\n","    \n","  import torch\n","  torch.cuda.is_available() \n","  from numba import cuda\n","  del trainer,args\n","  torch.cuda.empty_cache()\n","  #trainer.evaluate()\n","  return eval\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1659728985821,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"bMAFQ9PVu5tY"},"outputs":[],"source":["\n","  #run_trainer(model_name=\"/content/temp/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-102\",out_dir = out,epoch =epoch,df_train = df_train_high,df_valid=df_valid,batch_size=batch_size,eval_bool=True)\n","\n","def run_both_training(model_name,out_dir=\"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models\",run_low = False,batch_size = 16,df_valid = df_valid):\n","  print(\"starting \" ,model_name,\" at\" , \"run_low is\" , run_low)\n","  out = out_dir + \"/\" + model_name + \"/high\"\n","  epoch = 40\n","  import os\n","  if run_low:\n","    import os\n","    for root, dirs, files in os.walk(out):\n","      break\n","    #find best checkpoint\n","    max_mcc = -1.1\n","    for models in dirs:\n","      if models.startswith('checkpoint-'):\n","        #print(models)\n","        eval_ = run_trainer(model_name=out+\"/\"+models,out_dir = out,epoch =epoch,df_train = df_train_low,df_valid=df_valid,batch_size=batch_size,eval_bool=True)\n","        new_max = eval_.get(\"eval_matthews_correlation\")\n","        if  new_max \u003e max_mcc:\n","          max_mcc = new_max\n","          best_models = models\n","        torch.cuda.empty_cache()\n","\n","    model_name_for_low = out+\"/\"+best_models\n","    out = out_dir + \"/\" + model_name + \"/high_then_low-\" + best_models\n","    if not os.path.exists(out):\n","      \n","      # Create a new directory because it does not exist \n","      os.makedirs(out)\n","    else:\n","      shutil.rmtree(out)\n","    print(\"running on checkpoint \",model_name_for_low)\n","    run_trainer(model_name=model_name_for_low,out_dir = out,epoch =epoch,df_train = df_train_low,df_valid=df_valid,batch_size=batch_size,eval_bool=False)\n","    torch.cuda.empty_cache()\n","    print(model_name,\" is completed for low\")\n","\n","  else:\n","    if not os.path.exists(out):\n","      \n","      # Create a new directory because it does not exist \n","      os.makedirs(out)\n","    else:\n","      shutil.rmtree(out)\n","\n","    run_trainer(model_name=model_name,out_dir = out,epoch =epoch,df_train = df_train_high,df_valid=df_valid,batch_size=batch_size,eval_bool = False)\n","    torch.cuda.empty_cache()\n","    print(model_name,\" is completed for high\")\n","    #out = out_dir + \"/\" + model_name + \"/high_then_low\"\n","    #run_trainer(model_name=model_name,out_dir= out,epoch=50,df_train = df_train_low,df_valid=df_valid)\n","\n","#run_both_training(model_name =\"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\" ,out_dir =\"/content/temp\",run_low = False)\n","#run_both_training(model_name =\"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\" ,out_dir =\"/content/temp\",run_low = True)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1659728985824,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"FK_NHP6ZyFIH"},"outputs":[],"source":["\n","#run_both_training(\"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\")\n","\n","\n","#run_both_training(\"cardiffnlp/bertweet-base-sentiment\")\n","#run_both_training(\"svalabs/twitter-xlm-roberta-bitcoin-sentiment\")\n","\n","#run_both_training(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n","#run_both_training(\"amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061\")\n","\n","#run_both_training(\"finiteautomata/bertweet-base-sentiment-analysis\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"QVZ1Wxg77L8g"},"outputs":[{"name":"stdout","output_type":"stream","text":["starting  Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment  at run_low is True\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf4e829dedfe48a78f547c35117a34ea","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51b9b3487f8b4fa38b5cfd09a6d2dac7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c445fba869a5411a868e1e4aba825a5b","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.71k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:21]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Didn't find file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/added_tokens.json. We won't load it.\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/vocab.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/merges.txt\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/tokenizer.json\n","loading file None\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/special_tokens_map.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/tokenizer_config.json\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f454d09910ec46019bd1e38640bf9144","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c836d4b35b34d3b8a573144eb48f742","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"negative\",\n","    \"1\": \"neutral\",\n","    \"2\": \"positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"negative\": 0,\n","    \"neutral\": 1,\n","    \"positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-1700.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:21]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Didn't find file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/added_tokens.json. We won't load it.\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/vocab.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/merges.txt\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/tokenizer.json\n","loading file None\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/special_tokens_map.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/tokenizer_config.json\n"]},{"name":"stdout","output_type":"stream","text":["running on checkpoint  /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca57b0d5fea24bbc90654f688e4db3b1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"663f02df8c244004b3c355ff8a926ebd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"negative\",\n","    \"1\": \"neutral\",\n","    \"2\": \"positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"negative\": 0,\n","    \"neutral\": 1,\n","    \"positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high/checkpoint-816.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 2251\n","  Num Epochs = 40\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5640\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='5640' max='5640' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [5640/5640 1:46:59, Epoch 40/40]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMatthews Correlation\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e1.310500\u003c/td\u003e\n","      \u003ctd\u003e0.790382\u003c/td\u003e\n","      \u003ctd\u003e0.375759\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.893800\u003c/td\u003e\n","      \u003ctd\u003e0.726976\u003c/td\u003e\n","      \u003ctd\u003e0.441899\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.839400\u003c/td\u003e\n","      \u003ctd\u003e0.759162\u003c/td\u003e\n","      \u003ctd\u003e0.439071\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e0.810700\u003c/td\u003e\n","      \u003ctd\u003e0.766757\u003c/td\u003e\n","      \u003ctd\u003e0.433589\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e0.772800\u003c/td\u003e\n","      \u003ctd\u003e0.741574\u003c/td\u003e\n","      \u003ctd\u003e0.451169\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6\u003c/td\u003e\n","      \u003ctd\u003e0.745900\u003c/td\u003e\n","      \u003ctd\u003e0.746552\u003c/td\u003e\n","      \u003ctd\u003e0.435622\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e7\u003c/td\u003e\n","      \u003ctd\u003e0.693000\u003c/td\u003e\n","      \u003ctd\u003e0.761767\u003c/td\u003e\n","      \u003ctd\u003e0.447374\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e8\u003c/td\u003e\n","      \u003ctd\u003e0.670700\u003c/td\u003e\n","      \u003ctd\u003e0.798372\u003c/td\u003e\n","      \u003ctd\u003e0.445268\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e9\u003c/td\u003e\n","      \u003ctd\u003e0.649200\u003c/td\u003e\n","      \u003ctd\u003e0.874594\u003c/td\u003e\n","      \u003ctd\u003e0.374655\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e10\u003c/td\u003e\n","      \u003ctd\u003e0.598200\u003c/td\u003e\n","      \u003ctd\u003e0.846946\u003c/td\u003e\n","      \u003ctd\u003e0.326138\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e11\u003c/td\u003e\n","      \u003ctd\u003e0.566800\u003c/td\u003e\n","      \u003ctd\u003e0.942509\u003c/td\u003e\n","      \u003ctd\u003e0.322673\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e12\u003c/td\u003e\n","      \u003ctd\u003e0.531400\u003c/td\u003e\n","      \u003ctd\u003e0.963336\u003c/td\u003e\n","      \u003ctd\u003e0.304035\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e13\u003c/td\u003e\n","      \u003ctd\u003e0.503500\u003c/td\u003e\n","      \u003ctd\u003e1.002439\u003c/td\u003e\n","      \u003ctd\u003e0.308737\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e14\u003c/td\u003e\n","      \u003ctd\u003e0.479500\u003c/td\u003e\n","      \u003ctd\u003e1.165354\u003c/td\u003e\n","      \u003ctd\u003e0.281635\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e15\u003c/td\u003e\n","      \u003ctd\u003e0.442200\u003c/td\u003e\n","      \u003ctd\u003e1.106214\u003c/td\u003e\n","      \u003ctd\u003e0.309374\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e16\u003c/td\u003e\n","      \u003ctd\u003e0.416500\u003c/td\u003e\n","      \u003ctd\u003e1.309296\u003c/td\u003e\n","      \u003ctd\u003e0.263661\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e17\u003c/td\u003e\n","      \u003ctd\u003e0.396500\u003c/td\u003e\n","      \u003ctd\u003e1.230315\u003c/td\u003e\n","      \u003ctd\u003e0.299415\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e18\u003c/td\u003e\n","      \u003ctd\u003e0.373900\u003c/td\u003e\n","      \u003ctd\u003e1.260384\u003c/td\u003e\n","      \u003ctd\u003e0.272167\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e19\u003c/td\u003e\n","      \u003ctd\u003e0.350700\u003c/td\u003e\n","      \u003ctd\u003e1.391928\u003c/td\u003e\n","      \u003ctd\u003e0.262114\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e20\u003c/td\u003e\n","      \u003ctd\u003e0.326600\u003c/td\u003e\n","      \u003ctd\u003e1.449542\u003c/td\u003e\n","      \u003ctd\u003e0.260404\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e21\u003c/td\u003e\n","      \u003ctd\u003e0.307300\u003c/td\u003e\n","      \u003ctd\u003e1.374454\u003c/td\u003e\n","      \u003ctd\u003e0.279612\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e22\u003c/td\u003e\n","      \u003ctd\u003e0.302300\u003c/td\u003e\n","      \u003ctd\u003e1.397511\u003c/td\u003e\n","      \u003ctd\u003e0.278886\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003e0.287500\u003c/td\u003e\n","      \u003ctd\u003e1.511902\u003c/td\u003e\n","      \u003ctd\u003e0.272783\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e24\u003c/td\u003e\n","      \u003ctd\u003e0.258200\u003c/td\u003e\n","      \u003ctd\u003e1.435661\u003c/td\u003e\n","      \u003ctd\u003e0.280048\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e25\u003c/td\u003e\n","      \u003ctd\u003e0.247600\u003c/td\u003e\n","      \u003ctd\u003e1.655039\u003c/td\u003e\n","      \u003ctd\u003e0.274022\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e26\u003c/td\u003e\n","      \u003ctd\u003e0.241300\u003c/td\u003e\n","      \u003ctd\u003e1.556638\u003c/td\u003e\n","      \u003ctd\u003e0.273982\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e27\u003c/td\u003e\n","      \u003ctd\u003e0.235300\u003c/td\u003e\n","      \u003ctd\u003e1.697975\u003c/td\u003e\n","      \u003ctd\u003e0.255080\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e28\u003c/td\u003e\n","      \u003ctd\u003e0.221600\u003c/td\u003e\n","      \u003ctd\u003e1.743649\u003c/td\u003e\n","      \u003ctd\u003e0.258859\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e29\u003c/td\u003e\n","      \u003ctd\u003e0.214900\u003c/td\u003e\n","      \u003ctd\u003e1.731139\u003c/td\u003e\n","      \u003ctd\u003e0.257341\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e30\u003c/td\u003e\n","      \u003ctd\u003e0.212500\u003c/td\u003e\n","      \u003ctd\u003e1.657153\u003c/td\u003e\n","      \u003ctd\u003e0.273386\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e31\u003c/td\u003e\n","      \u003ctd\u003e0.205700\u003c/td\u003e\n","      \u003ctd\u003e1.715039\u003c/td\u003e\n","      \u003ctd\u003e0.269342\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e32\u003c/td\u003e\n","      \u003ctd\u003e0.187900\u003c/td\u003e\n","      \u003ctd\u003e1.748952\u003c/td\u003e\n","      \u003ctd\u003e0.269073\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e33\u003c/td\u003e\n","      \u003ctd\u003e0.185000\u003c/td\u003e\n","      \u003ctd\u003e1.788467\u003c/td\u003e\n","      \u003ctd\u003e0.263988\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e34\u003c/td\u003e\n","      \u003ctd\u003e0.190000\u003c/td\u003e\n","      \u003ctd\u003e1.866952\u003c/td\u003e\n","      \u003ctd\u003e0.258539\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e35\u003c/td\u003e\n","      \u003ctd\u003e0.184000\u003c/td\u003e\n","      \u003ctd\u003e1.880684\u003c/td\u003e\n","      \u003ctd\u003e0.259092\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e36\u003c/td\u003e\n","      \u003ctd\u003e0.177100\u003c/td\u003e\n","      \u003ctd\u003e1.875802\u003c/td\u003e\n","      \u003ctd\u003e0.262534\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e37\u003c/td\u003e\n","      \u003ctd\u003e0.176500\u003c/td\u003e\n","      \u003ctd\u003e1.876428\u003c/td\u003e\n","      \u003ctd\u003e0.266337\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e38\u003c/td\u003e\n","      \u003ctd\u003e0.170500\u003c/td\u003e\n","      \u003ctd\u003e1.880477\u003c/td\u003e\n","      \u003ctd\u003e0.271227\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e39\u003c/td\u003e\n","      \u003ctd\u003e0.151900\u003c/td\u003e\n","      \u003ctd\u003e1.856809\u003c/td\u003e\n","      \u003ctd\u003e0.269941\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e40\u003c/td\u003e\n","      \u003ctd\u003e0.167500\u003c/td\u003e\n","      \u003ctd\u003e1.867021\u003c/td\u003e\n","      \u003ctd\u003e0.268275\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-141] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-423] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-282] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-564] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-846] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-987] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1128] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1269] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1410] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1551] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1692] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1833] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-1974] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2115] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2256] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2397] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2538] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2679] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2820] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-2961] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3102] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3243] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3384] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3525] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3666] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3807] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-3948] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4089] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4230] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4371] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4512] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4653] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4794] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-4935] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5076] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5217] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5358] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5640\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5640/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5640/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5640/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5640/special_tokens_map.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-5499] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/checkpoint-705 (score: 0.45116853352689895).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       40.0\n","  total_flos               = 22063713GF\n","  train_loss               =     0.4174\n","  train_runtime            = 1:47:00.46\n","  train_samples            =       2251\n","  train_samples_per_second =     14.024\n","  train_steps_per_second   =      0.878\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='150' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:44]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                     =       40.0\n","  eval_loss                 =     0.7416\n","  eval_matthews_correlation =     0.4512\n","  eval_runtime              = 0:00:22.31\n","  eval_samples              =       1195\n","  eval_samples_per_second   =     53.551\n","  eval_steps_per_second     =      3.361\n","saving to  /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment/high_then_low-checkpoint-816/logs.p\n","[{'loss': 1.3105, 'learning_rate': 2.584793733908235e-06, 'epoch': 1.0, 'step': 141}, {'eval_loss': 0.790382444858551, 'eval_matthews_correlation': 0.37575911596913475, 'eval_runtime': 22.2693, 'eval_samples_per_second': 53.661, 'eval_steps_per_second': 3.368, 'epoch': 1.0, 'step': 141}, {'loss': 0.8938, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}, {'eval_loss': 0.7269756197929382, 'eval_matthews_correlation': 0.4418994437953655, 'eval_runtime': 22.2811, 'eval_samples_per_second': 53.633, 'eval_steps_per_second': 3.366, 'epoch': 2.0, 'step': 282}, {'loss': 0.8394, 'learning_rate': 2.4522402090924283e-06, 'epoch': 3.0, 'step': 423}, {'eval_loss': 0.7591622471809387, 'eval_matthews_correlation': 0.43907144812278764, 'eval_runtime': 22.2737, 'eval_samples_per_second': 53.651, 'eval_steps_per_second': 3.367, 'epoch': 3.0, 'step': 423}, {'loss': 0.8107, 'learning_rate': 2.385963446684525e-06, 'epoch': 4.0, 'step': 564}, {'eval_loss': 0.7667573094367981, 'eval_matthews_correlation': 0.43358925829971584, 'eval_runtime': 22.2819, 'eval_samples_per_second': 53.631, 'eval_steps_per_second': 3.366, 'epoch': 4.0, 'step': 564}, {'loss': 0.7728, 'learning_rate': 2.319686684276621e-06, 'epoch': 5.0, 'step': 705}, {'eval_loss': 0.7415736317634583, 'eval_matthews_correlation': 0.45116853352689895, 'eval_runtime': 22.2682, 'eval_samples_per_second': 53.664, 'eval_steps_per_second': 3.368, 'epoch': 5.0, 'step': 705}, {'loss': 0.7459, 'learning_rate': 2.2534099218687176e-06, 'epoch': 6.0, 'step': 846}, {'eval_loss': 0.7465517520904541, 'eval_matthews_correlation': 0.43562229483291337, 'eval_runtime': 22.2972, 'eval_samples_per_second': 53.594, 'eval_steps_per_second': 3.364, 'epoch': 6.0, 'step': 846}, {'loss': 0.693, 'learning_rate': 2.187133159460814e-06, 'epoch': 7.0, 'step': 987}, {'eval_loss': 0.7617669701576233, 'eval_matthews_correlation': 0.4473741450897212, 'eval_runtime': 22.2872, 'eval_samples_per_second': 53.618, 'eval_steps_per_second': 3.365, 'epoch': 7.0, 'step': 987}, {'loss': 0.6707, 'learning_rate': 2.1208563970529108e-06, 'epoch': 8.0, 'step': 1128}, {'eval_loss': 0.7983715534210205, 'eval_matthews_correlation': 0.44526798895309466, 'eval_runtime': 22.2884, 'eval_samples_per_second': 53.615, 'eval_steps_per_second': 3.365, 'epoch': 8.0, 'step': 1128}, {'loss': 0.6492, 'learning_rate': 2.0545796346450073e-06, 'epoch': 9.0, 'step': 1269}, {'eval_loss': 0.8745942711830139, 'eval_matthews_correlation': 0.3746554307565798, 'eval_runtime': 22.2668, 'eval_samples_per_second': 53.667, 'eval_steps_per_second': 3.368, 'epoch': 9.0, 'step': 1269}, {'loss': 0.5982, 'learning_rate': 1.988302872237104e-06, 'epoch': 10.0, 'step': 1410}, {'eval_loss': 0.8469460606575012, 'eval_matthews_correlation': 0.326138166031764, 'eval_runtime': 22.2704, 'eval_samples_per_second': 53.659, 'eval_steps_per_second': 3.368, 'epoch': 10.0, 'step': 1410}, {'loss': 0.5668, 'learning_rate': 1.9220261098292005e-06, 'epoch': 11.0, 'step': 1551}, {'eval_loss': 0.9425092935562134, 'eval_matthews_correlation': 0.32267286500181086, 'eval_runtime': 22.2797, 'eval_samples_per_second': 53.636, 'eval_steps_per_second': 3.366, 'epoch': 11.0, 'step': 1551}, {'loss': 0.5314, 'learning_rate': 1.8557493474212969e-06, 'epoch': 12.0, 'step': 1692}, {'eval_loss': 0.9633356928825378, 'eval_matthews_correlation': 0.304035285760411, 'eval_runtime': 22.2859, 'eval_samples_per_second': 53.621, 'eval_steps_per_second': 3.365, 'epoch': 12.0, 'step': 1692}, {'loss': 0.5035, 'learning_rate': 1.7894725850133937e-06, 'epoch': 13.0, 'step': 1833}, {'eval_loss': 1.0024386644363403, 'eval_matthews_correlation': 0.30873674263563244, 'eval_runtime': 22.3171, 'eval_samples_per_second': 53.546, 'eval_steps_per_second': 3.361, 'epoch': 13.0, 'step': 1833}, {'loss': 0.4795, 'learning_rate': 1.72319582260549e-06, 'epoch': 14.0, 'step': 1974}, {'eval_loss': 1.1653536558151245, 'eval_matthews_correlation': 0.28163452343105727, 'eval_runtime': 22.2659, 'eval_samples_per_second': 53.669, 'eval_steps_per_second': 3.368, 'epoch': 14.0, 'step': 1974}, {'loss': 0.4422, 'learning_rate': 1.6569190601975866e-06, 'epoch': 15.0, 'step': 2115}, {'eval_loss': 1.1062140464782715, 'eval_matthews_correlation': 0.3093738897901685, 'eval_runtime': 22.2775, 'eval_samples_per_second': 53.642, 'eval_steps_per_second': 3.367, 'epoch': 15.0, 'step': 2115}, {'loss': 0.4165, 'learning_rate': 1.5906422977896832e-06, 'epoch': 16.0, 'step': 2256}, {'eval_loss': 1.3092963695526123, 'eval_matthews_correlation': 0.2636610631670115, 'eval_runtime': 22.2743, 'eval_samples_per_second': 53.649, 'eval_steps_per_second': 3.367, 'epoch': 16.0, 'step': 2256}, {'loss': 0.3965, 'learning_rate': 1.5243655353817795e-06, 'epoch': 17.0, 'step': 2397}, {'eval_loss': 1.230315089225769, 'eval_matthews_correlation': 0.29941497662529204, 'eval_runtime': 22.3002, 'eval_samples_per_second': 53.587, 'eval_steps_per_second': 3.363, 'epoch': 17.0, 'step': 2397}, {'loss': 0.3739, 'learning_rate': 1.4580887729738763e-06, 'epoch': 18.0, 'step': 2538}, {'eval_loss': 1.260384202003479, 'eval_matthews_correlation': 0.2721666564900488, 'eval_runtime': 22.3466, 'eval_samples_per_second': 53.476, 'eval_steps_per_second': 3.356, 'epoch': 18.0, 'step': 2538}, {'loss': 0.3507, 'learning_rate': 1.3918120105659727e-06, 'epoch': 19.0, 'step': 2679}, {'eval_loss': 1.3919278383255005, 'eval_matthews_correlation': 0.2621135030679744, 'eval_runtime': 22.2941, 'eval_samples_per_second': 53.602, 'eval_steps_per_second': 3.364, 'epoch': 19.0, 'step': 2679}, {'loss': 0.3266, 'learning_rate': 1.3255352481580693e-06, 'epoch': 20.0, 'step': 2820}, {'eval_loss': 1.4495415687561035, 'eval_matthews_correlation': 0.2604040743888413, 'eval_runtime': 22.2828, 'eval_samples_per_second': 53.629, 'eval_steps_per_second': 3.366, 'epoch': 20.0, 'step': 2820}, {'loss': 0.3073, 'learning_rate': 1.2592584857501659e-06, 'epoch': 21.0, 'step': 2961}, {'eval_loss': 1.3744540214538574, 'eval_matthews_correlation': 0.27961244161619536, 'eval_runtime': 22.3007, 'eval_samples_per_second': 53.586, 'eval_steps_per_second': 3.363, 'epoch': 21.0, 'step': 2961}, {'loss': 0.3023, 'learning_rate': 1.1929817233422624e-06, 'epoch': 22.0, 'step': 3102}, {'eval_loss': 1.3975107669830322, 'eval_matthews_correlation': 0.27888624423897085, 'eval_runtime': 22.2842, 'eval_samples_per_second': 53.625, 'eval_steps_per_second': 3.366, 'epoch': 22.0, 'step': 3102}, {'loss': 0.2875, 'learning_rate': 1.1267049609343588e-06, 'epoch': 23.0, 'step': 3243}, {'eval_loss': 1.51190185546875, 'eval_matthews_correlation': 0.27278254249840445, 'eval_runtime': 22.2814, 'eval_samples_per_second': 53.632, 'eval_steps_per_second': 3.366, 'epoch': 23.0, 'step': 3243}, {'loss': 0.2582, 'learning_rate': 1.0604281985264554e-06, 'epoch': 24.0, 'step': 3384}, {'eval_loss': 1.4356608390808105, 'eval_matthews_correlation': 0.28004767536391345, 'eval_runtime': 22.3067, 'eval_samples_per_second': 53.571, 'eval_steps_per_second': 3.362, 'epoch': 24.0, 'step': 3384}, {'loss': 0.2476, 'learning_rate': 9.94151436118552e-07, 'epoch': 25.0, 'step': 3525}, {'eval_loss': 1.6550387144088745, 'eval_matthews_correlation': 0.274021861988056, 'eval_runtime': 22.287, 'eval_samples_per_second': 53.619, 'eval_steps_per_second': 3.365, 'epoch': 25.0, 'step': 3525}, {'loss': 0.2413, 'learning_rate': 9.278746737106484e-07, 'epoch': 26.0, 'step': 3666}, {'eval_loss': 1.5566381216049194, 'eval_matthews_correlation': 0.27398185938304354, 'eval_runtime': 22.2911, 'eval_samples_per_second': 53.609, 'eval_steps_per_second': 3.365, 'epoch': 26.0, 'step': 3666}, {'loss': 0.2353, 'learning_rate': 8.61597911302745e-07, 'epoch': 27.0, 'step': 3807}, {'eval_loss': 1.6979751586914062, 'eval_matthews_correlation': 0.2550800135500771, 'eval_runtime': 22.2969, 'eval_samples_per_second': 53.595, 'eval_steps_per_second': 3.364, 'epoch': 27.0, 'step': 3807}, {'loss': 0.2216, 'learning_rate': 7.953211488948416e-07, 'epoch': 28.0, 'step': 3948}, {'eval_loss': 1.743648648262024, 'eval_matthews_correlation': 0.2588589368200626, 'eval_runtime': 22.2778, 'eval_samples_per_second': 53.641, 'eval_steps_per_second': 3.367, 'epoch': 28.0, 'step': 3948}, {'loss': 0.2149, 'learning_rate': 7.290443864869382e-07, 'epoch': 29.0, 'step': 4089}, {'eval_loss': 1.7311393022537231, 'eval_matthews_correlation': 0.2573405352678869, 'eval_runtime': 22.3213, 'eval_samples_per_second': 53.536, 'eval_steps_per_second': 3.36, 'epoch': 29.0, 'step': 4089}, {'loss': 0.2125, 'learning_rate': 6.627676240790346e-07, 'epoch': 30.0, 'step': 4230}, {'eval_loss': 1.657152533531189, 'eval_matthews_correlation': 0.27338592003955503, 'eval_runtime': 22.3213, 'eval_samples_per_second': 53.536, 'eval_steps_per_second': 3.36, 'epoch': 30.0, 'step': 4230}, {'loss': 0.2057, 'learning_rate': 5.964908616711312e-07, 'epoch': 31.0, 'step': 4371}, {'eval_loss': 1.7150394916534424, 'eval_matthews_correlation': 0.2693420414193502, 'eval_runtime': 22.3225, 'eval_samples_per_second': 53.534, 'eval_steps_per_second': 3.36, 'epoch': 31.0, 'step': 4371}, {'loss': 0.1879, 'learning_rate': 5.302140992632277e-07, 'epoch': 32.0, 'step': 4512}, {'eval_loss': 1.7489519119262695, 'eval_matthews_correlation': 0.2690731470893474, 'eval_runtime': 22.3011, 'eval_samples_per_second': 53.585, 'eval_steps_per_second': 3.363, 'epoch': 32.0, 'step': 4512}, {'loss': 0.185, 'learning_rate': 4.639373368553242e-07, 'epoch': 33.0, 'step': 4653}, {'eval_loss': 1.7884665727615356, 'eval_matthews_correlation': 0.2639883952700387, 'eval_runtime': 22.3046, 'eval_samples_per_second': 53.577, 'eval_steps_per_second': 3.363, 'epoch': 33.0, 'step': 4653}, {'loss': 0.19, 'learning_rate': 3.976605744474208e-07, 'epoch': 34.0, 'step': 4794}, {'eval_loss': 1.866951584815979, 'eval_matthews_correlation': 0.25853911321652157, 'eval_runtime': 22.2878, 'eval_samples_per_second': 53.617, 'eval_steps_per_second': 3.365, 'epoch': 34.0, 'step': 4794}, {'loss': 0.184, 'learning_rate': 3.313838120395173e-07, 'epoch': 35.0, 'step': 4935}, {'eval_loss': 1.88068425655365, 'eval_matthews_correlation': 0.2590924049174448, 'eval_runtime': 22.3018, 'eval_samples_per_second': 53.583, 'eval_steps_per_second': 3.363, 'epoch': 35.0, 'step': 4935}, {'loss': 0.1771, 'learning_rate': 2.6510704963161385e-07, 'epoch': 36.0, 'step': 5076}, {'eval_loss': 1.8758022785186768, 'eval_matthews_correlation': 0.2625340562524287, 'eval_runtime': 22.2923, 'eval_samples_per_second': 53.606, 'eval_steps_per_second': 3.364, 'epoch': 36.0, 'step': 5076}, {'loss': 0.1765, 'learning_rate': 1.988302872237104e-07, 'epoch': 37.0, 'step': 5217}, {'eval_loss': 1.876428246498108, 'eval_matthews_correlation': 0.26633660499098555, 'eval_runtime': 22.2812, 'eval_samples_per_second': 53.633, 'eval_steps_per_second': 3.366, 'epoch': 37.0, 'step': 5217}, {'loss': 0.1705, 'learning_rate': 1.3255352481580692e-07, 'epoch': 38.0, 'step': 5358}, {'eval_loss': 1.880476713180542, 'eval_matthews_correlation': 0.2712271764686451, 'eval_runtime': 22.3062, 'eval_samples_per_second': 53.573, 'eval_steps_per_second': 3.362, 'epoch': 38.0, 'step': 5358}, {'loss': 0.1519, 'learning_rate': 6.627676240790346e-08, 'epoch': 39.0, 'step': 5499}, {'eval_loss': 1.856809377670288, 'eval_matthews_correlation': 0.26994096508961524, 'eval_runtime': 22.2738, 'eval_samples_per_second': 53.651, 'eval_steps_per_second': 3.367, 'epoch': 39.0, 'step': 5499}, {'loss': 0.1675, 'learning_rate': 0.0, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 1.8670214414596558, 'eval_matthews_correlation': 0.2682745138461367, 'eval_runtime': 22.262, 'eval_samples_per_second': 53.679, 'eval_steps_per_second': 3.369, 'epoch': 40.0, 'step': 5640}, {'train_runtime': 6420.4652, 'train_samples_per_second': 14.024, 'train_steps_per_second': 0.878, 'total_flos': 2.369073213222912e+16, 'train_loss': 0.41741698853513026, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 0.7415736317634583, 'eval_matthews_correlation': 0.45116853352689895, 'eval_runtime': 22.3151, 'eval_samples_per_second': 53.551, 'eval_steps_per_second': 3.361, 'epoch': 40.0, 'step': 5640}]\n","{'loss': 0.8938, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}\n","\u003cclass 'list'\u003e\n","[{'loss': 1.3105, 'learning_rate': 2.584793733908235e-06, 'epoch': 1.0, 'step': 141}, {'eval_loss': 0.790382444858551, 'eval_matthews_correlation': 0.37575911596913475, 'eval_runtime': 22.2693, 'eval_samples_per_second': 53.661, 'eval_steps_per_second': 3.368, 'epoch': 1.0, 'step': 141}, {'loss': 0.8938, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}, {'eval_loss': 0.7269756197929382, 'eval_matthews_correlation': 0.4418994437953655, 'eval_runtime': 22.2811, 'eval_samples_per_second': 53.633, 'eval_steps_per_second': 3.366, 'epoch': 2.0, 'step': 282}, {'loss': 0.8394, 'learning_rate': 2.4522402090924283e-06, 'epoch': 3.0, 'step': 423}, {'eval_loss': 0.7591622471809387, 'eval_matthews_correlation': 0.43907144812278764, 'eval_runtime': 22.2737, 'eval_samples_per_second': 53.651, 'eval_steps_per_second': 3.367, 'epoch': 3.0, 'step': 423}, {'loss': 0.8107, 'learning_rate': 2.385963446684525e-06, 'epoch': 4.0, 'step': 564}, {'eval_loss': 0.7667573094367981, 'eval_matthews_correlation': 0.43358925829971584, 'eval_runtime': 22.2819, 'eval_samples_per_second': 53.631, 'eval_steps_per_second': 3.366, 'epoch': 4.0, 'step': 564}, {'loss': 0.7728, 'learning_rate': 2.319686684276621e-06, 'epoch': 5.0, 'step': 705}, {'eval_loss': 0.7415736317634583, 'eval_matthews_correlation': 0.45116853352689895, 'eval_runtime': 22.2682, 'eval_samples_per_second': 53.664, 'eval_steps_per_second': 3.368, 'epoch': 5.0, 'step': 705}, {'loss': 0.7459, 'learning_rate': 2.2534099218687176e-06, 'epoch': 6.0, 'step': 846}, {'eval_loss': 0.7465517520904541, 'eval_matthews_correlation': 0.43562229483291337, 'eval_runtime': 22.2972, 'eval_samples_per_second': 53.594, 'eval_steps_per_second': 3.364, 'epoch': 6.0, 'step': 846}, {'loss': 0.693, 'learning_rate': 2.187133159460814e-06, 'epoch': 7.0, 'step': 987}, {'eval_loss': 0.7617669701576233, 'eval_matthews_correlation': 0.4473741450897212, 'eval_runtime': 22.2872, 'eval_samples_per_second': 53.618, 'eval_steps_per_second': 3.365, 'epoch': 7.0, 'step': 987}, {'loss': 0.6707, 'learning_rate': 2.1208563970529108e-06, 'epoch': 8.0, 'step': 1128}, {'eval_loss': 0.7983715534210205, 'eval_matthews_correlation': 0.44526798895309466, 'eval_runtime': 22.2884, 'eval_samples_per_second': 53.615, 'eval_steps_per_second': 3.365, 'epoch': 8.0, 'step': 1128}, {'loss': 0.6492, 'learning_rate': 2.0545796346450073e-06, 'epoch': 9.0, 'step': 1269}, {'eval_loss': 0.8745942711830139, 'eval_matthews_correlation': 0.3746554307565798, 'eval_runtime': 22.2668, 'eval_samples_per_second': 53.667, 'eval_steps_per_second': 3.368, 'epoch': 9.0, 'step': 1269}, {'loss': 0.5982, 'learning_rate': 1.988302872237104e-06, 'epoch': 10.0, 'step': 1410}, {'eval_loss': 0.8469460606575012, 'eval_matthews_correlation': 0.326138166031764, 'eval_runtime': 22.2704, 'eval_samples_per_second': 53.659, 'eval_steps_per_second': 3.368, 'epoch': 10.0, 'step': 1410}, {'loss': 0.5668, 'learning_rate': 1.9220261098292005e-06, 'epoch': 11.0, 'step': 1551}, {'eval_loss': 0.9425092935562134, 'eval_matthews_correlation': 0.32267286500181086, 'eval_runtime': 22.2797, 'eval_samples_per_second': 53.636, 'eval_steps_per_second': 3.366, 'epoch': 11.0, 'step': 1551}, {'loss': 0.5314, 'learning_rate': 1.8557493474212969e-06, 'epoch': 12.0, 'step': 1692}, {'eval_loss': 0.9633356928825378, 'eval_matthews_correlation': 0.304035285760411, 'eval_runtime': 22.2859, 'eval_samples_per_second': 53.621, 'eval_steps_per_second': 3.365, 'epoch': 12.0, 'step': 1692}, {'loss': 0.5035, 'learning_rate': 1.7894725850133937e-06, 'epoch': 13.0, 'step': 1833}, {'eval_loss': 1.0024386644363403, 'eval_matthews_correlation': 0.30873674263563244, 'eval_runtime': 22.3171, 'eval_samples_per_second': 53.546, 'eval_steps_per_second': 3.361, 'epoch': 13.0, 'step': 1833}, {'loss': 0.4795, 'learning_rate': 1.72319582260549e-06, 'epoch': 14.0, 'step': 1974}, {'eval_loss': 1.1653536558151245, 'eval_matthews_correlation': 0.28163452343105727, 'eval_runtime': 22.2659, 'eval_samples_per_second': 53.669, 'eval_steps_per_second': 3.368, 'epoch': 14.0, 'step': 1974}, {'loss': 0.4422, 'learning_rate': 1.6569190601975866e-06, 'epoch': 15.0, 'step': 2115}, {'eval_loss': 1.1062140464782715, 'eval_matthews_correlation': 0.3093738897901685, 'eval_runtime': 22.2775, 'eval_samples_per_second': 53.642, 'eval_steps_per_second': 3.367, 'epoch': 15.0, 'step': 2115}, {'loss': 0.4165, 'learning_rate': 1.5906422977896832e-06, 'epoch': 16.0, 'step': 2256}, {'eval_loss': 1.3092963695526123, 'eval_matthews_correlation': 0.2636610631670115, 'eval_runtime': 22.2743, 'eval_samples_per_second': 53.649, 'eval_steps_per_second': 3.367, 'epoch': 16.0, 'step': 2256}, {'loss': 0.3965, 'learning_rate': 1.5243655353817795e-06, 'epoch': 17.0, 'step': 2397}, {'eval_loss': 1.230315089225769, 'eval_matthews_correlation': 0.29941497662529204, 'eval_runtime': 22.3002, 'eval_samples_per_second': 53.587, 'eval_steps_per_second': 3.363, 'epoch': 17.0, 'step': 2397}, {'loss': 0.3739, 'learning_rate': 1.4580887729738763e-06, 'epoch': 18.0, 'step': 2538}, {'eval_loss': 1.260384202003479, 'eval_matthews_correlation': 0.2721666564900488, 'eval_runtime': 22.3466, 'eval_samples_per_second': 53.476, 'eval_steps_per_second': 3.356, 'epoch': 18.0, 'step': 2538}, {'loss': 0.3507, 'learning_rate': 1.3918120105659727e-06, 'epoch': 19.0, 'step': 2679}, {'eval_loss': 1.3919278383255005, 'eval_matthews_correlation': 0.2621135030679744, 'eval_runtime': 22.2941, 'eval_samples_per_second': 53.602, 'eval_steps_per_second': 3.364, 'epoch': 19.0, 'step': 2679}, {'loss': 0.3266, 'learning_rate': 1.3255352481580693e-06, 'epoch': 20.0, 'step': 2820}, {'eval_loss': 1.4495415687561035, 'eval_matthews_correlation': 0.2604040743888413, 'eval_runtime': 22.2828, 'eval_samples_per_second': 53.629, 'eval_steps_per_second': 3.366, 'epoch': 20.0, 'step': 2820}, {'loss': 0.3073, 'learning_rate': 1.2592584857501659e-06, 'epoch': 21.0, 'step': 2961}, {'eval_loss': 1.3744540214538574, 'eval_matthews_correlation': 0.27961244161619536, 'eval_runtime': 22.3007, 'eval_samples_per_second': 53.586, 'eval_steps_per_second': 3.363, 'epoch': 21.0, 'step': 2961}, {'loss': 0.3023, 'learning_rate': 1.1929817233422624e-06, 'epoch': 22.0, 'step': 3102}, {'eval_loss': 1.3975107669830322, 'eval_matthews_correlation': 0.27888624423897085, 'eval_runtime': 22.2842, 'eval_samples_per_second': 53.625, 'eval_steps_per_second': 3.366, 'epoch': 22.0, 'step': 3102}, {'loss': 0.2875, 'learning_rate': 1.1267049609343588e-06, 'epoch': 23.0, 'step': 3243}, {'eval_loss': 1.51190185546875, 'eval_matthews_correlation': 0.27278254249840445, 'eval_runtime': 22.2814, 'eval_samples_per_second': 53.632, 'eval_steps_per_second': 3.366, 'epoch': 23.0, 'step': 3243}, {'loss': 0.2582, 'learning_rate': 1.0604281985264554e-06, 'epoch': 24.0, 'step': 3384}, {'eval_loss': 1.4356608390808105, 'eval_matthews_correlation': 0.28004767536391345, 'eval_runtime': 22.3067, 'eval_samples_per_second': 53.571, 'eval_steps_per_second': 3.362, 'epoch': 24.0, 'step': 3384}, {'loss': 0.2476, 'learning_rate': 9.94151436118552e-07, 'epoch': 25.0, 'step': 3525}, {'eval_loss': 1.6550387144088745, 'eval_matthews_correlation': 0.274021861988056, 'eval_runtime': 22.287, 'eval_samples_per_second': 53.619, 'eval_steps_per_second': 3.365, 'epoch': 25.0, 'step': 3525}, {'loss': 0.2413, 'learning_rate': 9.278746737106484e-07, 'epoch': 26.0, 'step': 3666}, {'eval_loss': 1.5566381216049194, 'eval_matthews_correlation': 0.27398185938304354, 'eval_runtime': 22.2911, 'eval_samples_per_second': 53.609, 'eval_steps_per_second': 3.365, 'epoch': 26.0, 'step': 3666}, {'loss': 0.2353, 'learning_rate': 8.61597911302745e-07, 'epoch': 27.0, 'step': 3807}, {'eval_loss': 1.6979751586914062, 'eval_matthews_correlation': 0.2550800135500771, 'eval_runtime': 22.2969, 'eval_samples_per_second': 53.595, 'eval_steps_per_second': 3.364, 'epoch': 27.0, 'step': 3807}, {'loss': 0.2216, 'learning_rate': 7.953211488948416e-07, 'epoch': 28.0, 'step': 3948}, {'eval_loss': 1.743648648262024, 'eval_matthews_correlation': 0.2588589368200626, 'eval_runtime': 22.2778, 'eval_samples_per_second': 53.641, 'eval_steps_per_second': 3.367, 'epoch': 28.0, 'step': 3948}, {'loss': 0.2149, 'learning_rate': 7.290443864869382e-07, 'epoch': 29.0, 'step': 4089}, {'eval_loss': 1.7311393022537231, 'eval_matthews_correlation': 0.2573405352678869, 'eval_runtime': 22.3213, 'eval_samples_per_second': 53.536, 'eval_steps_per_second': 3.36, 'epoch': 29.0, 'step': 4089}, {'loss': 0.2125, 'learning_rate': 6.627676240790346e-07, 'epoch': 30.0, 'step': 4230}, {'eval_loss': 1.657152533531189, 'eval_matthews_correlation': 0.27338592003955503, 'eval_runtime': 22.3213, 'eval_samples_per_second': 53.536, 'eval_steps_per_second': 3.36, 'epoch': 30.0, 'step': 4230}, {'loss': 0.2057, 'learning_rate': 5.964908616711312e-07, 'epoch': 31.0, 'step': 4371}, {'eval_loss': 1.7150394916534424, 'eval_matthews_correlation': 0.2693420414193502, 'eval_runtime': 22.3225, 'eval_samples_per_second': 53.534, 'eval_steps_per_second': 3.36, 'epoch': 31.0, 'step': 4371}, {'loss': 0.1879, 'learning_rate': 5.302140992632277e-07, 'epoch': 32.0, 'step': 4512}, {'eval_loss': 1.7489519119262695, 'eval_matthews_correlation': 0.2690731470893474, 'eval_runtime': 22.3011, 'eval_samples_per_second': 53.585, 'eval_steps_per_second': 3.363, 'epoch': 32.0, 'step': 4512}, {'loss': 0.185, 'learning_rate': 4.639373368553242e-07, 'epoch': 33.0, 'step': 4653}, {'eval_loss': 1.7884665727615356, 'eval_matthews_correlation': 0.2639883952700387, 'eval_runtime': 22.3046, 'eval_samples_per_second': 53.577, 'eval_steps_per_second': 3.363, 'epoch': 33.0, 'step': 4653}, {'loss': 0.19, 'learning_rate': 3.976605744474208e-07, 'epoch': 34.0, 'step': 4794}, {'eval_loss': 1.866951584815979, 'eval_matthews_correlation': 0.25853911321652157, 'eval_runtime': 22.2878, 'eval_samples_per_second': 53.617, 'eval_steps_per_second': 3.365, 'epoch': 34.0, 'step': 4794}, {'loss': 0.184, 'learning_rate': 3.313838120395173e-07, 'epoch': 35.0, 'step': 4935}, {'eval_loss': 1.88068425655365, 'eval_matthews_correlation': 0.2590924049174448, 'eval_runtime': 22.3018, 'eval_samples_per_second': 53.583, 'eval_steps_per_second': 3.363, 'epoch': 35.0, 'step': 4935}, {'loss': 0.1771, 'learning_rate': 2.6510704963161385e-07, 'epoch': 36.0, 'step': 5076}, {'eval_loss': 1.8758022785186768, 'eval_matthews_correlation': 0.2625340562524287, 'eval_runtime': 22.2923, 'eval_samples_per_second': 53.606, 'eval_steps_per_second': 3.364, 'epoch': 36.0, 'step': 5076}, {'loss': 0.1765, 'learning_rate': 1.988302872237104e-07, 'epoch': 37.0, 'step': 5217}, {'eval_loss': 1.876428246498108, 'eval_matthews_correlation': 0.26633660499098555, 'eval_runtime': 22.2812, 'eval_samples_per_second': 53.633, 'eval_steps_per_second': 3.366, 'epoch': 37.0, 'step': 5217}, {'loss': 0.1705, 'learning_rate': 1.3255352481580692e-07, 'epoch': 38.0, 'step': 5358}, {'eval_loss': 1.880476713180542, 'eval_matthews_correlation': 0.2712271764686451, 'eval_runtime': 22.3062, 'eval_samples_per_second': 53.573, 'eval_steps_per_second': 3.362, 'epoch': 38.0, 'step': 5358}, {'loss': 0.1519, 'learning_rate': 6.627676240790346e-08, 'epoch': 39.0, 'step': 5499}, {'eval_loss': 1.856809377670288, 'eval_matthews_correlation': 0.26994096508961524, 'eval_runtime': 22.2738, 'eval_samples_per_second': 53.651, 'eval_steps_per_second': 3.367, 'epoch': 39.0, 'step': 5499}, {'loss': 0.1675, 'learning_rate': 0.0, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 1.8670214414596558, 'eval_matthews_correlation': 0.2682745138461367, 'eval_runtime': 22.262, 'eval_samples_per_second': 53.679, 'eval_steps_per_second': 3.369, 'epoch': 40.0, 'step': 5640}, {'train_runtime': 6420.4652, 'train_samples_per_second': 14.024, 'train_steps_per_second': 0.878, 'total_flos': 2.369073213222912e+16, 'train_loss': 0.41741698853513026, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 0.7415736317634583, 'eval_matthews_correlation': 0.45116853352689895, 'eval_runtime': 22.3151, 'eval_samples_per_second': 53.551, 'eval_steps_per_second': 3.361, 'epoch': 40.0, 'step': 5640}]\n","\u003cclass 'list'\u003e\n","True\n","Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment  is completed for low\n","starting  cardiffnlp/bertweet-base-sentiment  at run_low is True\n"]},{"name":"stderr","output_type":"stream","text":["loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/vocab.txt\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/bpe.codes\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/added_tokens.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/special_tokens_map.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/tokenizer_config.json\n","emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n","Adding \u003cmask\u003e to the vocabulary\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"088ec372f0d342a7bcfedc76a06e464c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4090b2957cb3494184b1674a6cd41f72","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:22]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/vocab.txt\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/bpe.codes\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/added_tokens.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/special_tokens_map.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/tokenizer_config.json\n","emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n","Adding \u003cmask\u003e to the vocabulary\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15862c7ffa4840e8b718bbad92b5d4eb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5798cecae1204f278cde669567869cb9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-1700.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:22]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/vocab.txt\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/bpe.codes\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/added_tokens.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/special_tokens_map.json\n","loading file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/tokenizer_config.json\n","emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n","Adding \u003cmask\u003e to the vocabulary\n"]},{"name":"stdout","output_type":"stream","text":["running on checkpoint  /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce8f41ee63664398a8eb39573bfa447a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"604b4d14cfc244c8b64a6fa91ce94999","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00\u003c?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 2251\n","})\n","Dataset({\n","    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 1195\n","})\n"]},{"name":"stderr","output_type":"stream","text":["loading configuration file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306\",\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 130,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"BertweetTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n","\n","All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high/checkpoint-306.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 2251\n","  Num Epochs = 40\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed \u0026 accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5640\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='5640' max='5640' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [5640/5640 1:48:27, Epoch 40/40]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eEpoch\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","      \u003cth\u003eValidation Loss\u003c/th\u003e\n","      \u003cth\u003eMatthews Correlation\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e0.997900\u003c/td\u003e\n","      \u003ctd\u003e0.786849\u003c/td\u003e\n","      \u003ctd\u003e0.414393\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e0.875600\u003c/td\u003e\n","      \u003ctd\u003e0.726169\u003c/td\u003e\n","      \u003ctd\u003e0.455361\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e0.834700\u003c/td\u003e\n","      \u003ctd\u003e0.758498\u003c/td\u003e\n","      \u003ctd\u003e0.442749\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e0.794100\u003c/td\u003e\n","      \u003ctd\u003e0.756889\u003c/td\u003e\n","      \u003ctd\u003e0.446081\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e0.754300\u003c/td\u003e\n","      \u003ctd\u003e0.741197\u003c/td\u003e\n","      \u003ctd\u003e0.458795\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e6\u003c/td\u003e\n","      \u003ctd\u003e0.724500\u003c/td\u003e\n","      \u003ctd\u003e0.758521\u003c/td\u003e\n","      \u003ctd\u003e0.440014\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e7\u003c/td\u003e\n","      \u003ctd\u003e0.697700\u003c/td\u003e\n","      \u003ctd\u003e0.765263\u003c/td\u003e\n","      \u003ctd\u003e0.437693\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e8\u003c/td\u003e\n","      \u003ctd\u003e0.665200\u003c/td\u003e\n","      \u003ctd\u003e0.776697\u003c/td\u003e\n","      \u003ctd\u003e0.445954\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e9\u003c/td\u003e\n","      \u003ctd\u003e0.639400\u003c/td\u003e\n","      \u003ctd\u003e0.828927\u003c/td\u003e\n","      \u003ctd\u003e0.421898\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e10\u003c/td\u003e\n","      \u003ctd\u003e0.607200\u003c/td\u003e\n","      \u003ctd\u003e0.899607\u003c/td\u003e\n","      \u003ctd\u003e0.413460\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e11\u003c/td\u003e\n","      \u003ctd\u003e0.567100\u003c/td\u003e\n","      \u003ctd\u003e0.976996\u003c/td\u003e\n","      \u003ctd\u003e0.409606\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e12\u003c/td\u003e\n","      \u003ctd\u003e0.539700\u003c/td\u003e\n","      \u003ctd\u003e0.848700\u003c/td\u003e\n","      \u003ctd\u003e0.401951\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e13\u003c/td\u003e\n","      \u003ctd\u003e0.520600\u003c/td\u003e\n","      \u003ctd\u003e0.932034\u003c/td\u003e\n","      \u003ctd\u003e0.390724\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e14\u003c/td\u003e\n","      \u003ctd\u003e0.487200\u003c/td\u003e\n","      \u003ctd\u003e1.015595\u003c/td\u003e\n","      \u003ctd\u003e0.396058\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e15\u003c/td\u003e\n","      \u003ctd\u003e0.467100\u003c/td\u003e\n","      \u003ctd\u003e1.033079\u003c/td\u003e\n","      \u003ctd\u003e0.378760\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e16\u003c/td\u003e\n","      \u003ctd\u003e0.437300\u003c/td\u003e\n","      \u003ctd\u003e1.116720\u003c/td\u003e\n","      \u003ctd\u003e0.375046\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e17\u003c/td\u003e\n","      \u003ctd\u003e0.414300\u003c/td\u003e\n","      \u003ctd\u003e1.092275\u003c/td\u003e\n","      \u003ctd\u003e0.377162\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e18\u003c/td\u003e\n","      \u003ctd\u003e0.377700\u003c/td\u003e\n","      \u003ctd\u003e1.060325\u003c/td\u003e\n","      \u003ctd\u003e0.313812\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e19\u003c/td\u003e\n","      \u003ctd\u003e0.374000\u003c/td\u003e\n","      \u003ctd\u003e1.170762\u003c/td\u003e\n","      \u003ctd\u003e0.356476\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e20\u003c/td\u003e\n","      \u003ctd\u003e0.357900\u003c/td\u003e\n","      \u003ctd\u003e1.158463\u003c/td\u003e\n","      \u003ctd\u003e0.310087\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e21\u003c/td\u003e\n","      \u003ctd\u003e0.338700\u003c/td\u003e\n","      \u003ctd\u003e1.202293\u003c/td\u003e\n","      \u003ctd\u003e0.243104\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e22\u003c/td\u003e\n","      \u003ctd\u003e0.344200\u003c/td\u003e\n","      \u003ctd\u003e1.239493\u003c/td\u003e\n","      \u003ctd\u003e0.245604\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003e0.310900\u003c/td\u003e\n","      \u003ctd\u003e1.249092\u003c/td\u003e\n","      \u003ctd\u003e0.247856\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e24\u003c/td\u003e\n","      \u003ctd\u003e0.308800\u003c/td\u003e\n","      \u003ctd\u003e1.266057\u003c/td\u003e\n","      \u003ctd\u003e0.251300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e25\u003c/td\u003e\n","      \u003ctd\u003e0.280500\u003c/td\u003e\n","      \u003ctd\u003e1.449782\u003c/td\u003e\n","      \u003ctd\u003e0.240529\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e26\u003c/td\u003e\n","      \u003ctd\u003e0.262700\u003c/td\u003e\n","      \u003ctd\u003e1.405656\u003c/td\u003e\n","      \u003ctd\u003e0.231011\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e27\u003c/td\u003e\n","      \u003ctd\u003e0.260600\u003c/td\u003e\n","      \u003ctd\u003e1.345683\u003c/td\u003e\n","      \u003ctd\u003e0.235869\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e28\u003c/td\u003e\n","      \u003ctd\u003e0.256400\u003c/td\u003e\n","      \u003ctd\u003e1.402586\u003c/td\u003e\n","      \u003ctd\u003e0.228319\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e29\u003c/td\u003e\n","      \u003ctd\u003e0.230800\u003c/td\u003e\n","      \u003ctd\u003e1.470564\u003c/td\u003e\n","      \u003ctd\u003e0.238759\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e30\u003c/td\u003e\n","      \u003ctd\u003e0.221500\u003c/td\u003e\n","      \u003ctd\u003e1.448865\u003c/td\u003e\n","      \u003ctd\u003e0.239123\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e31\u003c/td\u003e\n","      \u003ctd\u003e0.215600\u003c/td\u003e\n","      \u003ctd\u003e1.530137\u003c/td\u003e\n","      \u003ctd\u003e0.234931\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e32\u003c/td\u003e\n","      \u003ctd\u003e0.212700\u003c/td\u003e\n","      \u003ctd\u003e1.460495\u003c/td\u003e\n","      \u003ctd\u003e0.236012\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e33\u003c/td\u003e\n","      \u003ctd\u003e0.225700\u003c/td\u003e\n","      \u003ctd\u003e1.480372\u003c/td\u003e\n","      \u003ctd\u003e0.233472\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e34\u003c/td\u003e\n","      \u003ctd\u003e0.201800\u003c/td\u003e\n","      \u003ctd\u003e1.527751\u003c/td\u003e\n","      \u003ctd\u003e0.218291\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e35\u003c/td\u003e\n","      \u003ctd\u003e0.186700\u003c/td\u003e\n","      \u003ctd\u003e1.661829\u003c/td\u003e\n","      \u003ctd\u003e0.228925\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e36\u003c/td\u003e\n","      \u003ctd\u003e0.188000\u003c/td\u003e\n","      \u003ctd\u003e1.670327\u003c/td\u003e\n","      \u003ctd\u003e0.229445\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e37\u003c/td\u003e\n","      \u003ctd\u003e0.182700\u003c/td\u003e\n","      \u003ctd\u003e1.635980\u003c/td\u003e\n","      \u003ctd\u003e0.222370\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e38\u003c/td\u003e\n","      \u003ctd\u003e0.186100\u003c/td\u003e\n","      \u003ctd\u003e1.611247\u003c/td\u003e\n","      \u003ctd\u003e0.231047\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e39\u003c/td\u003e\n","      \u003ctd\u003e0.187500\u003c/td\u003e\n","      \u003ctd\u003e1.598374\u003c/td\u003e\n","      \u003ctd\u003e0.237958\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e40\u003c/td\u003e\n","      \u003ctd\u003e0.181700\u003c/td\u003e\n","      \u003ctd\u003e1.598947\u003c/td\u003e\n","      \u003ctd\u003e0.237095\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282/added_tokens.json\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-141] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-423] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-282] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-564] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-846] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-987] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1128] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1269] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1410] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1551] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1692] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1833] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-1974] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2115] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2256] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2397] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2538] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2679] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2820] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-2961] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3102] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3243] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3384] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3525] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3666] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3807] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-3948] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4089] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4230] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4371] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4512] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4653] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4794] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-4935] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5076] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5217] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5358] due to args.save_total_limit\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640\n","Configuration saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640/config.json\n","Model weights saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640/pytorch_model.bin\n","tokenizer config file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640/tokenizer_config.json\n","Special tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640/special_tokens_map.json\n","added tokens file saved in /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5640/added_tokens.json\n","Deleting older checkpoint [/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-5499] due to args.save_total_limit\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/checkpoint-705 (score: 0.45879460817017415).\n","The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["***** train metrics *****\n","  epoch                    =       40.0\n","  total_flos               = 22063713GF\n","  train_loss               =     0.4229\n","  train_runtime            = 1:48:28.87\n","  train_samples            =       2251\n","  train_samples_per_second =     13.833\n","  train_steps_per_second   =      0.867\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='150' max='75' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [75/75 00:44]\n","    \u003c/div\u003e\n","    "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1195\n","  Batch size = 16\n"]},{"name":"stdout","output_type":"stream","text":["***** eval metrics *****\n","  epoch                     =       40.0\n","  eval_loss                 =     0.7412\n","  eval_matthews_correlation =     0.4588\n","  eval_runtime              = 0:00:22.55\n","  eval_samples              =       1195\n","  eval_samples_per_second   =     52.991\n","  eval_steps_per_second     =      3.326\n","saving to  /content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/cardiffnlp/bertweet-base-sentiment/high_then_low-checkpoint-306/logs.p\n","[{'loss': 0.9979, 'learning_rate': 2.584793733908235e-06, 'epoch': 1.0, 'step': 141}, {'eval_loss': 0.7868486642837524, 'eval_matthews_correlation': 0.41439262424027556, 'eval_runtime': 22.5777, 'eval_samples_per_second': 52.928, 'eval_steps_per_second': 3.322, 'epoch': 1.0, 'step': 141}, {'loss': 0.8756, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}, {'eval_loss': 0.7261686325073242, 'eval_matthews_correlation': 0.45536111249109534, 'eval_runtime': 22.5887, 'eval_samples_per_second': 52.903, 'eval_steps_per_second': 3.32, 'epoch': 2.0, 'step': 282}, {'loss': 0.8347, 'learning_rate': 2.4522402090924283e-06, 'epoch': 3.0, 'step': 423}, {'eval_loss': 0.7584980130195618, 'eval_matthews_correlation': 0.442748937702135, 'eval_runtime': 22.5754, 'eval_samples_per_second': 52.934, 'eval_steps_per_second': 3.322, 'epoch': 3.0, 'step': 423}, {'loss': 0.7941, 'learning_rate': 2.385963446684525e-06, 'epoch': 4.0, 'step': 564}, {'eval_loss': 0.7568894624710083, 'eval_matthews_correlation': 0.44608081386447346, 'eval_runtime': 22.5716, 'eval_samples_per_second': 52.943, 'eval_steps_per_second': 3.323, 'epoch': 4.0, 'step': 564}, {'loss': 0.7543, 'learning_rate': 2.319686684276621e-06, 'epoch': 5.0, 'step': 705}, {'eval_loss': 0.7411968111991882, 'eval_matthews_correlation': 0.45879460817017415, 'eval_runtime': 22.5684, 'eval_samples_per_second': 52.95, 'eval_steps_per_second': 3.323, 'epoch': 5.0, 'step': 705}, {'loss': 0.7245, 'learning_rate': 2.2534099218687176e-06, 'epoch': 6.0, 'step': 846}, {'eval_loss': 0.7585211992263794, 'eval_matthews_correlation': 0.440013796330713, 'eval_runtime': 22.5742, 'eval_samples_per_second': 52.937, 'eval_steps_per_second': 3.322, 'epoch': 6.0, 'step': 846}, {'loss': 0.6977, 'learning_rate': 2.187133159460814e-06, 'epoch': 7.0, 'step': 987}, {'eval_loss': 0.7652630805969238, 'eval_matthews_correlation': 0.43769317590175777, 'eval_runtime': 22.5863, 'eval_samples_per_second': 52.908, 'eval_steps_per_second': 3.321, 'epoch': 7.0, 'step': 987}, {'loss': 0.6652, 'learning_rate': 2.1208563970529108e-06, 'epoch': 8.0, 'step': 1128}, {'eval_loss': 0.7766966223716736, 'eval_matthews_correlation': 0.4459543471141036, 'eval_runtime': 22.5997, 'eval_samples_per_second': 52.877, 'eval_steps_per_second': 3.319, 'epoch': 8.0, 'step': 1128}, {'loss': 0.6394, 'learning_rate': 2.0545796346450073e-06, 'epoch': 9.0, 'step': 1269}, {'eval_loss': 0.828927218914032, 'eval_matthews_correlation': 0.4218980374550271, 'eval_runtime': 22.577, 'eval_samples_per_second': 52.93, 'eval_steps_per_second': 3.322, 'epoch': 9.0, 'step': 1269}, {'loss': 0.6072, 'learning_rate': 1.988302872237104e-06, 'epoch': 10.0, 'step': 1410}, {'eval_loss': 0.8996071219444275, 'eval_matthews_correlation': 0.4134603303171571, 'eval_runtime': 22.5625, 'eval_samples_per_second': 52.964, 'eval_steps_per_second': 3.324, 'epoch': 10.0, 'step': 1410}, {'loss': 0.5671, 'learning_rate': 1.9220261098292005e-06, 'epoch': 11.0, 'step': 1551}, {'eval_loss': 0.9769960045814514, 'eval_matthews_correlation': 0.4096060588296595, 'eval_runtime': 22.5912, 'eval_samples_per_second': 52.897, 'eval_steps_per_second': 3.32, 'epoch': 11.0, 'step': 1551}, {'loss': 0.5397, 'learning_rate': 1.8557493474212969e-06, 'epoch': 12.0, 'step': 1692}, {'eval_loss': 0.848700225353241, 'eval_matthews_correlation': 0.40195117224361143, 'eval_runtime': 22.5681, 'eval_samples_per_second': 52.951, 'eval_steps_per_second': 3.323, 'epoch': 12.0, 'step': 1692}, {'loss': 0.5206, 'learning_rate': 1.7894725850133937e-06, 'epoch': 13.0, 'step': 1833}, {'eval_loss': 0.932034432888031, 'eval_matthews_correlation': 0.3907239269494925, 'eval_runtime': 22.5816, 'eval_samples_per_second': 52.919, 'eval_steps_per_second': 3.321, 'epoch': 13.0, 'step': 1833}, {'loss': 0.4872, 'learning_rate': 1.72319582260549e-06, 'epoch': 14.0, 'step': 1974}, {'eval_loss': 1.0155953168869019, 'eval_matthews_correlation': 0.3960578078107727, 'eval_runtime': 22.5594, 'eval_samples_per_second': 52.971, 'eval_steps_per_second': 3.325, 'epoch': 14.0, 'step': 1974}, {'loss': 0.4671, 'learning_rate': 1.6569190601975866e-06, 'epoch': 15.0, 'step': 2115}, {'eval_loss': 1.0330790281295776, 'eval_matthews_correlation': 0.37875962720826284, 'eval_runtime': 22.5641, 'eval_samples_per_second': 52.96, 'eval_steps_per_second': 3.324, 'epoch': 15.0, 'step': 2115}, {'loss': 0.4373, 'learning_rate': 1.5906422977896832e-06, 'epoch': 16.0, 'step': 2256}, {'eval_loss': 1.1167200803756714, 'eval_matthews_correlation': 0.37504625883616804, 'eval_runtime': 22.526, 'eval_samples_per_second': 53.05, 'eval_steps_per_second': 3.329, 'epoch': 16.0, 'step': 2256}, {'loss': 0.4143, 'learning_rate': 1.5243655353817795e-06, 'epoch': 17.0, 'step': 2397}, {'eval_loss': 1.0922753810882568, 'eval_matthews_correlation': 0.3771615059003849, 'eval_runtime': 22.5261, 'eval_samples_per_second': 53.05, 'eval_steps_per_second': 3.329, 'epoch': 17.0, 'step': 2397}, {'loss': 0.3777, 'learning_rate': 1.4580887729738763e-06, 'epoch': 18.0, 'step': 2538}, {'eval_loss': 1.060325264930725, 'eval_matthews_correlation': 0.3138119766947409, 'eval_runtime': 22.5527, 'eval_samples_per_second': 52.987, 'eval_steps_per_second': 3.326, 'epoch': 18.0, 'step': 2538}, {'loss': 0.374, 'learning_rate': 1.3918120105659727e-06, 'epoch': 19.0, 'step': 2679}, {'eval_loss': 1.1707617044448853, 'eval_matthews_correlation': 0.35647621210504943, 'eval_runtime': 22.5579, 'eval_samples_per_second': 52.975, 'eval_steps_per_second': 3.325, 'epoch': 19.0, 'step': 2679}, {'loss': 0.3579, 'learning_rate': 1.3255352481580693e-06, 'epoch': 20.0, 'step': 2820}, {'eval_loss': 1.1584633588790894, 'eval_matthews_correlation': 0.3100874587130162, 'eval_runtime': 22.5979, 'eval_samples_per_second': 52.881, 'eval_steps_per_second': 3.319, 'epoch': 20.0, 'step': 2820}, {'loss': 0.3387, 'learning_rate': 1.2592584857501659e-06, 'epoch': 21.0, 'step': 2961}, {'eval_loss': 1.202292799949646, 'eval_matthews_correlation': 0.24310393082827025, 'eval_runtime': 22.5791, 'eval_samples_per_second': 52.925, 'eval_steps_per_second': 3.322, 'epoch': 21.0, 'step': 2961}, {'loss': 0.3442, 'learning_rate': 1.1929817233422624e-06, 'epoch': 22.0, 'step': 3102}, {'eval_loss': 1.2394925355911255, 'eval_matthews_correlation': 0.2456043458353585, 'eval_runtime': 22.5745, 'eval_samples_per_second': 52.936, 'eval_steps_per_second': 3.322, 'epoch': 22.0, 'step': 3102}, {'loss': 0.3109, 'learning_rate': 1.1267049609343588e-06, 'epoch': 23.0, 'step': 3243}, {'eval_loss': 1.2490915060043335, 'eval_matthews_correlation': 0.2478561490535534, 'eval_runtime': 22.5716, 'eval_samples_per_second': 52.943, 'eval_steps_per_second': 3.323, 'epoch': 23.0, 'step': 3243}, {'loss': 0.3088, 'learning_rate': 1.0604281985264554e-06, 'epoch': 24.0, 'step': 3384}, {'eval_loss': 1.2660571336746216, 'eval_matthews_correlation': 0.25130019248653185, 'eval_runtime': 22.5199, 'eval_samples_per_second': 53.064, 'eval_steps_per_second': 3.33, 'epoch': 24.0, 'step': 3384}, {'loss': 0.2805, 'learning_rate': 9.94151436118552e-07, 'epoch': 25.0, 'step': 3525}, {'eval_loss': 1.449782371520996, 'eval_matthews_correlation': 0.24052899213769954, 'eval_runtime': 22.5364, 'eval_samples_per_second': 53.025, 'eval_steps_per_second': 3.328, 'epoch': 25.0, 'step': 3525}, {'loss': 0.2627, 'learning_rate': 9.278746737106484e-07, 'epoch': 26.0, 'step': 3666}, {'eval_loss': 1.4056564569473267, 'eval_matthews_correlation': 0.23101125817911333, 'eval_runtime': 22.5948, 'eval_samples_per_second': 52.888, 'eval_steps_per_second': 3.319, 'epoch': 26.0, 'step': 3666}, {'loss': 0.2606, 'learning_rate': 8.61597911302745e-07, 'epoch': 27.0, 'step': 3807}, {'eval_loss': 1.345682978630066, 'eval_matthews_correlation': 0.23586857144335793, 'eval_runtime': 22.5754, 'eval_samples_per_second': 52.934, 'eval_steps_per_second': 3.322, 'epoch': 27.0, 'step': 3807}, {'loss': 0.2564, 'learning_rate': 7.953211488948416e-07, 'epoch': 28.0, 'step': 3948}, {'eval_loss': 1.4025861024856567, 'eval_matthews_correlation': 0.22831869291909548, 'eval_runtime': 22.6089, 'eval_samples_per_second': 52.855, 'eval_steps_per_second': 3.317, 'epoch': 28.0, 'step': 3948}, {'loss': 0.2308, 'learning_rate': 7.290443864869382e-07, 'epoch': 29.0, 'step': 4089}, {'eval_loss': 1.470563530921936, 'eval_matthews_correlation': 0.23875910008571513, 'eval_runtime': 22.5648, 'eval_samples_per_second': 52.959, 'eval_steps_per_second': 3.324, 'epoch': 29.0, 'step': 4089}, {'loss': 0.2215, 'learning_rate': 6.627676240790346e-07, 'epoch': 30.0, 'step': 4230}, {'eval_loss': 1.4488650560379028, 'eval_matthews_correlation': 0.23912256714597163, 'eval_runtime': 22.6383, 'eval_samples_per_second': 52.787, 'eval_steps_per_second': 3.313, 'epoch': 30.0, 'step': 4230}, {'loss': 0.2156, 'learning_rate': 5.964908616711312e-07, 'epoch': 31.0, 'step': 4371}, {'eval_loss': 1.5301367044448853, 'eval_matthews_correlation': 0.23493064410565892, 'eval_runtime': 22.5564, 'eval_samples_per_second': 52.978, 'eval_steps_per_second': 3.325, 'epoch': 31.0, 'step': 4371}, {'loss': 0.2127, 'learning_rate': 5.302140992632277e-07, 'epoch': 32.0, 'step': 4512}, {'eval_loss': 1.4604954719543457, 'eval_matthews_correlation': 0.2360120638147077, 'eval_runtime': 22.5565, 'eval_samples_per_second': 52.978, 'eval_steps_per_second': 3.325, 'epoch': 32.0, 'step': 4512}, {'loss': 0.2257, 'learning_rate': 4.639373368553242e-07, 'epoch': 33.0, 'step': 4653}, {'eval_loss': 1.4803723096847534, 'eval_matthews_correlation': 0.23347165386000293, 'eval_runtime': 22.5971, 'eval_samples_per_second': 52.883, 'eval_steps_per_second': 3.319, 'epoch': 33.0, 'step': 4653}, {'loss': 0.2018, 'learning_rate': 3.976605744474208e-07, 'epoch': 34.0, 'step': 4794}, {'eval_loss': 1.5277512073516846, 'eval_matthews_correlation': 0.21829054286920893, 'eval_runtime': 22.5467, 'eval_samples_per_second': 53.001, 'eval_steps_per_second': 3.326, 'epoch': 34.0, 'step': 4794}, {'loss': 0.1867, 'learning_rate': 3.313838120395173e-07, 'epoch': 35.0, 'step': 4935}, {'eval_loss': 1.6618291139602661, 'eval_matthews_correlation': 0.228924597434146, 'eval_runtime': 22.5347, 'eval_samples_per_second': 53.029, 'eval_steps_per_second': 3.328, 'epoch': 35.0, 'step': 4935}, {'loss': 0.188, 'learning_rate': 2.6510704963161385e-07, 'epoch': 36.0, 'step': 5076}, {'eval_loss': 1.6703267097473145, 'eval_matthews_correlation': 0.22944492824899304, 'eval_runtime': 22.572, 'eval_samples_per_second': 52.942, 'eval_steps_per_second': 3.323, 'epoch': 36.0, 'step': 5076}, {'loss': 0.1827, 'learning_rate': 1.988302872237104e-07, 'epoch': 37.0, 'step': 5217}, {'eval_loss': 1.6359800100326538, 'eval_matthews_correlation': 0.22236984035428717, 'eval_runtime': 22.5161, 'eval_samples_per_second': 53.073, 'eval_steps_per_second': 3.331, 'epoch': 37.0, 'step': 5217}, {'loss': 0.1861, 'learning_rate': 1.3255352481580692e-07, 'epoch': 38.0, 'step': 5358}, {'eval_loss': 1.611247181892395, 'eval_matthews_correlation': 0.23104707447603937, 'eval_runtime': 22.5622, 'eval_samples_per_second': 52.965, 'eval_steps_per_second': 3.324, 'epoch': 38.0, 'step': 5358}, {'loss': 0.1875, 'learning_rate': 6.627676240790346e-08, 'epoch': 39.0, 'step': 5499}, {'eval_loss': 1.5983740091323853, 'eval_matthews_correlation': 0.23795805574556775, 'eval_runtime': 22.5213, 'eval_samples_per_second': 53.061, 'eval_steps_per_second': 3.33, 'epoch': 39.0, 'step': 5499}, {'loss': 0.1817, 'learning_rate': 0.0, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 1.5989471673965454, 'eval_matthews_correlation': 0.237094661662564, 'eval_runtime': 22.5695, 'eval_samples_per_second': 52.947, 'eval_steps_per_second': 3.323, 'epoch': 40.0, 'step': 5640}, {'train_runtime': 6508.8773, 'train_samples_per_second': 13.833, 'train_steps_per_second': 0.867, 'total_flos': 2.369073213222912e+16, 'train_loss': 0.422930302180297, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 0.7411968111991882, 'eval_matthews_correlation': 0.45879460817017415, 'eval_runtime': 22.5508, 'eval_samples_per_second': 52.991, 'eval_steps_per_second': 3.326, 'epoch': 40.0, 'step': 5640}]\n","{'loss': 0.8756, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}\n","\u003cclass 'list'\u003e\n","[{'loss': 0.9979, 'learning_rate': 2.584793733908235e-06, 'epoch': 1.0, 'step': 141}, {'eval_loss': 0.7868486642837524, 'eval_matthews_correlation': 0.41439262424027556, 'eval_runtime': 22.5777, 'eval_samples_per_second': 52.928, 'eval_steps_per_second': 3.322, 'epoch': 1.0, 'step': 141}, {'loss': 0.8756, 'learning_rate': 2.5185169715003317e-06, 'epoch': 2.0, 'step': 282}, {'eval_loss': 0.7261686325073242, 'eval_matthews_correlation': 0.45536111249109534, 'eval_runtime': 22.5887, 'eval_samples_per_second': 52.903, 'eval_steps_per_second': 3.32, 'epoch': 2.0, 'step': 282}, {'loss': 0.8347, 'learning_rate': 2.4522402090924283e-06, 'epoch': 3.0, 'step': 423}, {'eval_loss': 0.7584980130195618, 'eval_matthews_correlation': 0.442748937702135, 'eval_runtime': 22.5754, 'eval_samples_per_second': 52.934, 'eval_steps_per_second': 3.322, 'epoch': 3.0, 'step': 423}, {'loss': 0.7941, 'learning_rate': 2.385963446684525e-06, 'epoch': 4.0, 'step': 564}, {'eval_loss': 0.7568894624710083, 'eval_matthews_correlation': 0.44608081386447346, 'eval_runtime': 22.5716, 'eval_samples_per_second': 52.943, 'eval_steps_per_second': 3.323, 'epoch': 4.0, 'step': 564}, {'loss': 0.7543, 'learning_rate': 2.319686684276621e-06, 'epoch': 5.0, 'step': 705}, {'eval_loss': 0.7411968111991882, 'eval_matthews_correlation': 0.45879460817017415, 'eval_runtime': 22.5684, 'eval_samples_per_second': 52.95, 'eval_steps_per_second': 3.323, 'epoch': 5.0, 'step': 705}, {'loss': 0.7245, 'learning_rate': 2.2534099218687176e-06, 'epoch': 6.0, 'step': 846}, {'eval_loss': 0.7585211992263794, 'eval_matthews_correlation': 0.440013796330713, 'eval_runtime': 22.5742, 'eval_samples_per_second': 52.937, 'eval_steps_per_second': 3.322, 'epoch': 6.0, 'step': 846}, {'loss': 0.6977, 'learning_rate': 2.187133159460814e-06, 'epoch': 7.0, 'step': 987}, {'eval_loss': 0.7652630805969238, 'eval_matthews_correlation': 0.43769317590175777, 'eval_runtime': 22.5863, 'eval_samples_per_second': 52.908, 'eval_steps_per_second': 3.321, 'epoch': 7.0, 'step': 987}, {'loss': 0.6652, 'learning_rate': 2.1208563970529108e-06, 'epoch': 8.0, 'step': 1128}, {'eval_loss': 0.7766966223716736, 'eval_matthews_correlation': 0.4459543471141036, 'eval_runtime': 22.5997, 'eval_samples_per_second': 52.877, 'eval_steps_per_second': 3.319, 'epoch': 8.0, 'step': 1128}, {'loss': 0.6394, 'learning_rate': 2.0545796346450073e-06, 'epoch': 9.0, 'step': 1269}, {'eval_loss': 0.828927218914032, 'eval_matthews_correlation': 0.4218980374550271, 'eval_runtime': 22.577, 'eval_samples_per_second': 52.93, 'eval_steps_per_second': 3.322, 'epoch': 9.0, 'step': 1269}, {'loss': 0.6072, 'learning_rate': 1.988302872237104e-06, 'epoch': 10.0, 'step': 1410}, {'eval_loss': 0.8996071219444275, 'eval_matthews_correlation': 0.4134603303171571, 'eval_runtime': 22.5625, 'eval_samples_per_second': 52.964, 'eval_steps_per_second': 3.324, 'epoch': 10.0, 'step': 1410}, {'loss': 0.5671, 'learning_rate': 1.9220261098292005e-06, 'epoch': 11.0, 'step': 1551}, {'eval_loss': 0.9769960045814514, 'eval_matthews_correlation': 0.4096060588296595, 'eval_runtime': 22.5912, 'eval_samples_per_second': 52.897, 'eval_steps_per_second': 3.32, 'epoch': 11.0, 'step': 1551}, {'loss': 0.5397, 'learning_rate': 1.8557493474212969e-06, 'epoch': 12.0, 'step': 1692}, {'eval_loss': 0.848700225353241, 'eval_matthews_correlation': 0.40195117224361143, 'eval_runtime': 22.5681, 'eval_samples_per_second': 52.951, 'eval_steps_per_second': 3.323, 'epoch': 12.0, 'step': 1692}, {'loss': 0.5206, 'learning_rate': 1.7894725850133937e-06, 'epoch': 13.0, 'step': 1833}, {'eval_loss': 0.932034432888031, 'eval_matthews_correlation': 0.3907239269494925, 'eval_runtime': 22.5816, 'eval_samples_per_second': 52.919, 'eval_steps_per_second': 3.321, 'epoch': 13.0, 'step': 1833}, {'loss': 0.4872, 'learning_rate': 1.72319582260549e-06, 'epoch': 14.0, 'step': 1974}, {'eval_loss': 1.0155953168869019, 'eval_matthews_correlation': 0.3960578078107727, 'eval_runtime': 22.5594, 'eval_samples_per_second': 52.971, 'eval_steps_per_second': 3.325, 'epoch': 14.0, 'step': 1974}, {'loss': 0.4671, 'learning_rate': 1.6569190601975866e-06, 'epoch': 15.0, 'step': 2115}, {'eval_loss': 1.0330790281295776, 'eval_matthews_correlation': 0.37875962720826284, 'eval_runtime': 22.5641, 'eval_samples_per_second': 52.96, 'eval_steps_per_second': 3.324, 'epoch': 15.0, 'step': 2115}, {'loss': 0.4373, 'learning_rate': 1.5906422977896832e-06, 'epoch': 16.0, 'step': 2256}, {'eval_loss': 1.1167200803756714, 'eval_matthews_correlation': 0.37504625883616804, 'eval_runtime': 22.526, 'eval_samples_per_second': 53.05, 'eval_steps_per_second': 3.329, 'epoch': 16.0, 'step': 2256}, {'loss': 0.4143, 'learning_rate': 1.5243655353817795e-06, 'epoch': 17.0, 'step': 2397}, {'eval_loss': 1.0922753810882568, 'eval_matthews_correlation': 0.3771615059003849, 'eval_runtime': 22.5261, 'eval_samples_per_second': 53.05, 'eval_steps_per_second': 3.329, 'epoch': 17.0, 'step': 2397}, {'loss': 0.3777, 'learning_rate': 1.4580887729738763e-06, 'epoch': 18.0, 'step': 2538}, {'eval_loss': 1.060325264930725, 'eval_matthews_correlation': 0.3138119766947409, 'eval_runtime': 22.5527, 'eval_samples_per_second': 52.987, 'eval_steps_per_second': 3.326, 'epoch': 18.0, 'step': 2538}, {'loss': 0.374, 'learning_rate': 1.3918120105659727e-06, 'epoch': 19.0, 'step': 2679}, {'eval_loss': 1.1707617044448853, 'eval_matthews_correlation': 0.35647621210504943, 'eval_runtime': 22.5579, 'eval_samples_per_second': 52.975, 'eval_steps_per_second': 3.325, 'epoch': 19.0, 'step': 2679}, {'loss': 0.3579, 'learning_rate': 1.3255352481580693e-06, 'epoch': 20.0, 'step': 2820}, {'eval_loss': 1.1584633588790894, 'eval_matthews_correlation': 0.3100874587130162, 'eval_runtime': 22.5979, 'eval_samples_per_second': 52.881, 'eval_steps_per_second': 3.319, 'epoch': 20.0, 'step': 2820}, {'loss': 0.3387, 'learning_rate': 1.2592584857501659e-06, 'epoch': 21.0, 'step': 2961}, {'eval_loss': 1.202292799949646, 'eval_matthews_correlation': 0.24310393082827025, 'eval_runtime': 22.5791, 'eval_samples_per_second': 52.925, 'eval_steps_per_second': 3.322, 'epoch': 21.0, 'step': 2961}, {'loss': 0.3442, 'learning_rate': 1.1929817233422624e-06, 'epoch': 22.0, 'step': 3102}, {'eval_loss': 1.2394925355911255, 'eval_matthews_correlation': 0.2456043458353585, 'eval_runtime': 22.5745, 'eval_samples_per_second': 52.936, 'eval_steps_per_second': 3.322, 'epoch': 22.0, 'step': 3102}, {'loss': 0.3109, 'learning_rate': 1.1267049609343588e-06, 'epoch': 23.0, 'step': 3243}, {'eval_loss': 1.2490915060043335, 'eval_matthews_correlation': 0.2478561490535534, 'eval_runtime': 22.5716, 'eval_samples_per_second': 52.943, 'eval_steps_per_second': 3.323, 'epoch': 23.0, 'step': 3243}, {'loss': 0.3088, 'learning_rate': 1.0604281985264554e-06, 'epoch': 24.0, 'step': 3384}, {'eval_loss': 1.2660571336746216, 'eval_matthews_correlation': 0.25130019248653185, 'eval_runtime': 22.5199, 'eval_samples_per_second': 53.064, 'eval_steps_per_second': 3.33, 'epoch': 24.0, 'step': 3384}, {'loss': 0.2805, 'learning_rate': 9.94151436118552e-07, 'epoch': 25.0, 'step': 3525}, {'eval_loss': 1.449782371520996, 'eval_matthews_correlation': 0.24052899213769954, 'eval_runtime': 22.5364, 'eval_samples_per_second': 53.025, 'eval_steps_per_second': 3.328, 'epoch': 25.0, 'step': 3525}, {'loss': 0.2627, 'learning_rate': 9.278746737106484e-07, 'epoch': 26.0, 'step': 3666}, {'eval_loss': 1.4056564569473267, 'eval_matthews_correlation': 0.23101125817911333, 'eval_runtime': 22.5948, 'eval_samples_per_second': 52.888, 'eval_steps_per_second': 3.319, 'epoch': 26.0, 'step': 3666}, {'loss': 0.2606, 'learning_rate': 8.61597911302745e-07, 'epoch': 27.0, 'step': 3807}, {'eval_loss': 1.345682978630066, 'eval_matthews_correlation': 0.23586857144335793, 'eval_runtime': 22.5754, 'eval_samples_per_second': 52.934, 'eval_steps_per_second': 3.322, 'epoch': 27.0, 'step': 3807}, {'loss': 0.2564, 'learning_rate': 7.953211488948416e-07, 'epoch': 28.0, 'step': 3948}, {'eval_loss': 1.4025861024856567, 'eval_matthews_correlation': 0.22831869291909548, 'eval_runtime': 22.6089, 'eval_samples_per_second': 52.855, 'eval_steps_per_second': 3.317, 'epoch': 28.0, 'step': 3948}, {'loss': 0.2308, 'learning_rate': 7.290443864869382e-07, 'epoch': 29.0, 'step': 4089}, {'eval_loss': 1.470563530921936, 'eval_matthews_correlation': 0.23875910008571513, 'eval_runtime': 22.5648, 'eval_samples_per_second': 52.959, 'eval_steps_per_second': 3.324, 'epoch': 29.0, 'step': 4089}, {'loss': 0.2215, 'learning_rate': 6.627676240790346e-07, 'epoch': 30.0, 'step': 4230}, {'eval_loss': 1.4488650560379028, 'eval_matthews_correlation': 0.23912256714597163, 'eval_runtime': 22.6383, 'eval_samples_per_second': 52.787, 'eval_steps_per_second': 3.313, 'epoch': 30.0, 'step': 4230}, {'loss': 0.2156, 'learning_rate': 5.964908616711312e-07, 'epoch': 31.0, 'step': 4371}, {'eval_loss': 1.5301367044448853, 'eval_matthews_correlation': 0.23493064410565892, 'eval_runtime': 22.5564, 'eval_samples_per_second': 52.978, 'eval_steps_per_second': 3.325, 'epoch': 31.0, 'step': 4371}, {'loss': 0.2127, 'learning_rate': 5.302140992632277e-07, 'epoch': 32.0, 'step': 4512}, {'eval_loss': 1.4604954719543457, 'eval_matthews_correlation': 0.2360120638147077, 'eval_runtime': 22.5565, 'eval_samples_per_second': 52.978, 'eval_steps_per_second': 3.325, 'epoch': 32.0, 'step': 4512}, {'loss': 0.2257, 'learning_rate': 4.639373368553242e-07, 'epoch': 33.0, 'step': 4653}, {'eval_loss': 1.4803723096847534, 'eval_matthews_correlation': 0.23347165386000293, 'eval_runtime': 22.5971, 'eval_samples_per_second': 52.883, 'eval_steps_per_second': 3.319, 'epoch': 33.0, 'step': 4653}, {'loss': 0.2018, 'learning_rate': 3.976605744474208e-07, 'epoch': 34.0, 'step': 4794}, {'eval_loss': 1.5277512073516846, 'eval_matthews_correlation': 0.21829054286920893, 'eval_runtime': 22.5467, 'eval_samples_per_second': 53.001, 'eval_steps_per_second': 3.326, 'epoch': 34.0, 'step': 4794}, {'loss': 0.1867, 'learning_rate': 3.313838120395173e-07, 'epoch': 35.0, 'step': 4935}, {'eval_loss': 1.6618291139602661, 'eval_matthews_correlation': 0.228924597434146, 'eval_runtime': 22.5347, 'eval_samples_per_second': 53.029, 'eval_steps_per_second': 3.328, 'epoch': 35.0, 'step': 4935}, {'loss': 0.188, 'learning_rate': 2.6510704963161385e-07, 'epoch': 36.0, 'step': 5076}, {'eval_loss': 1.6703267097473145, 'eval_matthews_correlation': 0.22944492824899304, 'eval_runtime': 22.572, 'eval_samples_per_second': 52.942, 'eval_steps_per_second': 3.323, 'epoch': 36.0, 'step': 5076}, {'loss': 0.1827, 'learning_rate': 1.988302872237104e-07, 'epoch': 37.0, 'step': 5217}, {'eval_loss': 1.6359800100326538, 'eval_matthews_correlation': 0.22236984035428717, 'eval_runtime': 22.5161, 'eval_samples_per_second': 53.073, 'eval_steps_per_second': 3.331, 'epoch': 37.0, 'step': 5217}, {'loss': 0.1861, 'learning_rate': 1.3255352481580692e-07, 'epoch': 38.0, 'step': 5358}, {'eval_loss': 1.611247181892395, 'eval_matthews_correlation': 0.23104707447603937, 'eval_runtime': 22.5622, 'eval_samples_per_second': 52.965, 'eval_steps_per_second': 3.324, 'epoch': 38.0, 'step': 5358}, {'loss': 0.1875, 'learning_rate': 6.627676240790346e-08, 'epoch': 39.0, 'step': 5499}, {'eval_loss': 1.5983740091323853, 'eval_matthews_correlation': 0.23795805574556775, 'eval_runtime': 22.5213, 'eval_samples_per_second': 53.061, 'eval_steps_per_second': 3.33, 'epoch': 39.0, 'step': 5499}, {'loss': 0.1817, 'learning_rate': 0.0, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 1.5989471673965454, 'eval_matthews_correlation': 0.237094661662564, 'eval_runtime': 22.5695, 'eval_samples_per_second': 52.947, 'eval_steps_per_second': 3.323, 'epoch': 40.0, 'step': 5640}, {'train_runtime': 6508.8773, 'train_samples_per_second': 13.833, 'train_steps_per_second': 0.867, 'total_flos': 2.369073213222912e+16, 'train_loss': 0.422930302180297, 'epoch': 40.0, 'step': 5640}, {'eval_loss': 0.7411968111991882, 'eval_matthews_correlation': 0.45879460817017415, 'eval_runtime': 22.5508, 'eval_samples_per_second': 52.991, 'eval_steps_per_second': 3.326, 'epoch': 40.0, 'step': 5640}]\n","\u003cclass 'list'\u003e\n","True\n","cardiffnlp/bertweet-base-sentiment  is completed for low\n"]}],"source":["run_both_training(\"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\",run_low = True)\n","run_both_training(\"cardiffnlp/bertweet-base-sentiment\",run_low = True)\n","\n","#run_both_training(\"finiteautomata/bertweet-base-sentiment-analysis\",run_low = True)\n","#run_both_training(\"svalabs/twitter-xlm-roberta-bitcoin-sentiment\",run_low = True)\n","\n","#run_both_training(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",run_low = True)\n","#run_both_training(\"amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061\",run_low = True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IEUvCosOxZJF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Aug  5 23:29:32 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   70C    P0    50W / 250W |    731MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-84QX-hcdTAm"},"outputs":[],"source":["!kill -9 -1"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNZFOQ3rj9/MSnNOUEMFvbV","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1yBhFFV8lhhrvIuHWqZXq0rNyFyucdsGI","name":"finetuner-sentiment v3.ipynb","provenance":[{"file_id":"1fLPYM974gspAK8lzDX6jVnX9t_k53Uy0","timestamp":1659707072244},{"file_id":"1NrcZb-f3iK-NOB6kdsr4eWbqQP6hh08Y","timestamp":1659707013573},{"file_id":"1BUoiPdIj_YosKbOIoOiNQnE0VXVErHUg","timestamp":1659144449674}],"version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03c9649268c94cf8a224ee62b9a4f5f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03fcaa235bf74afaba4c64c2469305e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04426976874545119f28f3c7362b841b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aa45b6be5234f2aaac6fd802f77c6b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13d355e2751d4feba333ff8a999ccb8f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1418b9ebe78f4aab87d8c875fc211c7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14ea863a0cf2499b9ef176308e481784":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17b3599b8e5f4f65b06e63683ebb6c61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9a4e9632f9c47509aecf3e11228111b","placeholder":"​","style":"IPY_MODEL_8fa9f3a1debc47f6a44ee1d9c864fd59","value":" 2/2 [00:00\u0026lt;00:00,  5.02ba/s]"}},"17d4f455fa8b4b6fad49a26168dac241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14ea863a0cf2499b9ef176308e481784","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69196b88d016445291dcbfa41e40f087","value":2}},"1954a50f1ff74469b419ecf1e026ed5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a2227354af450f8dcaebe3e96163df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19f211c551f74ccc8ed97a01665ad1f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a2227354af450f8dcaebe3e96163df","placeholder":"​","style":"IPY_MODEL_03fcaa235bf74afaba4c64c2469305e9","value":"100%"}},"2023b7e4ec7e477996a7399daea1136a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"209e3906003c424ab3bbeb34076b507a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"214ea15e2332478f957e031c1ad28b97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2557b88f98204a8b9f05caa91b893ff7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6abf6069df4c33862fc21d3319c925":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dae0583a7724ac8bcb87450c61b2d9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c836d4b35b34d3b8a573144eb48f742":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_626fdcb5b8e54a60b9c60d83cc3a0df1","IPY_MODEL_17d4f455fa8b4b6fad49a26168dac241","IPY_MODEL_17b3599b8e5f4f65b06e63683ebb6c61"],"layout":"IPY_MODEL_9a25680dfda6469691e5ae0e46e93aeb"}},"3d6b3fbe90124aaab5c4b373bf2ccd7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13d355e2751d4feba333ff8a999ccb8f","placeholder":"​","style":"IPY_MODEL_214ea15e2332478f957e031c1ad28b97","value":" 3/3 [00:00\u0026lt;00:00,  5.34ba/s]"}},"3e188714feb54a38abdacc86ff6b8d4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76ce1693d4d14680b3485cf8252499dc","placeholder":"​","style":"IPY_MODEL_f95b9410565c4a4f95ff3fe529cf3eb7","value":"100%"}},"3f171304a7c2498bb952e49c5bef6ceb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79775bb56708418fb42c7b7db6430009","placeholder":"​","style":"IPY_MODEL_2dae0583a7724ac8bcb87450c61b2d9a","value":" 3/3 [00:00\u0026lt;00:00,  5.38ba/s]"}},"40b47ffa8254474ab543037679b9be6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f8715a18bb14cfa9bd95a0cd2309b8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04426976874545119f28f3c7362b841b","placeholder":"​","style":"IPY_MODEL_9a876e9b8832430287f007b5d68db8d5","value":" 4.47k/? [00:00\u0026lt;00:00, 102kB/s]"}},"51b9b3487f8b4fa38b5cfd09a6d2dac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5cc20942551e4809865db24a41ccdc54","IPY_MODEL_aa98493f832a44babf7997208ee07321","IPY_MODEL_d7ec053a4ca448ada69e87b7b52a29a6"],"layout":"IPY_MODEL_f6bbe503df734bc882fbe0bc34c4f006"}},"566a2a80477040aa979cef2c9d38edcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5cc20942551e4809865db24a41ccdc54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aa45b6be5234f2aaac6fd802f77c6b3","placeholder":"​","style":"IPY_MODEL_c1099decaa104e94b2e45160120d2b7b","value":"100%"}},"626fdcb5b8e54a60b9c60d83cc3a0df1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2023b7e4ec7e477996a7399daea1136a","placeholder":"​","style":"IPY_MODEL_c316c5ae8030429a8b7fe18310d48824","value":"100%"}},"663f02df8c244004b3c355ff8a926ebd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b31baa89753c4691975032d62bdfd958","IPY_MODEL_960584c6d35d4d5e83d5d5d8961459ef","IPY_MODEL_9678d2b1c38d4f6cbce4cac175131e30"],"layout":"IPY_MODEL_712db5a6aff6410babc517cddc476676"}},"67c810c23c1549168ac3f503ef74ed63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69196b88d016445291dcbfa41e40f087":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c7d37d1793f42f48ba20d6f912677fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fcc862576bf4a8990fdbb2aefd0b23a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"712db5a6aff6410babc517cddc476676":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72b32a614716490aad19d61676a176df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7695f18984db4a90a06236d97a89a7e9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76ce1693d4d14680b3485cf8252499dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79775bb56708418fb42c7b7db6430009":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88cc3af66c8145388522ee7ef6a5b4e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a54d03b1a1d4a72b01b561b8a62ad07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f3dc9da82eb40669bb425fdf75923e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1954a50f1ff74469b419ecf1e026ed5e","placeholder":"​","style":"IPY_MODEL_df60dd8f3e7445aca221d88754757f73","value":" 3/3 [00:00\u0026lt;00:00,  4.56ba/s]"}},"8fa9f3a1debc47f6a44ee1d9c864fd59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"960584c6d35d4d5e83d5d5d8961459ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7695f18984db4a90a06236d97a89a7e9","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72b32a614716490aad19d61676a176df","value":2}},"9678d2b1c38d4f6cbce4cac175131e30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e3682e2bb744fdfbefb3b20e1925e62","placeholder":"​","style":"IPY_MODEL_566a2a80477040aa979cef2c9d38edcc","value":" 2/2 [00:00\u0026lt;00:00,  5.00ba/s]"}},"9a25680dfda6469691e5ae0e46e93aeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a876e9b8832430287f007b5d68db8d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e3682e2bb744fdfbefb3b20e1925e62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f9455770c2b44b7a55238f20950b051":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a14f8a4d88664de188eda9950115bf47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55364cd87ba47b6875051fb7ee2549d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1418b9ebe78f4aab87d8c875fc211c7b","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f9455770c2b44b7a55238f20950b051","value":3}},"a7a2de9e1126425c98815bd277d0ad3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a94d1d1304764b88a9da7e77325fc40c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2363a1d03af49e2a4d59a3dd54d9edc","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ffea2bb522e24157b88528ffae6cedf7","value":3}},"aa98493f832a44babf7997208ee07321":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1acdbf6ffbb4e60ac6d4aae18c8a880","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af635c15ade6498dbdc305f2b272bd11","value":2}},"aadc996c92524eb0a6b5b4752f912823":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af635c15ade6498dbdc305f2b272bd11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b04cc61ccdc04126a3b054e02581d169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b244035d2a5e4a438cebd89150a6a13b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67c810c23c1549168ac3f503ef74ed63","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88cc3af66c8145388522ee7ef6a5b4e6","value":3}},"b31baa89753c4691975032d62bdfd958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7a2de9e1126425c98815bd277d0ad3a","placeholder":"​","style":"IPY_MODEL_aadc996c92524eb0a6b5b4752f912823","value":"100%"}},"b81390d72f3a4ac5a89fc44f9c37339d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bca330ff14024c148d865adec2e440af","placeholder":"​","style":"IPY_MODEL_209e3906003c424ab3bbeb34076b507a","value":"Downloading builder script: "}},"bca330ff14024c148d865adec2e440af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf4e829dedfe48a78f547c35117a34ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2cc5addf97b44a492de8d193a15e47c","IPY_MODEL_a94d1d1304764b88a9da7e77325fc40c","IPY_MODEL_8f3dc9da82eb40669bb425fdf75923e6"],"layout":"IPY_MODEL_40b47ffa8254474ab543037679b9be6d"}},"c1099decaa104e94b2e45160120d2b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c316c5ae8030429a8b7fe18310d48824":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c445fba869a5411a868e1e4aba825a5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b81390d72f3a4ac5a89fc44f9c37339d","IPY_MODEL_e70f6023e9424bca8731cd448154ab0f","IPY_MODEL_4f8715a18bb14cfa9bd95a0cd2309b8d"],"layout":"IPY_MODEL_03c9649268c94cf8a224ee62b9a4f5f8"}},"c9a4e9632f9c47509aecf3e11228111b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca57b0d5fea24bbc90654f688e4db3b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e188714feb54a38abdacc86ff6b8d4d","IPY_MODEL_a55364cd87ba47b6875051fb7ee2549d","IPY_MODEL_3f171304a7c2498bb952e49c5bef6ceb"],"layout":"IPY_MODEL_8a54d03b1a1d4a72b01b561b8a62ad07"}},"d7ec053a4ca448ada69e87b7b52a29a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e325c3d4f4bf44ea81abd8ca55aba6a5","placeholder":"​","style":"IPY_MODEL_6fcc862576bf4a8990fdbb2aefd0b23a","value":" 2/2 [00:00\u0026lt;00:00,  4.52ba/s]"}},"df60dd8f3e7445aca221d88754757f73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2363a1d03af49e2a4d59a3dd54d9edc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e325c3d4f4bf44ea81abd8ca55aba6a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e70f6023e9424bca8731cd448154ab0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b6abf6069df4c33862fc21d3319c925","max":1705,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c7d37d1793f42f48ba20d6f912677fe","value":1705}},"f1acdbf6ffbb4e60ac6d4aae18c8a880":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2cc5addf97b44a492de8d193a15e47c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2557b88f98204a8b9f05caa91b893ff7","placeholder":"​","style":"IPY_MODEL_b04cc61ccdc04126a3b054e02581d169","value":"100%"}},"f454d09910ec46019bd1e38640bf9144":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19f211c551f74ccc8ed97a01665ad1f0","IPY_MODEL_b244035d2a5e4a438cebd89150a6a13b","IPY_MODEL_3d6b3fbe90124aaab5c4b373bf2ccd7e"],"layout":"IPY_MODEL_a14f8a4d88664de188eda9950115bf47"}},"f6bbe503df734bc882fbe0bc34c4f006":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f95b9410565c4a4f95ff3fe529cf3eb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffea2bb522e24157b88528ffae6cedf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}