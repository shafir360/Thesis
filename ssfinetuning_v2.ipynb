{"cells":[{"cell_type":"markdown","metadata":{"id":"ZNIZqVYxzhZW"},"source":["In this notebook, a lightweight pretrained model [albert](https://arxiv.org/abs/1909.11942) is finetuned on cola dataset, using three semisupervised training method: (i) [PiModel](https://arxiv.org/abs/1610.02242), (ii) [Temporal Ensembling](https://arxiv.org/abs/1610.02242) and (iii) [Co-Training](https://arxiv.org/abs/1610.02242) using ssfinetuning package.\n","\n","\n","We will perform three steps for each model:\n","\n","1. Installation\n","2. Training \n","3. Plotting the results using ssfinetuning plotter.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3225,"status":"ok","timestamp":1658845808021,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"TJvmmVo7loJb","outputId":"81d2f3ad-52e8-460c-84be-69b9f54283ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1658845808022,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"LeqAlz1fd0cc"},"outputs":[],"source":["test_mode = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"fJEcKzH12Z6u"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 13.3 MB/s \n","\u001b[?25hCollecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 18.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 55.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.8.1)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/shafir360/ssfinetuningV2\n","  Cloning https://github.com/shafir360/ssfinetuningV2 to /tmp/pip-req-build-zr6ss7pm\n","  Running command git clone -q https://github.com/shafir360/ssfinetuningV2 /tmp/pip-req-build-zr6ss7pm\n","Requirement already satisfied: torch\u003e=1.7 in /usr/local/lib/python3.7/dist-packages (from ssfinetuningV2==0.2.1) (1.12.0+cu113)\n","Collecting transformers==4.2.2\n","  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 14.9 MB/s \n","\u001b[?25hCollecting datasets==1.5\n","  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 63.4 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib\u003e=3.1.3 in /usr/local/lib/python3.7/dist-packages (from ssfinetuningV2==0.2.1) (3.2.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (0.3.5.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (1.3.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (0.70.13)\n","Collecting tqdm\u003c4.50.0,\u003e=4.27\n","  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 10.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (1.21.6)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (2.23.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 63.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow\u003e=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (6.0.1)\n","Collecting fsspec\n","  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 69.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.5-\u003essfinetuningV2==0.2.1) (4.12.0)\n","Collecting huggingface-hub\u003c0.1.0\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2-\u003essfinetuningV2==0.2.1) (21.3)\n","Collecting tokenizers==0.9.4\n","  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 51.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 63.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2-\u003essfinetuningV2==0.2.1) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2-\u003essfinetuningV2==0.2.1) (2022.6.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (4.1.1)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.3-\u003essfinetuningV2==0.2.1) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u003e=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.3-\u003essfinetuningV2==0.2.1) (3.0.9)\n","Requirement already satisfied: python-dateutil\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.3-\u003essfinetuningV2==0.2.1) (2.8.2)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib\u003e=3.1.3-\u003essfinetuningV2==0.2.1) (0.11.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.1-\u003ematplotlib\u003e=3.1.3-\u003essfinetuningV2==0.2.1) (1.15.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (3.0.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (3.8.1)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets==1.5-\u003essfinetuningV2==0.2.1) (2022.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.2.2-\u003essfinetuningV2==0.2.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.2.2-\u003essfinetuningV2==0.2.1) (1.1.0)\n","Building wheels for collected packages: ssfinetuningV2, sacremoses\n","  Building wheel for ssfinetuningV2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ssfinetuningV2: filename=ssfinetuningV2-0.2.1-py3-none-any.whl size=28761 sha256=3e2ae8844d2896489a2e633c2de90c3a1018213fa33207fa01be3affab67b9cf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-95os2nxt/wheels/9f/22/14/8bcdb730af3fd303ca3b4e1d25a0e36258c35f5103eb0e2d5a\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=a71ec331b146f82e67e61b7007db255aed68a78fe6a41886a65af7d25525d838\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built ssfinetuningV2 sacremoses\n","Installing collected packages: tqdm, xxhash, tokenizers, sacremoses, huggingface-hub, fsspec, transformers, datasets, ssfinetuningV2\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.0\n","    Uninstalling tqdm-4.64.0:\n","      Successfully uninstalled tqdm-4.64.0\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.12.1\n","    Uninstalling tokenizers-0.12.1:\n","      Successfully uninstalled tokenizers-0.12.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.8.1\n","    Uninstalling huggingface-hub-0.8.1:\n","      Successfully uninstalled huggingface-hub-0.8.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.21.0\n","    Uninstalling transformers-4.21.0:\n","      Successfully uninstalled transformers-4.21.0\n","Successfully installed datasets-1.5.0 fsspec-2022.7.0 huggingface-hub-0.0.19 sacremoses-0.0.53 ssfinetuningV2-0.2.1 tokenizers-0.9.4 tqdm-4.49.0 transformers-4.2.2 xxhash-3.0.0\n"]}],"source":["#!pip3 install ssfinetuning\n","!pip install transformers\n","!pip install tokenizers\n","!pip install git+https://github.com/shafir360/ssfinetuningV2\n","#!pip3 install ssfinetuningV2\n","#!pip install ssfinetuning==0.2.1\n","\n","#!git clone https://github.com/shafir360/ssfinetuningV2\n","#!sudo python /content/ssfinetuningV2/setup.py install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"z1rNGGLifoFr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pyarrow\u003e=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n","Requirement already satisfied: tqdm\u003c4.50.0,\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.49.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: huggingface-hub\u003c0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets) (4.1.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets) (3.7.1)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c0.1.0-\u003edatasets) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.9-\u003ehuggingface-hub\u003c0.1.0-\u003edatasets) (3.0.9)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003e=2.19.0-\u003edatasets) (1.24.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003edatasets) (3.8.1)\n","Requirement already satisfied: pytz\u003e=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2022.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-\u003edatasets) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas-\u003edatasets) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}],"source":["#!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","#!pip install wandb\n","import json\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","import tensorflow as tf\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IT72RkAb2ioC"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","import torch\n","from datasets import load_dataset, load_metric\n","from ssfinetuning import train_with_ssl"]},{"cell_type":"markdown","metadata":{"id":"DXFKTVrt3ek4"},"source":["Downloading cola dataset via HuggingFace's [datasets](https://github.com/huggingface/datasets) library."]},{"cell_type":"markdown","metadata":{"id":"VjxcULDi4AEa"},"source":["Setting the seed value for reproducible results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tL7veo4f3V00"},"outputs":[],"source":["\n","torch.manual_seed(970)\n","torch.cuda.manual_seed_all(970)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KSt2Pc-mfRQD"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-73137556-d25b-441b-93ee-8f6f8b856ae1\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e🔥🔥🔥 ANOTHER MASSIVE WIN! 🔥🔥🔥\\n\\nLUNA/USD Take-...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e\u0026amp;gt;There is no inflation\\n\u0026amp;gt;There is some i...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eChina’s largest broadcaster’s NFT series becom...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e❗Bee Network-Cloud Mining on Your Phone ❗☁️⛏️💰...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eHave you heard of $PIT, one of the largest com...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e460\u003c/th\u003e\n","      \u003ctd\u003ethe strongest bullish signal flashed for Bitco...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e461\u003c/th\u003e\n","      \u003ctd\u003ethe strongest bullish signal flashed for Bitco...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e462\u003c/th\u003e\n","      \u003ctd\u003ewhre is the $dot whales\\ndont miss these coins...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e463\u003c/th\u003e\n","      \u003ctd\u003eyou are always talking about Bitcoin but MarkM...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e464\u003c/th\u003e\n","      \u003ctd\u003e||\\n            Buy $ariva! \\n||\\n            ...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e465 rows × 2 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73137556-d25b-441b-93ee-8f6f8b856ae1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-73137556-d25b-441b-93ee-8f6f8b856ae1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-73137556-d25b-441b-93ee-8f6f8b856ae1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                                  text     label\n","0    🔥🔥🔥 ANOTHER MASSIVE WIN! 🔥🔥🔥\\n\\nLUNA/USD Take-...  Positive\n","1    \u0026gt;There is no inflation\\n\u0026gt;There is some i...   Neutral\n","2    China’s largest broadcaster’s NFT series becom...   Neutral\n","3    ❗Bee Network-Cloud Mining on Your Phone ❗☁️⛏️💰...   Neutral\n","4    Have you heard of $PIT, one of the largest com...  Positive\n","..                                                 ...       ...\n","460  the strongest bullish signal flashed for Bitco...  Positive\n","461  the strongest bullish signal flashed for Bitco...  Positive\n","462  whre is the $dot whales\\ndont miss these coins...  Positive\n","463  you are always talking about Bitcoin but MarkM...   Neutral\n","464  ||\\n            Buy $ariva! \\n||\\n            ...   Neutral\n","\n","[465 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-7b5cbfb1-3558-4e4a-a1f6-d7dbfc1c7874\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e\\n\\n  Funds Increase Bitcoin $BTC Exposure\\n R...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e\\n\\n $39,309.32\\n $1.186\\n $389.10\\n $0.8356\\n...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e\\n\\n BTC identified from CryptoMichNL's Tweet,...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e\\n\\nBTC ETH BNB SOL ADA \\n\\n $BTC $43140.45 (1...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e\\n\\nBTCMarkets\\n $88748\\n $841\\n $5385\\n $265\\...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1195\u003c/th\u003e\n","      \u003ctd\u003ethe strongest bullish signal flashed for Bitco...\u003c/td\u003e\n","      \u003ctd\u003eNegative\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1196\u003c/th\u003e\n","      \u003ctd\u003etime to doge to  dogecoin DogecoinToTheMoon bi...\u003c/td\u003e\n","      \u003ctd\u003espam\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1197\u003c/th\u003e\n","      \u003ctd\u003ewe use bitcoin and mises to protect ourselves ...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1198\u003c/th\u003e\n","      \u003ctd\u003eyou are a decrept and your mind is linear.  bi...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1199\u003c/th\u003e\n","      \u003ctd\u003eyou post every 3 days spreading FUD on Bitcoin...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e1200 rows × 2 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b5cbfb1-3558-4e4a-a1f6-d7dbfc1c7874')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-7b5cbfb1-3558-4e4a-a1f6-d7dbfc1c7874 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7b5cbfb1-3558-4e4a-a1f6-d7dbfc1c7874');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                                   text     label\n","0     \\n\\n  Funds Increase Bitcoin $BTC Exposure\\n R...   Neutral\n","1     \\n\\n $39,309.32\\n $1.186\\n $389.10\\n $0.8356\\n...   Neutral\n","2     \\n\\n BTC identified from CryptoMichNL's Tweet,...   Neutral\n","3     \\n\\nBTC ETH BNB SOL ADA \\n\\n $BTC $43140.45 (1...   Neutral\n","4     \\n\\nBTCMarkets\\n $88748\\n $841\\n $5385\\n $265\\...   Neutral\n","...                                                 ...       ...\n","1195  the strongest bullish signal flashed for Bitco...  Negative\n","1196  time to doge to  dogecoin DogecoinToTheMoon bi...      spam\n","1197  we use bitcoin and mises to protect ourselves ...  Positive\n","1198  you are a decrept and your mind is linear.  bi...   Neutral\n","1199  you post every 3 days spreading FUD on Bitcoin...  Positive\n","\n","[1200 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  \u003cdiv id=\"df-077219ae-5730-4b25-b74c-2fda8926bf18\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e🔥🔥🔥 ANOTHER MASSIVE WIN! 🔥🔥🔥\\n\\nLUNA/USD Take-...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e\u0026amp;gt;There is no inflation\\n\u0026amp;gt;There is some i...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eChina’s largest broadcaster’s NFT series becom...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e❗Bee Network-Cloud Mining on Your Phone ❗☁️⛏️💰...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eHave you heard of $PIT, one of the largest com...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1195\u003c/th\u003e\n","      \u003ctd\u003ethe strongest bullish signal flashed for Bitco...\u003c/td\u003e\n","      \u003ctd\u003eNegative\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1196\u003c/th\u003e\n","      \u003ctd\u003etime to doge to  dogecoin DogecoinToTheMoon bi...\u003c/td\u003e\n","      \u003ctd\u003espam\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1197\u003c/th\u003e\n","      \u003ctd\u003ewe use bitcoin and mises to protect ourselves ...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1198\u003c/th\u003e\n","      \u003ctd\u003eyou are a decrept and your mind is linear.  bi...\u003c/td\u003e\n","      \u003ctd\u003eNeutral\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1199\u003c/th\u003e\n","      \u003ctd\u003eyou post every 3 days spreading FUD on Bitcoin...\u003c/td\u003e\n","      \u003ctd\u003ePositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e1665 rows × 2 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-077219ae-5730-4b25-b74c-2fda8926bf18')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-077219ae-5730-4b25-b74c-2fda8926bf18 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-077219ae-5730-4b25-b74c-2fda8926bf18');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                                   text     label\n","0     🔥🔥🔥 ANOTHER MASSIVE WIN! 🔥🔥🔥\\n\\nLUNA/USD Take-...  Positive\n","1     \u0026gt;There is no inflation\\n\u0026gt;There is some i...   Neutral\n","2     China’s largest broadcaster’s NFT series becom...   Neutral\n","3     ❗Bee Network-Cloud Mining on Your Phone ❗☁️⛏️💰...   Neutral\n","4     Have you heard of $PIT, one of the largest com...  Positive\n","...                                                 ...       ...\n","1195  the strongest bullish signal flashed for Bitco...  Negative\n","1196  time to doge to  dogecoin DogecoinToTheMoon bi...      spam\n","1197  we use bitcoin and mises to protect ourselves ...  Positive\n","1198  you are a decrept and your mind is linear.  bi...   Neutral\n","1199  you post every 3 days spreading FUD on Bitcoin...  Positive\n","\n","[1665 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","input = '/content/drive/MyDrive/fyp/fyp2/labelled data concat/full_labelled_data.csv'\n","#input_unlabelled = '/content/drive/MyDrive/fyp/fyp2/tweets16m_cleaned.csv.csv'\n","input_unlabelled = \"/content/drive/MyDrive/fyp/Bitcoin_tweets_cleaned_2.csv\"\n","model_name = \"svalabs/twitter-xlm-roberta-bitcoin-sentiment\"\n","#model_name = \"xlm-roberta-base\"\n","name_of_text_column = \"text\"\n","dir_of_model_out = '/content/drive/MyDrive/test_del'\n","\n","df_ = pd.read_csv(input)\n","if test_mode:\n","  df_unlabelled_ = pd.read_csv(input_unlabelled,nrows=5000)\n","else:\n","  df_unlabelled_ = pd.read_csv(input_unlabelled)\n","\n","\n","df_unlabelled_['tweet'] = df_unlabelled_[['tweet']].astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n","\n","df_ = df_[['text','label']]\n","\n","\n","display(df_)\n","#display(df_unlabelled_)\n","\n","test_path2= '/content/drive/MyDrive/fyp/fyp2/amazon turk improved labels/valid-test result /concat/improved_amazon_concat_valid'\n","train_path = '/content/drive/MyDrive/fyp/fyp2/amazon turk improved labels/train result/concat/improved_amazon_concat_train.csv'\n","test_v2 = pd.read_csv(test_path2)\n","train_v2 = pd.read_csv(train_path)\n","\n","display(train_v2)\n","df_ = pd.concat([df_,train_v2])\n","\n","display(df_)\n","\n","\n","\n","#df_test = df_.merge(on = 'text')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9RL9hexfnDay"},"outputs":[],"source":["#df_t = pd.read_csv(\"/content/drive/MyDrive/fyp/fyp2/Bitcoin_tweets_cleaned.csv\")\n","#df_t = pd.read_csv(\"/content/drive/MyDrive/fyp/fyp2/tweets16m_cleaned.csv.csv\")\n","#display(df_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IZyonhgzkH0A"},"outputs":[],"source":["#df1 = df_.drop_duplicates(subset=['text'])\n","#df1 = pd.read_csv(\"/content/drive/MyDrive/fyp/fyp2/sliced cleaned data/labellingdata28.csv\")\n","#display(df1)\n","#df2 = df_unlabelled_\n","#df2 = df_t.copy()\n","#df_m = df1.merge(df2, left_on='tweet', right_on='tweet')\n","#df_m.drop_duplicates(inplace = True,subset=['text'])\n","#df_m.drop_duplicates(inplace = True,subset=['tweet'])\n","#display(df_m)\n","##turks different due to less emoji"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KPtF2BAkj1yj"},"outputs":[],"source":["def match_with_batchsize(lim, batchsize):\n","    \"\"\"\n","    Function used by modify_datasets below to match return the integer closest to lim\n","    which is multiple of batchsize, i.e., lim%batchsize=0.\n","    \"\"\"\n","\n","    if lim % batchsize == 0:\n","        return lim\n","    else:\n","        return lim - lim % batchsize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TU5JvY-Pfke9"},"outputs":[{"name":"stdout","output_type":"stream","text":["123 40 168\n"]},{"data":{"text/plain":["Dataset({\n","    features: ['sentence', 'label'],\n","    num_rows: 1332\n","})"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Dataset({\n","    features: ['sentence', 'label'],\n","    num_rows: 4992\n","})"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u003cclass 'datasets.dataset_dict.DatasetDict'\u003e\n","DatasetDict({\n","    labeled: Dataset({\n","        features: ['sentence', 'label'],\n","        num_rows: 1332\n","    })\n","    validation: Dataset({\n","        features: ['index', 'sentence', 'label'],\n","        num_rows: 521\n","    })\n","    unlabeled: Dataset({\n","        features: ['sentence', 'label'],\n","        num_rows: 4992\n","    })\n","    labeled1: Dataset({\n","        features: ['sentence', 'label'],\n","        num_rows: 439\n","    })\n","    labeled2: Dataset({\n","        features: ['sentence', 'label'],\n","        num_rows: 893\n","    })\n","    train: Dataset({\n","        features: ['sentence', 'label'],\n","        num_rows: 6324\n","    })\n","})\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 432x288 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["\n","import datasets\n","from datasets import concatenate_datasets, DatasetDict\n","valid_percentage = 0.2\n","batch_size = 12\n","reduce_ = False\n","\n","def ConvertLabel2ModelLabel(label):\n","  if label == 'Positive':\n","    o = 2\n","  elif label == 'Neutral':\n","    o = 1\n","  elif label == 'Negative':\n","    o = 0\n","  elif label == 'spam':\n","    o = 1\n","  else: \n","    print(\" error at ConvertLabel2ModelLabel label is \" , label)\n","    o = np.nan\n","  return o\n","\n","def make_dataset(df_,df_unlabelled_,batch_size,valid_percentage,test_v2_,labeled1_frac=0.33):\n","  df = df_.copy()\n","  test_v2 = test_v2_.copy()\n","  df_unlabelled = df_unlabelled_.copy()\n","\n","  if reduce_:\n","    df_unlabelled = df_unlabelled[0:1000]\n","\n","\n","  #valid_size_each_class = int(df.shape[0] * valid_percentage)\n","  df_unlabelled = df_unlabelled.rename(columns={\"tweet\": \"sentence\"})\n","  df_unlabelled['label'] = -1\n","  df_unlabelled = df_unlabelled[['sentence', 'label']]\n","  df_unlabelled = df_unlabelled.dropna()\n","  #display(df_unlabelled)\n","\n","  #print(valid_size_each_class)\n","\n","\n","\n","\n","  sentences = df['text'].copy().to_list()\n","  labels = []\n","  urls = []\n","\n","\n","\n","\n","  df['label'] = df.label.apply(lambda x : ConvertLabel2ModelLabel(x) )\n","  df = df[['text','label']]\n","  df = df.rename(columns={\"text\": \"sentence\"})\n","  #display(df)\n","\n","  \n","  test_v2['label'] = test_v2.label.apply(lambda x : ConvertLabel2ModelLabel(x) )\n","  test_v2 = test_v2[['text','label']]\n","  test_v2 = test_v2.rename(columns={\"text\": \"sentence\"})\n","\n","\n","  df_pos = df[df['label'] == 2].sample(frac=1).reset_index(drop=False)\n","  df_neg = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n","  df_neu = df[df['label'] == 1].sample(frac=1).reset_index(drop=True)\n","\n","  data = []\n","\n","  distri_class = pd.DataFrame({'class':['Pos', 'Neu', 'Neg'], 'size':[df_pos.shape[0], df_neu.shape[0], df_neg.shape[0]]})\n","  ax = distri_class.plot.bar(x='class', y='size', rot=0)\n","\n","  valid_size_class_pos = int(df_pos.shape[0] * valid_percentage)\n","  valid_size_class_neg = int(df_neg.shape[0] * valid_percentage)\n","  valid_size_class_neu = int(df_neu.shape[0] * valid_percentage)\n","\n","  print(valid_size_class_pos,valid_size_class_neg,valid_size_class_neu)\n","\n","\n","\n","\n","  validation_dataset = df_pos[0:valid_size_class_pos]\n","  validation_dataset = validation_dataset.append(df_neg[0:valid_size_class_neg])\n","  validation_dataset = validation_dataset.append(df_neu[0:valid_size_class_neu])\n","\n","  validation_dataset = validation_dataset.append(test_v2)\n","\n","  validation_dataset = validation_dataset.sample(frac=1).reset_index(drop=True)\n","  validation_dataset = Dataset.from_dict(validation_dataset)\n","\n","\n","  labelled = df_pos[valid_size_class_pos:]\n","  labelled = labelled.append(df_neg[valid_size_class_neg:])\n","  labelled = labelled.append(df_neu[valid_size_class_neu:])\n","  labelled = labelled[0 : match_with_batchsize(labelled.shape[0],batch_size) ]\n","  labelled = labelled.sample(frac=1).reset_index(drop=True)\n","  labelled = labelled.drop(columns = 'index')\n","\n","  lab_lim = match_with_batchsize(labelled.shape[0],batch_size)\n","  labeled1 = labelled[0: int(labeled1_frac * lab_lim)]\n","  labeled2 = labelled[int(labeled1_frac * lab_lim):lab_lim]\n","\n","  labelled =  Dataset.from_dict(labelled)\n","  labeled1 =  Dataset.from_dict(labeled1)\n","  labeled2 =  Dataset.from_dict(labeled2)\n","\n","  df_unlabelled = df_unlabelled[ 0 : match_with_batchsize(df_unlabelled.shape[0],batch_size) ]\n","  df_unlabelled = df_unlabelled.sample(frac=1).reset_index(drop=True)\n","  unlabelled = Dataset.from_dict(df_unlabelled)\n","\n","\n","  dataset = datasets.DatasetDict({\"labeled\":labelled,\"validation\":validation_dataset,'unlabeled':unlabelled ,'labeled1':labeled1,'labeled2':labeled2})\n","  display(dataset['labeled'],dataset['unlabeled'])\n","  dataset['train'] = concatenate_datasets([dataset['labeled'], dataset['unlabeled']])\n","  return dataset\n","\n","\n","dataset = make_dataset(df_,df_unlabelled_,batch_size,valid_percentage,test_v2)\n","print(type(dataset))\n","print(dataset)"]},{"cell_type":"markdown","metadata":{"id":"BEd0lmKT4LME"},"source":["# FineTuning with PiModel\n","\n","## Options chosen for training in this example.\n","\n","1. labeled_fraction: This is the list which divides the dataset into different labeled fractions mentioned as the list elements. For example,\n","if a list of `labeled_fraction` is set as below, the function `train_with_ssl` will first keep the 15% of data as labeled and consider the rest as unlabeled and apply all the combinations of hyperparmeters to obtain the results. Then, it will do the same by keeping 35% of the labels and so on.  \n","\n","2. use_sup: This is whether the trainer should first train a supervised model and then use that trained model as an starting point for the ssl training.\n","\n","3. dataset: Huggingface dataset.\n","\n","4. model_name: NLP transformers model name as used by huggingface library. See this [page](https://huggingface.co/transformers/pretrained_models.html) for more information.\n","\n","5. arg_ta = `transformers.TrainingArguments`, check this [page](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments) for information. If using `train_with_ssl` then, there are default values setup in `default_args` module. These values are similar to this [notebook](https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb) since cola dataset has been used here. If you would like to keep all the other arguments [same](https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb) as this notebook but changed only few, it could be changed with using arg_ta as dictionary and changing that specific argument like below. Otherwise you can also a prepare whole new TrainingArguments object. \n","\n","6. arg_ta_sup: Similar to `arg_ta`, but for supervised model, if `use_sup` or `run_sup` has been enabled.\n","\n","7. ssl_model_type: Semisupervised model type."]},{"cell_type":"markdown","metadata":{"id":"6yxjlyDk-fi0"},"source":["**`unsup_hp`**:\n","\n","This is a dictionary of hyperparameter that you would like test you ssl model against. For example, in the case of PiModel the choices could be:\n","\n","- **w_ramprate**: linear rate at which the unsupervised weight would be increased from the initial value.\n","- **update_weights_steps:** interval steps after which unsupervised weight\n","would be updated by the w_ramprate etc.\n"," \n","Please look at [`PiModel`](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html#ssfinetuning.models.PiModel) or [`TrainerWithUWScheduler`](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html#ssfinetuning.trainer_util.TrainerWithUWScheduler) documentation for more information. \n","In this example we only use vary `w_ramprate`. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aRfjq5bqOJH-"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-49ed7ffe-187a-b3cf-2cef-6911f758bbde)\n","Your runtime has 54.8 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["!nvidia-smi -L\n","#nvidia-smi\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb \u003c 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l7YnUSlxYhA2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n","Requirement already satisfied: llvmlite\u003c0.35,\u003e=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n","Requirement already satisfied: numpy\u003e=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n"]}],"source":["!pip install numba\n","from numba import cuda"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VfYShAsKJ0Y0"},"outputs":[],"source":["from datasets import load_metric\n","metric_type = 'matthews_correlation'\n","#metric_type = 'accuracy'\n","\n","metric = load_metric(metric_type)\n","#batch_size = 8\n","\n","from numba import cuda\n","\n","def compute_metrics(eval_pred):\n","    from datasets import load_metric\n","    metric = load_metric(metric_type)\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","\n","print(\"start training\")\n","\n","\n","\n","sup_stats, stats_pi = train_with_ssl(#labeled_fraction=[0.5], \n","                                     use_sup=True,\n","                                     run_sup = False ,\n","                                     dataset = dataset, \n","                                     #text_column_name='text',\n","                                      num_labels = 3,\n","                                      compute_metrics = compute_metrics,\n","                                     #batch_size = batch_size,\n","                                      remove_dirs=False,\n","                                     #model_name=\"cardiffnlp/twitter-roberta-base-mar2022\", \n","                                     #model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","                                     model_name = \"svalabs/twitter-xlm-roberta-bitcoin-sentiment\",\n","                                     #model_name = \"/content/drive/MyDrive/fyp/fyp2/model/model2-supervised/svalabs/twitter-xlm-roberta-bitcoin-sentiment-finetuned-/run-0/checkpoint-119\",\n","                                     #model_name = \"albert-base-v2\",\n","                                     args_ta={'no_cuda' :False,\n","                                              'num_train_epochs':1,\n","                                              'per_device_train_batch_size':batch_size,\n","                                              #'batch_size' : batch_size,\n","                                              'per_device_eval_batch_size':batch_size,\n","                                              #'evaluation_strategy':'epoch',\n","                                              #'eval_steps' : 'epoch'\n","                                              #'metric_for_best_model':'matthews_correlation'\n","                                             'metric_for_best_model':metric_type,\n","                                              'output_dir':dir_of_model_out,\n","                                              #'save_strategy' : 'no',\n","                                              'load_best_model_at_end':True,\n","                                              'overwrite_output_dir':True,\n","                                              'save_total_limit':2,\n","\n","                                              \n","                                              \n","                                              },\n","                                     args_ta_sup={'no_cuda':False,\n","                                                  'num_train_epochs':2,\n","                                                  #'batch_size': batch_size,\n","                                                  'per_device_train_batch_size':batch_size,\n","                                                  'per_device_eval_batch_size':batch_size,\n","                                                  'metric_for_best_model':metric_type,\n","                                                  #'evaluation_strategy':\"epoch\",\n","                                                 # 'seed':'40',\n","\n","                                                  'learning_rate' : 4.5682e-5,\n","                                                  'weight_decay': 0.01,\n","                                                \n","                                                  },\n","                                     ssl_model_type=\"PiModel\", unsup_hp={'w_ramprate':[0.1]})\n","\n","\n","import torch\n","torch.cuda.empty_cache()\n","\n","#!kill -9 -1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bAv0GL2ElpyE"},"outputs":[],"source":["print(batch_size)\n","run_cotrain = False\n","if run_cotrain:\n","  sup_stats, stats_co = train_with_ssl( model_name=\"/content/drive/MyDrive/fyp/fyp2/model/model2-supervised/svalabs/twitter-xlm-roberta-bitcoin-sentiment-finetuned-/run-0/checkpoint-119\",\n","                                      use_sup=True,\n","                                      dataset = dataset, \n","                                    num_labels = 3,\n","                                    args_ta_sup={'per_device_train_batch_size':batch_size,\n","                                                'no_cuda':False, 'num_train_epochs':1, \n","                                                'per_device_eval_batch_size':batch_size,\n","                                                'learning_rate':1e-5}\n","                                      ,args_ta={'per_device_train_batch_size':batch_size,\n","                                                'num_train_epochs':1,\n","                                                'per_device_eval_batch_size':batch_size,\n","                                                'no_cuda':False,\n","                                                'learning_rate':1e-5}, \n","                                    ssl_model_type=\"CoTrain\", unsup_hp={'p_threshold':[0.2, 0.4, 0.85], 'epoch_per_cotrain':[1]})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XyPks1BTaQsr"},"outputs":[],"source":["from datasets import load_metric\n","import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","metric = load_metric('accuracy')\n","\n","batch_size = 14\n","cont = False\n","\n","def compute_metrics(eval_pred):\n","    from datasets import load_metric\n","    metric = load_metric('accuracy')\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","while cont:\n","  try:\n","    dataset = make_dataset(df_,df_unlabelled_,batch_size,valid_percentage)\n","    #cuda.select_device(0)\n","    #cuda.close()\n","    #torch.cuda.empty_cache()\n","    print(\"start gpu usage and batch size of \",batch_size)\n","    !nvidia-smi\n","    sup_stats, stats_pi = train_with_ssl(#labeled_fraction=[0.5], \n","                                        use_sup=True, \n","                                        dataset = dataset, \n","                                        #text_column_name='text',\n","                                        num_labels = 3,\n","                                        compute_metrics = compute_metrics,\n","                                        #batch_size = batch_size,\n","                                        # remove_dirs=False,\n","                                        model_name=\"cardiffnlp/twitter-roberta-base-mar2022\", \n","                                        #model_name = \"albert-base-v2\",\n","                                        args_ta={'no_cuda' :False,\n","                                                  'num_train_epochs':1,\n","                                                  'per_device_train_batch_size':batch_size,\n","                                                  #'batch_size' : batch_size,\n","                                                  'per_device_eval_batch_size':batch_size,\n","                                                  #'evaluation_strategy':'epoch',\n","                                                  #'eval_steps' : 'epoch'\n","                                                  #'metric_for_best_model':'matthews_correlation'\n","                                                  'metric_for_best_model':'accuracy'\n","                                                  }, \n","                                        args_ta_sup={'no_cuda':False,\n","                                                      'num_train_epochs':1,\n","                                                      #'batch_size': batch_size,\n","                                                      'per_device_train_batch_size':batch_size,\n","                                                  'per_device_eval_batch_size':batch_size,\n","                                                      'metric_for_best_model':'accuracy',\n","                                                      #'evaluation_strategy':\"epoch\",\n","                                                    \n","                                                      },\n","                                        ssl_model_type=\"PiModel\", unsup_hp={'w_ramprate':[0.01]})\n","  except Exception as e:\n","    print(e)\n","    print(\"Failed batch size of- \" , batch_size )\n","    print(\"failed gpu usage\")\n","    !nvidia-smi\n","    if batch_size == 2:\n","      print(\"full fail sorry mate\")\n","      cont = False\n","    else:\n","      batch_size = batch_size - 2\n","      pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y0hGsP7780db"},"outputs":[],"source":["print(sup_stats)\n","print(stats_pi)\n","print(len(sup_stats))\n","print(len(stats_pi))"]},{"cell_type":"markdown","metadata":{"id":"EnD148afCycp"},"source":["This is very time consuming step so just showing results from saved files alongwith correct steps to be used."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"U6G5DXr95qvh"},"outputs":[],"source":["#print(stats_pi[0])\n","#print(stats_pi[0][0]['eval_accuracy'])\n","\n","sup = []\n","unsup0_01 = []\n","unsup0_1 = []\n","\n","for x in sup_stats[0]:\n","  try:\n","    #print(x['eval_accuracy'])\n","    sup.append(x['eval_accuracy'])\n","  except:\n","    pass\n","\n","for x in stats_pi[0]:\n","  try:\n","    #print(x['eval_accuracy'])\n","    unsup0_01.append(x['eval_accuracy'])\n","  except:\n","    pass\n","\n","for x in stats_pi[1]:\n","  try:\n","    unsup0_1.append(x['eval_accuracy'])\n"," \n","  except:\n","    pass\n","\n","x = []\n","unsup0_01.pop()\n","unsup0_1.pop()\n","\n","print(sup)\n","print(unsup0_01)\n","print(unsup0_1)\n","for i in range(len(unsup0_01)):\n","  x.append(i)\n","\n","plt.plot(x,sup , label = \"sup\")\n","plt.plot(x,unsup0_01 , label = \"unsup rampert 0.01\")\n","plt.plot(x,unsup0_1, label = \"unsup rampert 0.1\")\n","plt.legend()\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OvDhFDIXTIfm"},"source":["**You can read more about `plotting_utils` module [here](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html?highlight=plotting#module-ssfinetuning.plotting_utils)**"]},{"cell_type":"markdown","metadata":{"id":"3m-O_QctF4x3"},"source":["# Finetuning with TemporalEnsemble model\n","\n","At this point, the colab notebook has been factory reset with reruns of first 4 cells. So to reproduce the same results, one might do that. \n","\n","In this case, we only check with one labeled fraction with checking out another hyperparamter important for this ssl model, `alpha`.\n","\n","You can read more about this in hyperparameter in [documentation](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html?highlight=alpha#ssfinetuning.models.TemporalEnsembleModel) or in the [original paper](https://arxiv.org/pdf/1610.02242.pdf). "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IZrbUZo_FrWc"},"outputs":[],"source":["from datasets import load_metric\n","metric = load_metric('accuracy')\n","\n","def compute_metrics(eval_pred):\n","    from datasets import load_metric\n","    metric = load_metric('accuracy')\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","\n","\n","sup_stats, stats_te = train_with_ssl( use_sup=True, dataset = dataset,model_name=\"cardiffnlp/twitter-roberta-base-mar2022\", \n","                                     compute_metrics = compute_metrics,\n","                                     num_labels = 3,\n","                                     args_ta={'no_cuda' :False,'num_train_epochs':10,'metric_for_best_model':'accuracy', 'learning_rate':5e-6,\n","                                              'per_device_train_batch_size':batch_size,\n","                                              'per_device_eval_batch_size':batch_size,},\n","                                     args_ta_sup={'no_cuda':False,'num_train_epochs':10,'metric_for_best_model':'accuracy',\n","                                                  'per_device_train_batch_size':batch_size,\n","                                              'per_device_eval_batch_size':batch_size,},\n","                                     ssl_model_type=\"TemporalEnsemble\", unsup_hp={'w_ramprate':[0.01,0.1], 'alpha':[0.3,0.6,0.9]})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7sYkY5kzLvaL"},"outputs":[],"source":["print(sup_stats)\n","print(stats_pi)\n","print(len(sup_stats))\n","print(len(stats_pi))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ok0LjwgWClxk"},"outputs":[],"source":["#print(stats_pi[0])\n","#print(stats_pi[0][0]['eval_accuracy'])\n","\n","sup = []\n","unsup0_01 = []\n","unsup0_1 = []\n","\n","for x in sup_stats[0]:\n","  try:\n","    #print(x['eval_accuracy'])\n","    sup.append(x['eval_accuracy'])\n","  except:\n","    pass\n","\n","for x in stats_pi[0]:\n","  try:\n","    #print(x['eval_accuracy'])\n","    unsup0_01.append(x['eval_accuracy'])\n","  except:\n","    pass\n","\n","for x in stats_pi[1]:\n","  try:\n","    unsup0_1.append(x['eval_accuracy'])\n"," \n","  except:\n","    pass\n","\n","x = []\n","unsup0_01.pop()\n","unsup0_1.pop()\n","\n","print(sup)\n","print(unsup0_01)\n","print(unsup0_1)\n","for i in range(len(unsup0_01)):\n","  x.append(i)\n","\n","plt.plot(x,sup , label = \"sup\")\n","plt.plot(x,unsup0_01 , label = \"unsup rampert 0.01\")\n","plt.plot(x,unsup0_1, label = \"unsup rampert 0.1\")\n","plt.legend()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ncg7HILzHnre"},"outputs":[],"source":["plotting_utils.sort_and_plot(data=dic_to_pandas(stats_te), data_to_compare=dic_to_pandas(sup_stats),\n","                             y_axis_col='eval_matthews_correlation', cols_to_find=[\"w_ramprate\",'alpha'])"]},{"cell_type":"markdown","metadata":{"id":"Jj6zVhghB4yX"},"source":["# Finetuning with CoTrain\n","\n","Continuing with the factory reset. \n","## Changed options:\n","\n","- num_train_epochs: In this case, we change number of training epochs to 5.\n","- learning_rate: learning rate is set 1e-5 instead of 2e-5 for default case.\n","\n","**`usup_hp`**\n","- **`p_threshold`**: Threshold probability for considering exchange between models.\n","- **`epoch_per_cotrain`**:Number of epochs to pass through training data while going through one iteration of cotraining. \n","\n","You can read more about this in the [`TrainerForCoTraining`](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html?highlight=trainerfor#ssfinetuning.trainer_util.TrainerForCoTraining) or [`CoTrain`](https://ssfinetuning.readthedocs.io/en/latest/ssfinetuning.html?highlight=CoTrain#ssfinetuning.models.CoTrain) documentation. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dCjq8KncJ7Mk"},"outputs":[],"source":["#sup_stats, stats_co = train_with_ssl([0.1, 0.5], model_name=\"albert-base-v2\", use_sup=True, dataset = dataset, \n"," #                                 args_ta_sup={'no_cuda':False, 'num_train_epochs':5, 'learning_rate':1e-5},args_ta={'no_cuda':False, 'learning_rate':1e-5}, \n","  #                                ssl_model_type=\"CoTrain\", unsup_hp={'p_threshold':[0.2, 0.4, 0.85], 'epoch_per_cotrain':[5]})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8C115fjGNPB1"},"outputs":[],"source":["plotting_utils.sort_and_plot(dis_col='l_fr', data=dic_to_pandas(stats_co), data_to_compare=dic_to_pandas(sup_stats), \n","                             y_axis_col='eval_matthews_correlation', cols_to_find=['epoch_per_cotrain', 'p_threshold'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RW-9x_9gN4i8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PM5U7dmz36Ib"},"outputs":[],"source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","#learning_rate = 5e-05\n","batch_size = 12\n","#eval_batch_size = 4\n","seed = 42\n","#optimizer = Adam \n","#with betas=(0.9,0.999) and epsilon=1e-08\n","adam_beta1 = 0.9\n","adam_beta2 =0.999\n","lr_scheduler_type = \"linear\"\n","num_epochs = 15\n","\n","arg_ta = TrainingArguments(\n","    f\"{model_name}-finetuned-\",\n","    #report_to=\"wandb\",\n","    evaluation_strategy = \"epoch\",\n","    #logging_strategy = \"epoch\",\n","    #save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=25,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='accuracy'\n","    #push_to_hub=True,\n",")\n","\n","\n","sup_stats, stats_pi = train_with_ssl(labeled_fraction=[1, 1, 1], use_sup=True, dataset = dataset,\n","                                     num_labels=3,\n","                                     #arg_ta = arg_ta,\n","                                     tokenizer=tokenizer,\n","                                     text_column_name = 'text',\n","                                     model_name=model_name, args_ta={'no_cuda' :False}, args_ta_sup={'no_cuda':False},\n","                                     ssl_model_type=\"PiModel\", unsup_hp={'w_ramprate':[0.01,0.1]})"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"ssfinetuning_v2.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}