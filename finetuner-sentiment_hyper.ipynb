{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32772,"status":"ok","timestamp":1659831537567,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"klpNNdQNRAzv","outputId":"316a33d3-a608-4d94-e964-e48332e1913e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32617,"status":"ok","timestamp":1659831570174,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"iX00mr6WfSHG","outputId":"dd49bba7-e229-4af5-f028-c857684eb3f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 15.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 65.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 67.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 14.4 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 71.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 56.0 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.11.1\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 77.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, fsspec, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.4.0 fsspec-2022.7.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 16.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting GPUtil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=0f1d8daf832b557a595a59ccba268eb87ca3df944785308a55124411df041c3e\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built GPUtil\n","Installing collected packages: GPUtil\n","Successfully installed GPUtil-1.4.0\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6781aa6110>"]},"metadata":{},"execution_count":2}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install GPUtil\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","#!pip install wandb\n","import json\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","import tensorflow as tf\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset\n","import torch\n","\n","RANDOM_SEED = 42\n","\n","np.random.seed(RANDOM_SEED)\n","\n","torch.manual_seed(RANDOM_SEED)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":988,"status":"ok","timestamp":1659831571143,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"NoSFcYvATZJ_"},"outputs":[],"source":["\n","def ConvertLabel2ModelLabel(label):\n","  if label == 'Positive':\n","    o = 2\n","  elif label == 'Neutral':\n","    o = 1\n","  elif label == 'Negative':\n","    o = 0\n","  elif label == 'spam':\n","    o = 1\n","  else: \n","    print(\" error at ConvertLabel2ModelLabel label is \" , label)\n","    o = np.nan\n","  return o\n","\n","\n","from GPUtil import showUtilization as gpu_usage\n","from numba import cuda\n","\n","def free_gpu_cache():\n","    print(\"Initial GPU Usage\")\n","    gpu_usage()                             \n","\n","    torch.cuda.empty_cache()\n","\n","    cuda.select_device(0)\n","    cuda.close()\n","    cuda.select_device(0)\n","\n","    print(\"GPU Usage after emptying the cache\")\n","    gpu_usage()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":830},"executionInfo":{"elapsed":3691,"status":"ok","timestamp":1659831574828,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"0Yk0sHM_ff68","outputId":"b14226f0-cdc3-469a-e2ee-eb0ad353c8fa"},"outputs":[{"output_type":"display_data","data":{"text/plain":["      Unnamed: 0                                               text     label  \\\n","0           1027  _Matt African variant can cost bitcoin to cras...  Positive   \n","1           1437  100k Bitcoin is inevitable, after 100k it's un...  Negative   \n","2            699  Full thanks to  as they have analysed this Bit...   Neutral   \n","3           1861  If Bitcoin  closes with the weekly candle abov...  Positive   \n","4            497  12 $BTC sold into the bid @ 34111    👈 Bitcoin...   Neutral   \n","...          ...                                                ...       ...   \n","3147         610  🌹List your token on FegEx. Just DM . ListOnFEG...   Neutral   \n","3148         842  MATIC__Polygon   CRYPTO    MARKET CRYPTO BUY_|...   Neutral   \n","3149         303  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...   Neutral   \n","3150        1544  Full thanks to  as they have analysed this Bit...  Positive   \n","3151        2062  Billionaire investor Warren Buffett has once a...  Positive   \n","\n","             source quality  \n","0      new_turk_low     low  \n","1      new_turk_low     low  \n","2     new_turk_high    high  \n","3      new_turk_low     low  \n","4     new_turk_high    high  \n","...             ...     ...  \n","3147   new_turk_low     low  \n","3148  new_turk_high    high  \n","3149    self_manual    high  \n","3150   new_turk_low     low  \n","3151   new_turk_low     low  \n","\n","[3152 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-d6f71bcc-33bb-4835-a336-0d02b53f825e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>source</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1027</td>\n","      <td>_Matt African variant can cost bitcoin to cras...</td>\n","      <td>Positive</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1437</td>\n","      <td>100k Bitcoin is inevitable, after 100k it's un...</td>\n","      <td>Negative</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>699</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>Neutral</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1861</td>\n","      <td>If Bitcoin  closes with the weekly candle abov...</td>\n","      <td>Positive</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>497</td>\n","      <td>12 $BTC sold into the bid @ 34111    👈 Bitcoin...</td>\n","      <td>Neutral</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3147</th>\n","      <td>610</td>\n","      <td>🌹List your token on FegEx. Just DM . ListOnFEG...</td>\n","      <td>Neutral</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>3148</th>\n","      <td>842</td>\n","      <td>MATIC__Polygon   CRYPTO    MARKET CRYPTO BUY_|...</td>\n","      <td>Neutral</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3149</th>\n","      <td>303</td>\n","      <td>__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...</td>\n","      <td>Neutral</td>\n","      <td>self_manual</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3150</th>\n","      <td>1544</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>Positive</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>3151</th>\n","      <td>2062</td>\n","      <td>Billionaire investor Warren Buffett has once a...</td>\n","      <td>Positive</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3152 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6f71bcc-33bb-4835-a336-0d02b53f825e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d6f71bcc-33bb-4835-a336-0d02b53f825e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d6f71bcc-33bb-4835-a336-0d02b53f825e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["      Unnamed: 0                                               text     label  \\\n","0            280  _vlz    bluesparrow will take over the world T...   Neutral   \n","1            349  Listing on coinbase couple of months ago ..  /...   Neutral   \n","2            276  Putting utility and longevity aside, I need to...  Positive   \n","3            239  I am very thankful to  for knowing me this dan...  Negative   \n","4            395  New intraday forecasts for EURUSD GBPUSD(Free)...   Neutral   \n","...          ...                                                ...       ...   \n","1010         461  Huuuuge News!!! New Utility Income Breakdown i...   Neutral   \n","1011         202  Full thanks to  as they have analysed this Bit...  Positive   \n","1012          74  Full thanks to  as they have analysed this Bit...  Positive   \n","1013         264  This zesty faucet from @_bitcoiner is making m...   Neutral   \n","1014          56  2021 was definitely the year of Hype, from Bit...  Negative   \n","\n","                source quality  \n","0     incomplete_valid    high  \n","1     incomplete_valid    high  \n","2               manual    high  \n","3        new_turk_high    high  \n","4     incomplete_valid    high  \n","...                ...     ...  \n","1010     new_turk_high    high  \n","1011  incomplete_valid    high  \n","1012  incomplete_valid    high  \n","1013  incomplete_valid    high  \n","1014     new_turk_high    high  \n","\n","[1015 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-47a4ea01-87e4-467a-a576-da6979884a98\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>source</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>280</td>\n","      <td>_vlz    bluesparrow will take over the world T...</td>\n","      <td>Neutral</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>349</td>\n","      <td>Listing on coinbase couple of months ago ..  /...</td>\n","      <td>Neutral</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>276</td>\n","      <td>Putting utility and longevity aside, I need to...</td>\n","      <td>Positive</td>\n","      <td>manual</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>239</td>\n","      <td>I am very thankful to  for knowing me this dan...</td>\n","      <td>Negative</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>395</td>\n","      <td>New intraday forecasts for EURUSD GBPUSD(Free)...</td>\n","      <td>Neutral</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1010</th>\n","      <td>461</td>\n","      <td>Huuuuge News!!! New Utility Income Breakdown i...</td>\n","      <td>Neutral</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1011</th>\n","      <td>202</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>Positive</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1012</th>\n","      <td>74</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>Positive</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1013</th>\n","      <td>264</td>\n","      <td>This zesty faucet from @_bitcoiner is making m...</td>\n","      <td>Neutral</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1014</th>\n","      <td>56</td>\n","      <td>2021 was definitely the year of Hype, from Bit...</td>\n","      <td>Negative</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1015 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47a4ea01-87e4-467a-a576-da6979884a98')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-47a4ea01-87e4-467a-a576-da6979884a98 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-47a4ea01-87e4-467a-a576-da6979884a98');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["train_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_final_5.0:3.0:3.0/train_dataset.csv'\n","valid_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_final_5.0:3.0:3.0/valid_dataset.csv'\n","model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n","#out = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/sva_eval'\n","out = \"/content/temp\"\n","\n","import shutil\n","\n","if not os.path.exists(out):\n","  \n","  # Create a new directory because it does not exist \n","  os.makedirs(out)\n","else:\n","  shutil.rmtree(out)\n","#shutil.rmtree(out)\n","\n","name_of_text_column = \"text\"\n","hyperparameter_tuning = True\n","reduce_ = False\n","num_hyper_trails = 300\n","hyper_size = 500\n","epoch = 10\n","\n","df_train = pd.read_csv(train_path)\n","df_valid = pd.read_csv(valid_path)\n","\n","display(df_train)\n","display(df_valid)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":830},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1659831574828,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"GoIvErk1T_LX","outputId":"16ce269b-d157-4a17-e40d-733abc28ec45"},"outputs":[{"output_type":"display_data","data":{"text/plain":["      Unnamed: 0                                               text  label  \\\n","0           1027  _Matt African variant can cost bitcoin to cras...      2   \n","1           1437  100k Bitcoin is inevitable, after 100k it's un...      0   \n","2            699  Full thanks to  as they have analysed this Bit...      1   \n","3           1861  If Bitcoin  closes with the weekly candle abov...      2   \n","4            497  12 $BTC sold into the bid @ 34111    👈 Bitcoin...      1   \n","...          ...                                                ...    ...   \n","3147         610  🌹List your token on FegEx. Just DM . ListOnFEG...      1   \n","3148         842  MATIC__Polygon   CRYPTO    MARKET CRYPTO BUY_|...      1   \n","3149         303  __crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...      1   \n","3150        1544  Full thanks to  as they have analysed this Bit...      2   \n","3151        2062  Billionaire investor Warren Buffett has once a...      2   \n","\n","             source quality  \n","0      new_turk_low     low  \n","1      new_turk_low     low  \n","2     new_turk_high    high  \n","3      new_turk_low     low  \n","4     new_turk_high    high  \n","...             ...     ...  \n","3147   new_turk_low     low  \n","3148  new_turk_high    high  \n","3149    self_manual    high  \n","3150   new_turk_low     low  \n","3151   new_turk_low     low  \n","\n","[3152 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-ddd14603-a8b9-4ad7-b77a-1623924c4111\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>source</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1027</td>\n","      <td>_Matt African variant can cost bitcoin to cras...</td>\n","      <td>2</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1437</td>\n","      <td>100k Bitcoin is inevitable, after 100k it's un...</td>\n","      <td>0</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>699</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>1</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1861</td>\n","      <td>If Bitcoin  closes with the weekly candle abov...</td>\n","      <td>2</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>497</td>\n","      <td>12 $BTC sold into the bid @ 34111    👈 Bitcoin...</td>\n","      <td>1</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3147</th>\n","      <td>610</td>\n","      <td>🌹List your token on FegEx. Just DM . ListOnFEG...</td>\n","      <td>1</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>3148</th>\n","      <td>842</td>\n","      <td>MATIC__Polygon   CRYPTO    MARKET CRYPTO BUY_|...</td>\n","      <td>1</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3149</th>\n","      <td>303</td>\n","      <td>__crypto _tw  🔥 SanjiInu 🔥  💥 KYC, CHARITY, NF...</td>\n","      <td>1</td>\n","      <td>self_manual</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3150</th>\n","      <td>1544</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>2</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","    <tr>\n","      <th>3151</th>\n","      <td>2062</td>\n","      <td>Billionaire investor Warren Buffett has once a...</td>\n","      <td>2</td>\n","      <td>new_turk_low</td>\n","      <td>low</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3152 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddd14603-a8b9-4ad7-b77a-1623924c4111')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ddd14603-a8b9-4ad7-b77a-1623924c4111 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ddd14603-a8b9-4ad7-b77a-1623924c4111');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["      Unnamed: 0                                               text  label  \\\n","0            280  _vlz    bluesparrow will take over the world T...      1   \n","1            349  Listing on coinbase couple of months ago ..  /...      1   \n","2            276  Putting utility and longevity aside, I need to...      2   \n","3            239  I am very thankful to  for knowing me this dan...      0   \n","4            395  New intraday forecasts for EURUSD GBPUSD(Free)...      1   \n","...          ...                                                ...    ...   \n","1010         461  Huuuuge News!!! New Utility Income Breakdown i...      1   \n","1011         202  Full thanks to  as they have analysed this Bit...      2   \n","1012          74  Full thanks to  as they have analysed this Bit...      2   \n","1013         264  This zesty faucet from @_bitcoiner is making m...      1   \n","1014          56  2021 was definitely the year of Hype, from Bit...      0   \n","\n","                source quality  \n","0     incomplete_valid    high  \n","1     incomplete_valid    high  \n","2               manual    high  \n","3        new_turk_high    high  \n","4     incomplete_valid    high  \n","...                ...     ...  \n","1010     new_turk_high    high  \n","1011  incomplete_valid    high  \n","1012  incomplete_valid    high  \n","1013  incomplete_valid    high  \n","1014     new_turk_high    high  \n","\n","[1015 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-6fbeb7ca-716e-4ce7-bef9-c55b2aaae5d5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>source</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>280</td>\n","      <td>_vlz    bluesparrow will take over the world T...</td>\n","      <td>1</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>349</td>\n","      <td>Listing on coinbase couple of months ago ..  /...</td>\n","      <td>1</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>276</td>\n","      <td>Putting utility and longevity aside, I need to...</td>\n","      <td>2</td>\n","      <td>manual</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>239</td>\n","      <td>I am very thankful to  for knowing me this dan...</td>\n","      <td>0</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>395</td>\n","      <td>New intraday forecasts for EURUSD GBPUSD(Free)...</td>\n","      <td>1</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1010</th>\n","      <td>461</td>\n","      <td>Huuuuge News!!! New Utility Income Breakdown i...</td>\n","      <td>1</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1011</th>\n","      <td>202</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>2</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1012</th>\n","      <td>74</td>\n","      <td>Full thanks to  as they have analysed this Bit...</td>\n","      <td>2</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1013</th>\n","      <td>264</td>\n","      <td>This zesty faucet from @_bitcoiner is making m...</td>\n","      <td>1</td>\n","      <td>incomplete_valid</td>\n","      <td>high</td>\n","    </tr>\n","    <tr>\n","      <th>1014</th>\n","      <td>56</td>\n","      <td>2021 was definitely the year of Hype, from Bit...</td>\n","      <td>0</td>\n","      <td>new_turk_high</td>\n","      <td>high</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1015 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fbeb7ca-716e-4ce7-bef9-c55b2aaae5d5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6fbeb7ca-716e-4ce7-bef9-c55b2aaae5d5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6fbeb7ca-716e-4ce7-bef9-c55b2aaae5d5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["df_train['label'] = df_train['label'].apply(ConvertLabel2ModelLabel)\n","df_valid['label'] = df_valid['label'].apply(ConvertLabel2ModelLabel)\n","\n","\n","display(df_train)\n","display(df_valid)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":708,"status":"ok","timestamp":1659831574828,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"9Xi5yBzDgJrk","outputId":"2a294169-0e7a-4515-a0cc-8fc270617daf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality'],\n","    num_rows: 3152\n","})\n","Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality'],\n","    num_rows: 1015\n","})\n","Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality'],\n","    num_rows: 500\n","})\n"]}],"source":["if reduce_:\n","  df_train = df_train[0:200]\n","\n","df_train_hyper = df_train[0:hyper_size] \n","\n","dataset = ds.dataset(pa.Table.from_pandas(df_valid).to_batches())\n","### convert to Huggingface dataset\n","validation_dataset_torch = Dataset(pa.Table.from_pandas(df_valid))\n","\n","dataset = ds.dataset(pa.Table.from_pandas(df_train).to_batches())\n","### convert to Huggingface dataset\n","training_dataset_torch = Dataset(pa.Table.from_pandas(df_train))\n","\n","dataset = ds.dataset(pa.Table.from_pandas(df_train_hyper).to_batches())\n","### convert to Huggingface dataset\n","training_dataset_torch_hyper = Dataset(pa.Table.from_pandas(df_train_hyper))\n","\n","\n","print(training_dataset_torch)\n","print(validation_dataset_torch)\n","print(training_dataset_torch_hyper)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":704,"status":"ok","timestamp":1659831574829,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"aNeojj49TM9s"},"outputs":[],"source":["\n","\n","def balanced_df(df_):\n","  #print(df.shape[0])\n","  df = df_.copy()\n","  df = df[['text','label']]\n","\n","  df_pos = df[df['label'] == 2].sample(frac=1).reset_index(drop=True)\n","  df_neg = df[df['label'] == 0].sample(frac=1).reset_index(drop=True)\n","  df_neu = df[df['label'] == 1 ].sample(frac=1).reset_index(drop=True)\n","  \n","\n","  pos = df_pos.shape[0]\n","  a = pos\n","  #print(a)\n","  neg = df_neg.shape[0]\n","  b = neg\n","  #print(b)\n","  neu = df_neu.shape[0]\n","  c = neu\n","  #print(c)\n","  #print(a+b+c)\n","  if a < b and a < c :\n","    smallest = a\n","  if b < a and b < c :\n","    smallest = b\n","  if c < a and c < b :\n","    smallest = c\n","\n","  smallest = int(smallest/2)\n","\n","  li_test = [df_pos[0:smallest],df_neg[0:smallest],df_neu[0:smallest]]\n","  li_valid = [df_pos[smallest:smallest*2],df_neg[smallest:smallest*2],df_neu[smallest:smallest*2]]\n","  #print(\"check\")\n","  #display(df_pos[0:smallest])\n","  unused_li = [df_pos[smallest*2:],df_neg[smallest*2:],df_neu[smallest*2:]]\n","  \n","\n","  return  pd.concat(li_valid,axis=0).sample(frac=1).reset_index(drop=True),pd.concat(li_test,axis=0).sample(frac=1).reset_index(drop=True), pd.concat(unused_li,axis=0).sample(frac=1).reset_index(drop=True)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":6484,"status":"ok","timestamp":1659831580610,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"y0aboBlPf3Fs","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["1782b42d886d4a3f96df537eed30aede","693d3965471a477fb37e0e835271064f","987b7030732546e2b1b16357ab530089","42f4089901c149268ebb4734fca4bcfa","a3230ec4a6444e7f853e6a7ee1b220d0","e22d3616ada84b4ba6b851f4f9c1b463","113a6618391b4ccc9e69e7dcf69b42a2","f7f1f21d15c140acbca2b4ebc97c162a","efc6c9facc964275858892066131718f","e60fc7dc062b4621be02813f45511eb8","add2b3debc214d929e5ab3b71f1815d7","f2ae1a3a0b264623bc0973d3a0f88441","946343733a204c159dbc9d80d20fc1d2","76dd03776d4f4d31b3deaed2263559e3","40fef74c39ff49ec9411f67746259ed0","838c2b3a6e464e30a80ca555c002a6bf","19fac454ca6d4f02b4e21a637caaac15","a8eacba920554ad09aea1aecfc54743e","6c69cdb9b0ac429583eaa03febc3c6e4","5f7a0e77ba3d4fff818f5feade99db20","703376c205dc4c5b8f1600621799fc9c","e326cb512041456b9fe4eb3b03c0833a","696b96f2ffc4452fb90c350822483fe3","e9ce29647cd0436e9cae8cc70d7517fd","0c71ea1394e1400ab696eefa2cb30492","defdac1cdccf48099bda164e4b656079","7f890e1c1fc4473aacee37dece3854f2","777fe49ca4cc40dd9bbe46be60087194","1db8ed1f3ccc46faa2b6d601ff5d4ab1","e3798742e9d049cebc93a1f8e8b19140","db830aab0b4745ff8559e40e02859082","2d79b519f38d4a8c9fe3ceb736318765","272fa9bce0a643e7808ca38f2f9cb6a7"]},"outputId":"2d6e7021-b2dc-4a02-94fd-70ca89f5962d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1782b42d886d4a3f96df537eed30aede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading sentencepiece.bpe.model:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ae1a3a0b264623bc0973d3a0f88441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696b96f2ffc4452fb90c350822483fe3"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name,model_max_length=512)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339,"referenced_widgets":["0f106d04bded4041afd74a5de590950c","ffa5a7cbea4647df9b43b7ba232612a3","364f6f5f68cc41dca8dfc1f33cfc698e","4f947cadf1d3420db812db14386fa60d","bba8f06c64274ad4910fd1650c5734c3","25316bf5295b44a8a5a82da0b2782811","a4eb3b4435f54953af4a6405d9accb68","74439ba28a8445fd833e2bd1ef538c86","4337ac82fc334bb9b521080b0497bad1","e1189379089d45bcaa8918a9d541821d","b55ddeb9ffcb4ce4b186d5ed23ae687c","aecd9efdfd17416aba56440b240501ca","3efd7d6add254fc6a4091c6b4eb499a4","b43c6dfd2343471099f92b1d11ccb158","40d5d7abb7cb4cc391982392220f9f66","051693cc188b4177905c594cde5de7b5","3fa730b452ca45a588f485db5b697c58","8f94808af7054b3695ee36c21fa17bc5","6f0e44edd5fd4ab6a7f1fa97e679f2df","d0681f0e61cb49be9327146531349326","8153f677ecce463b9ee6424d02ec3824","3cc0b787a45a4e2080ae0b9c54d31bf5","4c121fd87a3641e792925b9d6be3bf74","e19e58bdb0f8453aa467661b3184ebd2","23f781a9ea484c4da64822ea5b13d703","a5bfda4f1a8b402bae88d8d5473b0020","8008442de894474b9813705775afb5dd","862007405a1c451c8ebf4880414caab5","fd5e58cceadf4868ab56be043dc68d6a","5f77e1f171414f4d8729502a7a1825fc","adb0b80200c74454b2f9baab1d92925b","277cfadce5c94d0598c01b081e4dc937","19e5d477f0c04c1f9d192cc2676d4358"]},"executionInfo":{"elapsed":2824,"status":"ok","timestamp":1659831583412,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"sk0C5LiPh7lh","outputId":"a42c6c65-482b-4470-ec08-adb057d2ebea"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'datasets.arrow_dataset.Dataset'>\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f106d04bded4041afd74a5de590950c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecd9efdfd17416aba56440b240501ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c121fd87a3641e792925b9d6be3bf74"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality', 'input_ids', 'attention_mask'],\n","    num_rows: 3152\n","})\n","Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality', 'input_ids', 'attention_mask'],\n","    num_rows: 1015\n","})\n","Dataset({\n","    features: ['Unnamed: 0', 'text', 'label', 'source', 'quality', 'input_ids', 'attention_mask'],\n","    num_rows: 500\n","})\n"]}],"source":["def tokenize_function(data):\n","    return tokenizer(data['text'], padding=\"max_length\", truncation=True,)\n","\n","print(type(training_dataset_torch))\n","train_dataset = training_dataset_torch.map(tokenize_function, batched=True)\n","eval_dataset = validation_dataset_torch.map(tokenize_function, batched=True)\n","train_dataset_hyper = training_dataset_torch_hyper.map(tokenize_function, batched=True)\n","print(train_dataset)\n","print(eval_dataset)\n","print(train_dataset_hyper)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1368,"status":"ok","timestamp":1659831584772,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"wLtrOH4w_Unn","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["40447846eed442c8a5b7afbb7d0a6569","0207c005d9e34af4a347adacacef0ae3","0737fcfae0c04bd2a66fcb58042bede1","0d310bacbbd0419e9a06c6a0455b4dd9","651fa098fc534916b4ebb14218c50d03","c78e93e90d154d0aa7fc41a091dee077","f448c0fa7703492f8b253637388852cc","7f7089d916984c90a71455f2a250f336","f42ce2203f864527aba72c98e493b524","116c90efde8f43d3850933f1c8a15c55","8e9cf73183e0429197c372fa3d53fc7a"]},"outputId":"76a21a2b-d323-4902-fbc9-e69d83fcbee4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40447846eed442c8a5b7afbb7d0a6569"}},"metadata":{}}],"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"matthews_correlation\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"executionInfo":{"elapsed":32356,"status":"ok","timestamp":1659831617109,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"FomHyQrikzKi","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f3a0a656fb4f41d0a78285a0db873a79","8f3ad4a9a76042038e74666f4a0b5d57","495f3a528d9e4e38b66c388526fc85d7","38457922428e4d59afa7e58249841e88","f77c186e221640af97c066724756d979","abc9ae04d1e24a898687ece2325516ac","110a48c02f37433d99ac1aee89ba2eec","437e113ebbf449f8a360a3f944400df4","124305aad197453589de22082f2953a3","93b54b200013407ab6d12e5f8cb668a3","eafc018254a1461a9d6b441cac3a4b00"]},"outputId":"01ea912c-64eb-416a-e470-5575a819a0cf"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a0a656fb4f41d0a78285a0db873a79"}},"metadata":{}}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","#model.to(device)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":117,"status":"ok","timestamp":1659831617115,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"SG6S7inA_RKv"},"outputs":[],"source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","#learning_rate = 5e-05\n","batch_size = 8\n","#eval_batch_size = 4\n","seed = 40\n","#optimizer = Adam \n","#with betas=(0.9,0.999) and epsilon=1e-08\n","adam_beta1 = 0.9\n","adam_beta2 =0.999\n","lr_scheduler_type = \"linear\"\n","num_epochs = 15\n","#args = TrainingArguments(\"test_trainer\",report_to=\"wandb\" ,logging_strategy = \"epoch\",evaluation_strategy=\"epoch\",learning_rate = learning_rate,num_train_epochs = num_epochs,lr_scheduler_type =lr_scheduler_type, adam_beta1 = adam_beta1,adam_beta2 =adam_beta2  )\n","\n","args = TrainingArguments(\n","    #'/content/drive/MyDrive/fyp/fyp2/model/model2-supervised/' + f\"{model_name}-finetuned-\",\n","    #'/content/' + f\"{model_name}-finetuned-\",\n","    out,\n","    #report_to=\"wandb\",\n","    overwrite_output_dir = True,\n","    logging_dir = out,\n","    evaluation_strategy = \"epoch\",\n","    logging_strategy = \"epoch\",\n","    #save_strategy = \"NO\",\n","    learning_rate=4.5682e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=epoch,\n","    weight_decay=0.01,\n","    #metric_for_best_model='matthews_correlation',\n","    save_total_limit = 2,\n","    save_strategy = \"no\",\n","    load_best_model_at_end = False,\n","    seed = 40\n","    #push_to_hub=True,\n",")\n","\n","\n","\n","\n","#trainer = Trainer(\n","   # model=model,\n","    #args=args,\n","   # train_dataset=train_dataset,\n","  #  eval_dataset=eval_dataset,\n"," #   compute_metrics=compute_metrics,\n","#)\n","\n","if hyperparameter_tuning == False:\n","\n","  trainer = Trainer(\n","      model,\n","      args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics\n","  )\n","  # training\n","  train_result = trainer.train() \n","  logs = trainer.state.log_history\n","  # compute train results\n","  metrics = train_result.metrics\n","  max_train_samples = len(train_dataset)\n","  metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","  # save train results\n","  trainer.log_metrics(\"train\", metrics)\n","  trainer.save_metrics(\"train\", metrics)\n","\n","  # compute evaluation results\n","  metrics = trainer.evaluate()\n","  max_val_samples = len(eval_dataset)\n","  metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n","\n","  # save evaluation results\n","  trainer.log_metrics(\"eval\", metrics)\n","  trainer.save_metrics(\"eval\", metrics)\n","\n","  import pickle\n","  save_logs_dir = out\n","  name_logs = \"logs\"\n","\n","  my_file = save_logs_dir + \"/\" + name_logs + \".p\"\n","  print(\"savint to \" , my_file)\n","\n","  pickle.dump( logs, open( my_file, \"wb\" )    ) \n","\n","  ld = pickle.load( open( my_file, \"rb\" ) )\n","\n","\n","  print(logs)\n","  print(logs[2])\n","  print(type(logs))\n","\n","  print(ld)\n","  print(type(ld))\n","  print(logs == ld)\n","  \n","  #trainer.evaluate()\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yi8FfU0EOJe-","outputId":"518d7016-1e2a-47d7-cc23-800096674ac0","executionInfo":{"status":"ok","timestamp":1659831627564,"user_tz":-60,"elapsed":10549,"user":{"displayName":"S Rahman","userId":"04000762971548563689"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 14.8 MB/s \n","\u001b[?25hCollecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Collecting alembic\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 63.0 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n","Collecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 10.9 MB/s \n","\u001b[?25hCollecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.39)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.12.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.9.0)\n","Collecting Mako\n","  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.9 MB/s \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 71.1 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 75.1 MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=36cd86139e1d649015c8d6f641de90c472b2e9ed17b4e62e76bc9fd4c62e1cb5\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.1 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}],"source":["#!pip install ray[tune]\n","!pip install optuna\n","import optuna"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xy3fDdz4Tjpy","executionInfo":{"status":"ok","timestamp":1659831632470,"user_tz":-60,"elapsed":4952,"user":{"displayName":"S Rahman","userId":"04000762971548563689"}},"outputId":"21cb2cc4-6940-40dd-d6e8-4c62fadba05d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 15.3 MB/s \n","\u001b[?25hInstalling collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n"]}],"source":["!pip install pickle5\n","import pickle5 as pickle"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ZUDUheF9OPp-","executionInfo":{"status":"ok","timestamp":1659831632471,"user_tz":-60,"elapsed":29,"user":{"displayName":"S Rahman","userId":"04000762971548563689"}}},"outputs":[],"source":["def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IEUvCosOxZJF","executionInfo":{"status":"ok","timestamp":1659831633089,"user_tz":-60,"elapsed":645,"user":{"displayName":"S Rahman","userId":"04000762971548563689"}},"outputId":"0ff0e43c-beaa-4000-8dbd-10f6024603c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Aug  7 00:20:32 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    24W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CmNuQNpwq_vB","outputId":"1b95bc0a-1598-4447-e496-f3e4cbd6c222"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","\u001b[32m[I 2022-08-07 00:20:43,103]\u001b[0m A new study created in memory with name: no-name-8eed4640-bb32-4706-a3c3-0494dc8a3c21\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 63\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.960500</td>\n","      <td>0.839276</td>\n","      <td>0.336672</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:21:29,794]\u001b[0m Trial 0 finished with value: 0.3366717497039904 and parameters: {'learning_rate': 1.0671839972477622e-05, 'num_train_epochs': 1, 'seed': 15, 'per_device_train_batch_size': 8}. Best is trial 0 with value: 0.3366717497039904.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 315\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 02:52, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.992200</td>\n","      <td>0.850254</td>\n","      <td>0.375627</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.911400</td>\n","      <td>0.818366</td>\n","      <td>0.376046</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.899700</td>\n","      <td>0.823815</td>\n","      <td>0.371501</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.875500</td>\n","      <td>0.819205</td>\n","      <td>0.373184</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.886000</td>\n","      <td>0.817568</td>\n","      <td>0.373184</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:24:26,673]\u001b[0m Trial 1 finished with value: 0.3731839642071969 and parameters: {'learning_rate': 1.5310451170128202e-06, 'num_train_epochs': 5, 'seed': 30, 'per_device_train_batch_size': 64}. Best is trial 1 with value: 0.3731839642071969.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 315\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 02:50, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.016700</td>\n","      <td>0.840658</td>\n","      <td>0.373209</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.932500</td>\n","      <td>0.833416</td>\n","      <td>0.377440</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.905200</td>\n","      <td>0.827840</td>\n","      <td>0.370956</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.892200</td>\n","      <td>0.820507</td>\n","      <td>0.378492</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.897800</td>\n","      <td>0.819063</td>\n","      <td>0.380658</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:27:21,643]\u001b[0m Trial 2 finished with value: 0.38065812029611495 and parameters: {'learning_rate': 1.2342011415651158e-06, 'num_train_epochs': 5, 'seed': 3, 'per_device_train_batch_size': 16}. Best is trial 2 with value: 0.38065812029611495.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 315\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [315/315 02:51, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.002700</td>\n","      <td>0.901632</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.927100</td>\n","      <td>0.865550</td>\n","      <td>0.177793</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.771200</td>\n","      <td>0.855389</td>\n","      <td>0.478589</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.552100</td>\n","      <td>0.908713</td>\n","      <td>0.447088</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.325300</td>\n","      <td>0.987747</td>\n","      <td>0.440649</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:30:16,973]\u001b[0m Trial 3 finished with value: 0.440649302590393 and parameters: {'learning_rate': 6.999159278178944e-05, 'num_train_epochs': 5, 'seed': 37, 'per_device_train_batch_size': 64}. Best is trial 3 with value: 0.440649302590393.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:08, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.973400</td>\n","      <td>0.835618</td>\n","      <td>0.399724</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.764100</td>\n","      <td>0.814342</td>\n","      <td>0.430182</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:31:29,101]\u001b[0m Trial 4 finished with value: 0.4301817602945327 and parameters: {'learning_rate': 5.242022169439337e-05, 'num_train_epochs': 2, 'seed': 17, 'per_device_train_batch_size': 16}. Best is trial 3 with value: 0.440649302590393.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 315\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/315 00:33 < 02:20, 1.80 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.976400</td>\n","      <td>0.814578</td>\n","      <td>0.291602</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:32:07,008]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:08, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.945100</td>\n","      <td>0.763911</td>\n","      <td>0.433740</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.732700</td>\n","      <td>0.794686</td>\n","      <td>0.474076</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:33:19,197]\u001b[0m Trial 6 finished with value: 0.4740763292759752 and parameters: {'learning_rate': 2.8095234045961032e-05, 'num_train_epochs': 2, 'seed': 39, 'per_device_train_batch_size': 16}. Best is trial 6 with value: 0.4740763292759752.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 63\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63/63 00:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.958100</td>\n","      <td>0.838314</td>\n","      <td>0.098730</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:33:57,173]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [126/126 01:08, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.955900</td>\n","      <td>0.818211</td>\n","      <td>0.383144</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.860400</td>\n","      <td>0.814189</td>\n","      <td>0.417253</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:35:09,307]\u001b[0m Trial 8 finished with value: 0.41725288574732244 and parameters: {'learning_rate': 6.989980204680141e-06, 'num_train_epochs': 2, 'seed': 18, 'per_device_train_batch_size': 4}. Best is trial 6 with value: 0.4740763292759752.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/126 00:33 < 00:35, 1.80 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.971400</td>\n","      <td>0.840560</td>\n","      <td>0.358258</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:35:47,239]\u001b[0m Trial 9 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 189\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [189/189 01:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.945700</td>\n","      <td>0.762971</td>\n","      <td>0.434871</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.736200</td>\n","      <td>0.801458</td>\n","      <td>0.172314</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.549500</td>\n","      <td>1.020252</td>\n","      <td>0.108551</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:37:34,148]\u001b[0m Trial 10 finished with value: 0.1085507040779418 and parameters: {'learning_rate': 2.6566446132314105e-05, 'num_train_epochs': 3, 'seed': 39, 'per_device_train_batch_size': 16}. Best is trial 6 with value: 0.4740763292759752.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 252\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/252 00:34 < 01:45, 1.79 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.992900</td>\n","      <td>1.042410</td>\n","      <td>0.001237</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:38:12,310]\u001b[0m Trial 11 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 189\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [189/189 01:42, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.943900</td>\n","      <td>0.797129</td>\n","      <td>0.428811</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.746700</td>\n","      <td>0.767721</td>\n","      <td>0.480533</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.600800</td>\n","      <td>0.791066</td>\n","      <td>0.256684</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2022-08-07 00:39:58,589]\u001b[0m Trial 12 finished with value: 0.25668367294048694 and parameters: {'learning_rate': 2.0486831469746512e-05, 'num_train_epochs': 3, 'seed': 35, 'per_device_train_batch_size': 32}. Best is trial 6 with value: 0.4740763292759752.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 252\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/252 00:34 < 01:45, 1.79 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.008900</td>\n","      <td>0.942178</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:40:36,622]\u001b[0m Trial 13 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 252\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/252 00:34 < 01:46, 1.78 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.998600</td>\n","      <td>0.931047</td>\n","      <td>0.352626</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:41:14,959]\u001b[0m Trial 14 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 126\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/126 00:34 < 00:35, 1.79 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.953300</td>\n","      <td>0.817256</td>\n","      <td>0.115645</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:41:53,022]\u001b[0m Trial 15 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 189\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='63' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 63/189 00:33 < 01:10, 1.80 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Matthews Correlation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.969000</td>\n","      <td>0.849714</td>\n","      <td>0.180839</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1015\n","  Batch size = 8\n","\u001b[32m[I 2022-08-07 00:42:30,947]\u001b[0m Trial 16 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.21.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: source, quality, text, Unnamed: 0. If source, quality, text, Unnamed: 0 are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 500\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 252\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='41' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 41/252 00:13 < 01:13, 2.86 it/s, Epoch 0.63/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["if hyperparameter_tuning:\n","  \n","  args.load_best_model_at_end = False\n","  args.save_stratgy = \"no\"\n","  #args.out_dir = \"/content/hyper_fold\"\n","  #model_args =args\n","  #model_args.reprocess_input_data = True\n","  #model_args.overwrite_output_dir = True\n","  #model_args.evaluate_during_training = True\n","  #model_args.manual_seed = 4\n","  #model_args.use_multiprocessing = True\n","\n","\n","  trainer = Trainer(\n","      model_init=model_init,\n","      args=args,\n","      train_dataset=train_dataset_hyper,\n","      eval_dataset=eval_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics\n","  )\n","\n","  best_run = trainer.hyperparameter_search(n_trials=num_hyper_trails, direction=\"maximize\")\n","  best_run\n","#trainer.train()\n","\n","#trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3WNsxzHS6je"},"outputs":[],"source":["if hyperparameter_tuning:\n","  best_run\n","  try:\n","    hyper_best = []\n","    for n, v in best_run.hyperparameters.items():\n","        t = [n,v]\n","        hyper_best.append(t)\n","        print(\"in\")\n","        print(n)\n","        print(v)\n","        print(\"out\")\n","        setattr(trainer.args, n, v)\n","\n","    import pickle\n","    my_file = out + \"/hyper_best.p\"\n","    print(\"saving hyper\")\n","    print(hyper_best)\n","    pickle.dump( hyper_best, open( my_file, \"wb\" )   ) \n","\n","    ld = pickle.load( open( my_file, \"rb\" ) )\n","    print(\"saved hyper\")\n","    print(ld)\n","\n","    #args.num_train_epochs = 40\n","    args.load_best_model_at_end = True\n","    args.save_stratgy = \"epoch\"\n","    args.out_dir = out\n","\n","    #free_gpu_cache()\n","\n","    trainer = Trainer(\n","      model,\n","      args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics\n","    )\n","\n","\n","    train_result = trainer.train() \n","  except Exception as e:\n","    print(e)\n","    print(\"fail\")\n","    #!kill -9 -1\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-84QX-hcdTAm"},"outputs":[],"source":["#!kill -9 -1\n","!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"finetuner-sentiment_hyper.ipynb","provenance":[{"file_id":"1BUoiPdIj_YosKbOIoOiNQnE0VXVErHUg","timestamp":1659784909738}],"background_execution":"on","mount_file_id":"1BUoiPdIj_YosKbOIoOiNQnE0VXVErHUg","authorship_tag":"ABX9TyNz3zySLaAjia5uXb1cGME2"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1782b42d886d4a3f96df537eed30aede":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_693d3965471a477fb37e0e835271064f","IPY_MODEL_987b7030732546e2b1b16357ab530089","IPY_MODEL_42f4089901c149268ebb4734fca4bcfa"],"layout":"IPY_MODEL_a3230ec4a6444e7f853e6a7ee1b220d0"}},"693d3965471a477fb37e0e835271064f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e22d3616ada84b4ba6b851f4f9c1b463","placeholder":"​","style":"IPY_MODEL_113a6618391b4ccc9e69e7dcf69b42a2","value":"Downloading config.json: 100%"}},"987b7030732546e2b1b16357ab530089":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7f1f21d15c140acbca2b4ebc97c162a","max":841,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efc6c9facc964275858892066131718f","value":841}},"42f4089901c149268ebb4734fca4bcfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e60fc7dc062b4621be02813f45511eb8","placeholder":"​","style":"IPY_MODEL_add2b3debc214d929e5ab3b71f1815d7","value":" 841/841 [00:00&lt;00:00, 17.6kB/s]"}},"a3230ec4a6444e7f853e6a7ee1b220d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22d3616ada84b4ba6b851f4f9c1b463":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"113a6618391b4ccc9e69e7dcf69b42a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7f1f21d15c140acbca2b4ebc97c162a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efc6c9facc964275858892066131718f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e60fc7dc062b4621be02813f45511eb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"add2b3debc214d929e5ab3b71f1815d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2ae1a3a0b264623bc0973d3a0f88441":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_946343733a204c159dbc9d80d20fc1d2","IPY_MODEL_76dd03776d4f4d31b3deaed2263559e3","IPY_MODEL_40fef74c39ff49ec9411f67746259ed0"],"layout":"IPY_MODEL_838c2b3a6e464e30a80ca555c002a6bf"}},"946343733a204c159dbc9d80d20fc1d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19fac454ca6d4f02b4e21a637caaac15","placeholder":"​","style":"IPY_MODEL_a8eacba920554ad09aea1aecfc54743e","value":"Downloading sentencepiece.bpe.model: 100%"}},"76dd03776d4f4d31b3deaed2263559e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c69cdb9b0ac429583eaa03febc3c6e4","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f7a0e77ba3d4fff818f5feade99db20","value":5069051}},"40fef74c39ff49ec9411f67746259ed0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_703376c205dc4c5b8f1600621799fc9c","placeholder":"​","style":"IPY_MODEL_e326cb512041456b9fe4eb3b03c0833a","value":" 4.83M/4.83M [00:00&lt;00:00, 21.5MB/s]"}},"838c2b3a6e464e30a80ca555c002a6bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19fac454ca6d4f02b4e21a637caaac15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8eacba920554ad09aea1aecfc54743e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c69cdb9b0ac429583eaa03febc3c6e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7a0e77ba3d4fff818f5feade99db20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"703376c205dc4c5b8f1600621799fc9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e326cb512041456b9fe4eb3b03c0833a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"696b96f2ffc4452fb90c350822483fe3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9ce29647cd0436e9cae8cc70d7517fd","IPY_MODEL_0c71ea1394e1400ab696eefa2cb30492","IPY_MODEL_defdac1cdccf48099bda164e4b656079"],"layout":"IPY_MODEL_7f890e1c1fc4473aacee37dece3854f2"}},"e9ce29647cd0436e9cae8cc70d7517fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_777fe49ca4cc40dd9bbe46be60087194","placeholder":"​","style":"IPY_MODEL_1db8ed1f3ccc46faa2b6d601ff5d4ab1","value":"Downloading special_tokens_map.json: 100%"}},"0c71ea1394e1400ab696eefa2cb30492":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3798742e9d049cebc93a1f8e8b19140","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db830aab0b4745ff8559e40e02859082","value":150}},"defdac1cdccf48099bda164e4b656079":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d79b519f38d4a8c9fe3ceb736318765","placeholder":"​","style":"IPY_MODEL_272fa9bce0a643e7808ca38f2f9cb6a7","value":" 150/150 [00:00&lt;00:00, 3.94kB/s]"}},"7f890e1c1fc4473aacee37dece3854f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"777fe49ca4cc40dd9bbe46be60087194":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db8ed1f3ccc46faa2b6d601ff5d4ab1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3798742e9d049cebc93a1f8e8b19140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db830aab0b4745ff8559e40e02859082":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d79b519f38d4a8c9fe3ceb736318765":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"272fa9bce0a643e7808ca38f2f9cb6a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f106d04bded4041afd74a5de590950c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffa5a7cbea4647df9b43b7ba232612a3","IPY_MODEL_364f6f5f68cc41dca8dfc1f33cfc698e","IPY_MODEL_4f947cadf1d3420db812db14386fa60d"],"layout":"IPY_MODEL_bba8f06c64274ad4910fd1650c5734c3"}},"ffa5a7cbea4647df9b43b7ba232612a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25316bf5295b44a8a5a82da0b2782811","placeholder":"​","style":"IPY_MODEL_a4eb3b4435f54953af4a6405d9accb68","value":"100%"}},"364f6f5f68cc41dca8dfc1f33cfc698e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74439ba28a8445fd833e2bd1ef538c86","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4337ac82fc334bb9b521080b0497bad1","value":4}},"4f947cadf1d3420db812db14386fa60d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1189379089d45bcaa8918a9d541821d","placeholder":"​","style":"IPY_MODEL_b55ddeb9ffcb4ce4b186d5ed23ae687c","value":" 4/4 [00:01&lt;00:00,  2.00ba/s]"}},"bba8f06c64274ad4910fd1650c5734c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25316bf5295b44a8a5a82da0b2782811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4eb3b4435f54953af4a6405d9accb68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74439ba28a8445fd833e2bd1ef538c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4337ac82fc334bb9b521080b0497bad1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1189379089d45bcaa8918a9d541821d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b55ddeb9ffcb4ce4b186d5ed23ae687c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aecd9efdfd17416aba56440b240501ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3efd7d6add254fc6a4091c6b4eb499a4","IPY_MODEL_b43c6dfd2343471099f92b1d11ccb158","IPY_MODEL_40d5d7abb7cb4cc391982392220f9f66"],"layout":"IPY_MODEL_051693cc188b4177905c594cde5de7b5"}},"3efd7d6add254fc6a4091c6b4eb499a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa730b452ca45a588f485db5b697c58","placeholder":"​","style":"IPY_MODEL_8f94808af7054b3695ee36c21fa17bc5","value":"100%"}},"b43c6dfd2343471099f92b1d11ccb158":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f0e44edd5fd4ab6a7f1fa97e679f2df","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0681f0e61cb49be9327146531349326","value":2}},"40d5d7abb7cb4cc391982392220f9f66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8153f677ecce463b9ee6424d02ec3824","placeholder":"​","style":"IPY_MODEL_3cc0b787a45a4e2080ae0b9c54d31bf5","value":" 2/2 [00:00&lt;00:00,  1.84ba/s]"}},"051693cc188b4177905c594cde5de7b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa730b452ca45a588f485db5b697c58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f94808af7054b3695ee36c21fa17bc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f0e44edd5fd4ab6a7f1fa97e679f2df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0681f0e61cb49be9327146531349326":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8153f677ecce463b9ee6424d02ec3824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cc0b787a45a4e2080ae0b9c54d31bf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c121fd87a3641e792925b9d6be3bf74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e19e58bdb0f8453aa467661b3184ebd2","IPY_MODEL_23f781a9ea484c4da64822ea5b13d703","IPY_MODEL_a5bfda4f1a8b402bae88d8d5473b0020"],"layout":"IPY_MODEL_8008442de894474b9813705775afb5dd"}},"e19e58bdb0f8453aa467661b3184ebd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_862007405a1c451c8ebf4880414caab5","placeholder":"​","style":"IPY_MODEL_fd5e58cceadf4868ab56be043dc68d6a","value":"100%"}},"23f781a9ea484c4da64822ea5b13d703":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f77e1f171414f4d8729502a7a1825fc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adb0b80200c74454b2f9baab1d92925b","value":1}},"a5bfda4f1a8b402bae88d8d5473b0020":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_277cfadce5c94d0598c01b081e4dc937","placeholder":"​","style":"IPY_MODEL_19e5d477f0c04c1f9d192cc2676d4358","value":" 1/1 [00:00&lt;00:00,  4.09ba/s]"}},"8008442de894474b9813705775afb5dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862007405a1c451c8ebf4880414caab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd5e58cceadf4868ab56be043dc68d6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f77e1f171414f4d8729502a7a1825fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adb0b80200c74454b2f9baab1d92925b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"277cfadce5c94d0598c01b081e4dc937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19e5d477f0c04c1f9d192cc2676d4358":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40447846eed442c8a5b7afbb7d0a6569":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0207c005d9e34af4a347adacacef0ae3","IPY_MODEL_0737fcfae0c04bd2a66fcb58042bede1","IPY_MODEL_0d310bacbbd0419e9a06c6a0455b4dd9"],"layout":"IPY_MODEL_651fa098fc534916b4ebb14218c50d03"}},"0207c005d9e34af4a347adacacef0ae3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c78e93e90d154d0aa7fc41a091dee077","placeholder":"​","style":"IPY_MODEL_f448c0fa7703492f8b253637388852cc","value":"Downloading builder script: "}},"0737fcfae0c04bd2a66fcb58042bede1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f7089d916984c90a71455f2a250f336","max":1705,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f42ce2203f864527aba72c98e493b524","value":1705}},"0d310bacbbd0419e9a06c6a0455b4dd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_116c90efde8f43d3850933f1c8a15c55","placeholder":"​","style":"IPY_MODEL_8e9cf73183e0429197c372fa3d53fc7a","value":" 4.47k/? [00:00&lt;00:00, 74.8kB/s]"}},"651fa098fc534916b4ebb14218c50d03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c78e93e90d154d0aa7fc41a091dee077":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f448c0fa7703492f8b253637388852cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f7089d916984c90a71455f2a250f336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f42ce2203f864527aba72c98e493b524":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"116c90efde8f43d3850933f1c8a15c55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9cf73183e0429197c372fa3d53fc7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3a0a656fb4f41d0a78285a0db873a79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f3ad4a9a76042038e74666f4a0b5d57","IPY_MODEL_495f3a528d9e4e38b66c388526fc85d7","IPY_MODEL_38457922428e4d59afa7e58249841e88"],"layout":"IPY_MODEL_f77c186e221640af97c066724756d979"}},"8f3ad4a9a76042038e74666f4a0b5d57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abc9ae04d1e24a898687ece2325516ac","placeholder":"​","style":"IPY_MODEL_110a48c02f37433d99ac1aee89ba2eec","value":"Downloading pytorch_model.bin: 100%"}},"495f3a528d9e4e38b66c388526fc85d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_437e113ebbf449f8a360a3f944400df4","max":1112271561,"min":0,"orientation":"horizontal","style":"IPY_MODEL_124305aad197453589de22082f2953a3","value":1112271561}},"38457922428e4d59afa7e58249841e88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93b54b200013407ab6d12e5f8cb668a3","placeholder":"​","style":"IPY_MODEL_eafc018254a1461a9d6b441cac3a4b00","value":" 1.04G/1.04G [00:28&lt;00:00, 52.7MB/s]"}},"f77c186e221640af97c066724756d979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc9ae04d1e24a898687ece2325516ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"110a48c02f37433d99ac1aee89ba2eec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"437e113ebbf449f8a360a3f944400df4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"124305aad197453589de22082f2953a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93b54b200013407ab6d12e5f8cb668a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eafc018254a1461a9d6b441cac3a4b00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}