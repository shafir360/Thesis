{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16910,"status":"ok","timestamp":1659706943373,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"klpNNdQNRAzv","outputId":"f96153b0-8af5-4320-fba3-1342c7a2fced"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21452,"status":"error","timestamp":1659706964806,"user":{"displayName":"S Rahman","userId":"04000762971548563689"},"user_tz":-60},"id":"iX00mr6WfSHG","outputId":"749d4456-bf4b-41ed-e3e1-3a8e706bfbbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.9 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 55.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 75.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[K     |████████████████████████████████| 365 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 93.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 95.5 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.11.1\n","  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n","\u001b[K     |████████████████████████████████| 141 kB 91.0 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 86.4 MB/s \n","\u001b[?25hRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n","    return self.__dep_map\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n","    raise AttributeError(attr)\n","AttributeError: _DistInfoDistribution__dep_map\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n","    status = self.run(options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n","    conflicts = self._determine_conflicts(to_install)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n","    return check_install_conflicts(to_install)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n","    package_set, _ = create_package_set_from_installed()\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n","    package_set[name] = PackageDetails(dist.version, dist.requires())\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n","    dm = self._dep_map\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n","    self.__dep_map = self._compute_dependencies()\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n","    reqs.extend(parse_requirements(req))\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n","    yield Requirement(line)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n","    super(Requirement, self).__init__(requirement_string)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n","    req = REQUIREMENT.parseString(requirement_string)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n","    loc, tokens = self._parse(instring, 0)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n","    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n","    loc, exprtokens = e._parse(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4781, in parseImpl\n","    return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4707, in parseImpl\n","    loc, tmptokens = self_expr_parse(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n","    loc, tokens = self.parseImpl(instring, preloc, doActions)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4259, in parseImpl\n","    maxExcLoc = err.loc\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 212, in _main\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n","    self._log(CRITICAL, msg, args, **kwargs)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1502, in _log\n","    fn, lno, func, sinfo = self.findCaller(stack_info)\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 1451, in findCaller\n","    f = currentframe()\n","  File \"/usr/lib/python3.7/logging/__init__.py\", line 154, in <lambda>\n","    currentframe = lambda: sys._getframe(3)\n","KeyboardInterrupt\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.9 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting GPUtil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=8049593737679f426cdb68f6570989635e9fea153335650276c4f2ca97a7ea8c\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built GPUtil\n","Installing collected packages: GPUtil\n","Successfully installed GPUtil-1.4.0\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-96e6093cc811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["!pip install transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install GPUtil\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","#!pip install wandb\n","import json\n","from transformers import AutoTokenizer\n","from transformers import TFAutoModelForSequenceClassification\n","import tensorflow as tf\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoSFcYvATZJ_"},"outputs":[],"source":["\n","def ConvertLabel2ModelLabel(label):\n","  if label == 'Positive':\n","    o = 2\n","  elif label == 'Neutral':\n","    o = 1\n","  elif label == 'Negative':\n","    o = 0\n","  elif label == 'spam':\n","    o = 1\n","  else: \n","    print(\" error at ConvertLabel2ModelLabel label is \" , label)\n","    o = np.nan\n","  return o\n","\n","import torch\n","from GPUtil import showUtilization as gpu_usage\n","from numba import cuda\n","\n","def free_gpu_cache():\n","    print(\"Initial GPU Usage\")\n","    gpu_usage()                             \n","\n","    torch.cuda.empty_cache()\n","\n","    cuda.select_device(0)\n","    cuda.close()\n","    cuda.select_device(0)\n","\n","    print(\"GPU Usage after emptying the cache\")\n","    gpu_usage()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Yk0sHM_ff68"},"outputs":[],"source":["train_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_3.0:4.0:4.0/train_dataset.csv'\n","valid_path = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/dataset/dataset_new_with_self_label_3.0:4.0:4.0/valid_dataset.csv'\n","#model_name = \"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\"\n","#out_dir = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/tomas23_3:4:4/with_high'\n","#out_dir2 = '/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models/tomas23_3:4:4/with_high_then_low'\n","\n","import shutil\n","\n","\n","\n","\n","\n","name_of_text_column = \"text\"\n","hyperparameter_tuning = False\n","reduce_ = False\n","num_hyper_trails = 100\n","epoch = 60\n","\n","df_train = pd.read_csv(train_path)\n","\n","df_train_high = df_train[df_train['quality']==\"high\"]\n","df_train_low = df_train[df_train['quality']==\"low\"]\n","\n","df_valid = pd.read_csv(valid_path)\n","\n","display(df_train)\n","display(df_valid)\n","display(df_train_high)\n","display(df_train_low)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoIvErk1T_LX"},"outputs":[],"source":["df_train['label'] = df_train['label'].apply(ConvertLabel2ModelLabel)\n","df_valid['label'] = df_valid['label'].apply(ConvertLabel2ModelLabel)\n","\n","df_train_high['label'] = df_train_high['label'].apply(ConvertLabel2ModelLabel)\n","df_train_low['label'] = df_train_low['label'].apply(ConvertLabel2ModelLabel)\n","\n","display(df_train)\n","display(df_valid)\n","display(df_train_high)\n","display(df_train_low)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SG6S7inA_RKv"},"outputs":[],"source":["from transformers import Trainer\n","from transformers import TrainingArguments\n","#learning_rate = 5e-05\n","#batch_size = 16\n","#eval_batch_size = 4\n","seed = 40\n","#optimizer = Adam \n","#with betas=(0.9,0.999) and epsilon=1e-08\n","adam_beta1 = 0.9\n","adam_beta2 =0.999\n","lr_scheduler_type = \"linear\"\n","num_epochs = 15\n","#args = TrainingArguments(\"test_trainer\",report_to=\"wandb\" ,logging_strategy = \"epoch\",evaluation_strategy=\"epoch\",learning_rate = learning_rate,num_train_epochs = num_epochs,lr_scheduler_type =lr_scheduler_type, adam_beta1 = adam_beta1,adam_beta2 =adam_beta2  )\n","\n","\n","def run_trainer(model_name,out_dir,epoch,batch_size,df_train,df_valid,):\n","  df_train = df_train[['text','label']].set_index('text')\n","  df_valid = df_valid[['text','label']].set_index('text')\n","  #display(df_train)\n","  #display(df_valid)\n","\n","  dataset = ds.dataset(pa.Table.from_pandas(df_valid).to_batches())\n","  ### convert to Huggingface dataset\n","  validation_dataset_torch = Dataset(pa.Table.from_pandas(df_valid))\n","\n","  dataset = ds.dataset(pa.Table.from_pandas(df_train).to_batches())\n","  ### convert to Huggingface dataset\n","  training_dataset_torch = Dataset(pa.Table.from_pandas(df_train))\n","\n","  #print(training_dataset_torch)\n","  #print(validation_dataset_torch)\n","\n","  from transformers import AutoTokenizer\n","\n","  tokenizer = AutoTokenizer.from_pretrained(model_name,model_max_length=512)\n","\n","\n","\n","  def tokenize_function(data):\n","      return tokenizer(data['text'], padding=\"max_length\", truncation=True,)\n","\n","\n","  train_dataset = training_dataset_torch.map(tokenize_function, batched=True)\n","  eval_dataset = validation_dataset_torch.map(tokenize_function, batched=True)\n","  print(train_dataset)\n","  print(eval_dataset)\n","\n","  import numpy as np\n","  from datasets import load_metric\n","\n","  metric = load_metric(\"matthews_correlation\")\n","\n","  def compute_metrics(eval_pred):\n","      logits, labels = eval_pred\n","      predictions = np.argmax(logits, axis=-1)\n","      return metric.compute(predictions=predictions, references=labels)\n","\n","\n","  from transformers import AutoModelForSequenceClassification\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n","  #model.to(device)\n","  args = TrainingArguments(\n","      #'/content/drive/MyDrive/fyp/fyp2/model/model2-supervised/' + f\"{model_name}-finetuned-\",\n","      #'/content/' + f\"{model_name}-finetuned-\",\n","      out_dir,\n","      #report_to=\"wandb\",\n","      overwrite_output_dir = True,\n","      logging_dir = out_dir,\n","      evaluation_strategy = \"epoch\",\n","      logging_strategy = \"epoch\",\n","      #save_strategy = \"NO\",\n","      learning_rate=2.6510704963161386e-06,\n","      per_device_train_batch_size=batch_size,\n","      per_device_eval_batch_size=batch_size,\n","      num_train_epochs=epoch,\n","      weight_decay=0.01,\n","      metric_for_best_model='matthews_correlation',\n","      save_total_limit = 2,\n","      save_strategy = \"epoch\",\n","      load_best_model_at_end = True,\n","      seed = 18\n","      #push_to_hub=True,\n","  )\n","\n","\n","\n","  trainer = Trainer(\n","      model,\n","      args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      tokenizer=tokenizer,\n","      compute_metrics=compute_metrics\n","  )\n","\n","  #trainer = Trainer(\n","    # model=model,\n","      #args=args,\n","    # train_dataset=train_dataset,\n","    #  eval_dataset=eval_dataset,\n","  #   compute_metrics=compute_metrics,\n","  #)\n","\n","\n","    # training\n","  train_result = trainer.train() \n","  logs = trainer.state.log_history\n","  # compute train results\n","  metrics = train_result.metrics\n","  max_train_samples = len(train_dataset)\n","  metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","\n","  # save train results\n","  trainer.log_metrics(\"train\", metrics)\n","  trainer.save_metrics(\"train\", metrics)\n","\n","  # compute evaluation results\n","  metrics = trainer.evaluate()\n","  max_val_samples = len(eval_dataset)\n","  metrics[\"eval_samples\"] = min(max_val_samples, len(eval_dataset))\n","\n","  # save evaluation results\n","  trainer.log_metrics(\"eval\", metrics)\n","  trainer.save_metrics(\"eval\", metrics)\n","  import pickle\n","  save_logs_dir = out_dir\n","  name_logs = \"logs\"\n","  \n","  my_file = save_logs_dir + \"/\" + name_logs + \".p\"\n","  print(\"saving to \",my_file )\n","\n","  pickle.dump( logs, open( my_file, \"wb\" )    ) \n","\n","  ld = pickle.load( open( my_file, \"rb\" ) )\n","\n","\n","  print(logs)\n","  print(logs[2])\n","  print(type(logs))\n","\n","  print(ld)\n","  print(type(ld))\n","  print(logs == ld)\n","  \n","  import torch\n","  torch.cuda.is_available() \n","  from numba import cuda\n","  del trainer,args\n","  torch.cuda.empty_cache()\n","  #trainer.evaluate()\n","\n"]},{"cell_type":"code","source":["def run_both_training(model_name,out_dir=\"/content/drive/MyDrive/fyp/fyp2/final models and datasets/final models\",batch_size = 16,df_valid = df_valid):\n","  print(\"starting \" ,model_name)\n","  out = out_dir + \"/\" + model_name + \"/high\"\n"," \n","  if not os.path.exists(out):\n","    \n","    # Create a new directory because it does not exist \n","    os.makedirs(out)\n","  else:\n","    shutil.rmtree(out)\n","\n","  run_trainer(model_name=model_name,out_dir = out,epoch =50,df_train = df_train_high,df_valid=df_valid,batch_size=batch_size)\n","  torch.cuda.empty_cache()\n","  print(model_name,\" is completed\")\n","  #out = out_dir + \"/\" + model_name + \"/high_then_low\"\n","  #run_trainer(model_name=model_name,out_dir= out,epoch=50,df_train = df_train_low,df_valid=df_valid)"],"metadata":{"id":"bMAFQ9PVu5tY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#run_both_training(\"Tomas23/twitter-roberta-base-mar2022-finetuned-sentiment\")\n","\n","run_both_training(\"cardiffnlp/bertweet-base-sentiment\")\n","run_both_training(\"svalabs/twitter-xlm-roberta-bitcoin-sentiment\")\n","run_both_training(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n","run_both_training(\"amansolanki/autonlp-Tweet-Sentiment-Extraction-20114061\")\n","run_both_training(\"finiteautomata/bertweet-base-sentiment-analysis\")\n","\n","\n"],"metadata":{"id":"FK_NHP6ZyFIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEUvCosOxZJF"},"outputs":[],"source":["!nvidia-smi "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-84QX-hcdTAm"},"outputs":[],"source":["!kill -9 -1"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Copy of finetuner-sentiment v2_3.ipynb","provenance":[{"file_id":"1yBhFFV8lhhrvIuHWqZXq0rNyFyucdsGI","timestamp":1659707135116},{"file_id":"1fLPYM974gspAK8lzDX6jVnX9t_k53Uy0","timestamp":1659707072244},{"file_id":"1NrcZb-f3iK-NOB6kdsr4eWbqQP6hh08Y","timestamp":1659707013573},{"file_id":"1BUoiPdIj_YosKbOIoOiNQnE0VXVErHUg","timestamp":1659144449674}],"mount_file_id":"1NrcZb-f3iK-NOB6kdsr4eWbqQP6hh08Y","authorship_tag":"ABX9TyP9YTKXMW4ws5QWXV6EjG9J"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}